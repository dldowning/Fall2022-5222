{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Yqgw3KbRjRs5",
        "xVvvM8gbjfj6",
        "IThS2W5HupYr",
        "4EdKeKDC5E8b",
        "C3GzWAOjGFhL"
      ],
      "mount_file_id": "1V2VCQG_9S4SEW8u4AoH7MH37jFacsbjU",
      "authorship_tag": "ABX9TyNhNsr94DN+cj/lanTiSB4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dldowning/Fall2022-5222/blob/main/5222_Tweet_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Rename nouns column "
      ],
      "metadata": {
        "id": "vCU0pgvLURP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import & Preprocess All Data"
      ],
      "metadata": {
        "id": "Z97RUdBCJGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.1* Import libraries and create main dataframe"
      ],
      "metadata": {
        "id": "Yqgw3KbRjRs5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0gm-p1OwcOs"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "  \n",
        "# read text file into pandas DataFrame\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt\", sep=\"\\n\", header=None)\n",
        "df.columns = ['TWEET']\n",
        "\n",
        "# Create columns for features \n",
        "df[\"Count: Words in + Lexicon\"] = np.nan\n",
        "df[\"Count: Words in - Lexicon\"] = np.nan\n",
        "df[\"Contain The word NO? \"] = np.nan\n",
        "df[\"Count: Nouns\"] = np.nan\n",
        "df[\"Contain !\"] = np.nan\n",
        "df[\"Ratio: Unique Words-Total Words\"] = np.nan\n",
        "df[\"Ratio: Stop Words-Total Words\"] = np.nan\n",
        "df[\"Count: Adjectives in + Lexicon\"] = np.nan\n",
        "df[\"Count: Adjectives in - Lexicon\"] = np.nan\n",
        "df[\"Log: Tweet word count\"] = np.nan\n",
        "df[\"Log: Length of Longest Word in Tweet\"] = np.nan\n",
        "df[\"Log: Count of Words with 5+ Characters\"] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "peakGoADUHke",
        "outputId": "1586e006-86fc-40ee-8e81-b9a6882ff676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   TWEET  \\\n",
              "0      QT @user In the original draft of the 7th book...   \n",
              "1      Ben Smith / Smith (concussion) remains out of ...   \n",
              "2      Sorry bout the stream last night I crashed out...   \n",
              "3      Chase Headley's RBI double in the 8th inning o...   \n",
              "4      @user Alciato: Bee will invest 150 million in ...   \n",
              "...                                                  ...   \n",
              "36368  @user \\\"\"So amazing to have the beautiful Lady...   \n",
              "36369  9 September has arrived, which means Apple's n...   \n",
              "36370  Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...   \n",
              "36371  @user no I'm in hilton head till the 8th lol g...   \n",
              "36372  WASHINGTON (Reuters) - U.S. Vice President Joe...   \n",
              "\n",
              "       Count: Words in + Lexicon  Count: Words in - Lexicon  \\\n",
              "0                            NaN                        NaN   \n",
              "1                            NaN                        NaN   \n",
              "2                            NaN                        NaN   \n",
              "3                            NaN                        NaN   \n",
              "4                            NaN                        NaN   \n",
              "...                          ...                        ...   \n",
              "36368                        NaN                        NaN   \n",
              "36369                        NaN                        NaN   \n",
              "36370                        NaN                        NaN   \n",
              "36371                        NaN                        NaN   \n",
              "36372                        NaN                        NaN   \n",
              "\n",
              "       Contain The word NO?   Count: Nouns  Contain !  \\\n",
              "0                        NaN           NaN        NaN   \n",
              "1                        NaN           NaN        NaN   \n",
              "2                        NaN           NaN        NaN   \n",
              "3                        NaN           NaN        NaN   \n",
              "4                        NaN           NaN        NaN   \n",
              "...                      ...           ...        ...   \n",
              "36368                    NaN           NaN        NaN   \n",
              "36369                    NaN           NaN        NaN   \n",
              "36370                    NaN           NaN        NaN   \n",
              "36371                    NaN           NaN        NaN   \n",
              "36372                    NaN           NaN        NaN   \n",
              "\n",
              "       Ratio: Unique Words-Total Words  Ratio: Stop Words-Total Words  \\\n",
              "0                                  NaN                            NaN   \n",
              "1                                  NaN                            NaN   \n",
              "2                                  NaN                            NaN   \n",
              "3                                  NaN                            NaN   \n",
              "4                                  NaN                            NaN   \n",
              "...                                ...                            ...   \n",
              "36368                              NaN                            NaN   \n",
              "36369                              NaN                            NaN   \n",
              "36370                              NaN                            NaN   \n",
              "36371                              NaN                            NaN   \n",
              "36372                              NaN                            NaN   \n",
              "\n",
              "       Count: Adjectives in + Lexicon  Count: Adjectives in - Lexicon  \\\n",
              "0                                 NaN                             NaN   \n",
              "1                                 NaN                             NaN   \n",
              "2                                 NaN                             NaN   \n",
              "3                                 NaN                             NaN   \n",
              "4                                 NaN                             NaN   \n",
              "...                               ...                             ...   \n",
              "36368                             NaN                             NaN   \n",
              "36369                             NaN                             NaN   \n",
              "36370                             NaN                             NaN   \n",
              "36371                             NaN                             NaN   \n",
              "36372                             NaN                             NaN   \n",
              "\n",
              "       Log: Tweet word count  Log: Length of Longest Word in Tweet  \\\n",
              "0                        NaN                                   NaN   \n",
              "1                        NaN                                   NaN   \n",
              "2                        NaN                                   NaN   \n",
              "3                        NaN                                   NaN   \n",
              "4                        NaN                                   NaN   \n",
              "...                      ...                                   ...   \n",
              "36368                    NaN                                   NaN   \n",
              "36369                    NaN                                   NaN   \n",
              "36370                    NaN                                   NaN   \n",
              "36371                    NaN                                   NaN   \n",
              "36372                    NaN                                   NaN   \n",
              "\n",
              "       Log: Count of Words with 5+ Characters  \n",
              "0                                         NaN  \n",
              "1                                         NaN  \n",
              "2                                         NaN  \n",
              "3                                         NaN  \n",
              "4                                         NaN  \n",
              "...                                       ...  \n",
              "36368                                     NaN  \n",
              "36369                                     NaN  \n",
              "36370                                     NaN  \n",
              "36371                                     NaN  \n",
              "36372                                     NaN  \n",
              "\n",
              "[36373 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d2f6391-7dc5-4486-a2e8-6f62b5ebbfe8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWEET</th>\n",
              "      <th>Count: Words in + Lexicon</th>\n",
              "      <th>Count: Words in - Lexicon</th>\n",
              "      <th>Contain The word NO?</th>\n",
              "      <th>Count: Nouns</th>\n",
              "      <th>Contain !</th>\n",
              "      <th>Ratio: Unique Words-Total Words</th>\n",
              "      <th>Ratio: Stop Words-Total Words</th>\n",
              "      <th>Count: Adjectives in + Lexicon</th>\n",
              "      <th>Count: Adjectives in - Lexicon</th>\n",
              "      <th>Log: Tweet word count</th>\n",
              "      <th>Log: Length of Longest Word in Tweet</th>\n",
              "      <th>Log: Count of Words with 5+ Characters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QT @user In the original draft of the 7th book...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ben Smith / Smith (concussion) remains out of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sorry bout the stream last night I crashed out...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36368</th>\n",
              "      <td>@user \\\"\"So amazing to have the beautiful Lady...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36369</th>\n",
              "      <td>9 September has arrived, which means Apple's n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36370</th>\n",
              "      <td>Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36371</th>\n",
              "      <td>@user no I'm in hilton head till the 8th lol g...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36372</th>\n",
              "      <td>WASHINGTON (Reuters) - U.S. Vice President Joe...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36373 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d2f6391-7dc5-4486-a2e8-6f62b5ebbfe8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d2f6391-7dc5-4486-a2e8-6f62b5ebbfe8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d2f6391-7dc5-4486-a2e8-6f62b5ebbfe8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Import & Unzip Datasets if needed"
      ],
      "metadata": {
        "id": "xVvvM8gbjfj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/socialsent/files/socialsent_subreddits.zip\n",
        "!unzip /content/socialsent_subreddits.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqMTZ8gYRgHI",
        "outputId": "40a21546-7d48-42f7-fdd7-11ab41e1c4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-11 13:41:02--  https://nlp.stanford.edu/projects/socialsent/files/socialsent_subreddits.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15659374 (15M) [application/zip]\n",
            "Saving to: ‘socialsent_subreddits.zip’\n",
            "\n",
            "socialsent_subreddi 100%[===================>]  14.93M  4.42MB/s    in 3.4s    \n",
            "\n",
            "2022-10-11 13:41:06 (4.42 MB/s) - ‘socialsent_subreddits.zip’ saved [15659374/15659374]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.2* Create dataframe for each of our selected subbeddit .tsv files "
      ],
      "metadata": {
        "id": "WIUUCEbbj7AE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SflpPbC16J6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read file names into list - will later use to iterate gathering data \n",
        "\n",
        "my_file = open(\"/content/drive/MyDrive/5290 Data/subreddit_files.txt\", \"r\")\n",
        "data = my_file.read()\n",
        "\n",
        "data_into_list = data.split(\"\\n\")\n",
        "print(data_into_list)\n",
        "my_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlpvD2rmurcT",
        "outputId": "7aa208ad-8135-48e8-8b86-48e40c70421c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['3DS.tsv', '4chan.tsv', '2007scape.tsv', 'ACTrade.tsv', 'amiugly.tsv', 'BabyBumps.tsv', 'baseball.tsv', 'canada.tsv', 'CasualConversation.tsv', 'DarknetMarkets.tsv', 'darksouls.tsv', 'elderscrollsonline.tsv', 'Eve.tsv', 'Fallout.tsv', 'fantasyfootball.tsv', 'GameDeals.tsv', 'gamegrumps.tsv', 'halo.tsv', 'Homebrewing.tsv', 'IAmA.tsv', 'india.tsv', 'jailbreak.tsv', 'Jokes.tsv', 'KerbalSpaceProgram.tsv', 'Keto.tsv', 'leagueoflegends.tsv', 'Libertarian.tsv', 'magicTCG.tsv', 'MakeupAddiction.tsv', 'Naruto.tsv', 'nba.tsv', 'occulus.tsv', 'OkCupid.tsv', 'Parenting.tsv', 'pathofexile.tsv', 'raisedbynarcissists.tsv', 'Random_Acts_Of_Amazon.tsv', 'science.tsv', 'Seattle.tsv', 'TalesFromRetail.tsv', 'talesfromtechsupport.tsv', 'ultrahardcore.tsv', 'videos.tsv', 'Warthunder.tsv', 'histfreq2000.tsv', 'histadj2000.tsv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.3* Create List That Holds All Subreddit Dataframes"
      ],
      "metadata": {
        "id": "05MRhqedjJES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use file names to create dataframe names \n",
        "\n",
        "ls_df_names = []            # List to hold subreddit dataframes \n",
        "\n",
        "for i in range(0,len(data_into_list)):   # For range in length of the # of subreddits we will use (44)    \n",
        "  x = data_into_list[i]                  # at index i of data_into_list assign the str value to x\n",
        "  dataframe_name = 'df_'+ x[:-4]          # with the assigned x value, append 'df_' to the front of te string and remove the last 4 characters \".tsv\". Leaves \"df_4chan\"\n",
        "\n",
        "  foo1 = dataframe_name\n",
        "  exec(foo1 + \" = ls_df_names.append(pd.read_csv(\\\"/content/drive/MyDrive/5290 Data/subreddits/\" + x +\"\\\",sep = '\\t', header=None)) \")    # Convert the string stored in dataframe_name to a variable name to hold the respective dataframe \n",
        "                                                                                                                                          # https://stackoverflow.com/questions/11553721/using-a-string-variable-as-a-variable-name\n",
        "  ls_df_names[i].columns = ['Word', 'Sentiment Score', 'drop']            # Create clumn names in each respective dataframe \n",
        "  ls_df_names[i] = ls_df_names[i].drop('drop', axis=1)                    # Drop unwanted column\n",
        "  \n"
      ],
      "metadata": {
        "id": "9szuf5iVjGJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_df_names[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1649ISxSKvUR",
        "outputId": "0782c997-37a1-4032-accb-59258bd691bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Word  Sentiment Score\n",
              "0             ui            -6.06\n",
              "1         shitty            -6.04\n",
              "2       pathetic            -5.80\n",
              "3           suck            -5.75\n",
              "4         stupid            -5.72\n",
              "...          ...              ...\n",
              "4334       remix             2.46\n",
              "4335      donkey             2.47\n",
              "4336  earthbound             2.51\n",
              "4337      unique             2.51\n",
              "4338         rad             2.75\n",
              "\n",
              "[4339 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51a9b60f-40c1-4946-a647-92fd629a344d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ui</td>\n",
              "      <td>-6.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shitty</td>\n",
              "      <td>-6.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pathetic</td>\n",
              "      <td>-5.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>suck</td>\n",
              "      <td>-5.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>stupid</td>\n",
              "      <td>-5.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4334</th>\n",
              "      <td>remix</td>\n",
              "      <td>2.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4335</th>\n",
              "      <td>donkey</td>\n",
              "      <td>2.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4336</th>\n",
              "      <td>earthbound</td>\n",
              "      <td>2.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4337</th>\n",
              "      <td>unique</td>\n",
              "      <td>2.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4338</th>\n",
              "      <td>rad</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4339 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51a9b60f-40c1-4946-a647-92fd629a344d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51a9b60f-40c1-4946-a647-92fd629a344d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51a9b60f-40c1-4946-a647-92fd629a344d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 Dictionary For Users to Refernce Dataframe Locations "
      ],
      "metadata": {
        "id": "MeeI_SeLqI8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = dict(zip(data_into_list, list(range(0, 46))))     # To easily reference all dataframe positions in list ls_df_names\n",
        "df_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeYk5GjjzxTl",
        "outputId": "f84d97c4-ed74-49e2-edeb-44f24417f2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'3DS.tsv': 0,\n",
              " '4chan.tsv': 1,\n",
              " '2007scape.tsv': 2,\n",
              " 'ACTrade.tsv': 3,\n",
              " 'amiugly.tsv': 4,\n",
              " 'BabyBumps.tsv': 5,\n",
              " 'baseball.tsv': 6,\n",
              " 'canada.tsv': 7,\n",
              " 'CasualConversation.tsv': 8,\n",
              " 'DarknetMarkets.tsv': 9,\n",
              " 'darksouls.tsv': 10,\n",
              " 'elderscrollsonline.tsv': 11,\n",
              " 'Eve.tsv': 12,\n",
              " 'Fallout.tsv': 13,\n",
              " 'fantasyfootball.tsv': 14,\n",
              " 'GameDeals.tsv': 15,\n",
              " 'gamegrumps.tsv': 16,\n",
              " 'halo.tsv': 17,\n",
              " 'Homebrewing.tsv': 18,\n",
              " 'IAmA.tsv': 19,\n",
              " 'india.tsv': 20,\n",
              " 'jailbreak.tsv': 21,\n",
              " 'Jokes.tsv': 22,\n",
              " 'KerbalSpaceProgram.tsv': 23,\n",
              " 'Keto.tsv': 24,\n",
              " 'leagueoflegends.tsv': 25,\n",
              " 'Libertarian.tsv': 26,\n",
              " 'magicTCG.tsv': 27,\n",
              " 'MakeupAddiction.tsv': 28,\n",
              " 'Naruto.tsv': 29,\n",
              " 'nba.tsv': 30,\n",
              " 'occulus.tsv': 31,\n",
              " 'OkCupid.tsv': 32,\n",
              " 'Parenting.tsv': 33,\n",
              " 'pathofexile.tsv': 34,\n",
              " 'raisedbynarcissists.tsv': 35,\n",
              " 'Random_Acts_Of_Amazon.tsv': 36,\n",
              " 'science.tsv': 37,\n",
              " 'Seattle.tsv': 38,\n",
              " 'TalesFromRetail.tsv': 39,\n",
              " 'talesfromtechsupport.tsv': 40,\n",
              " 'ultrahardcore.tsv': 41,\n",
              " 'videos.tsv': 42,\n",
              " 'Warthunder.tsv': 43,\n",
              " 'histfreq2000.tsv': 44,\n",
              " 'histadj2000.tsv': 45}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.4* Count the number of postive and negative words in each tweet"
      ],
      "metadata": {
        "id": "6pBV0Mhe11n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Generate Postitive and Negative Lexicon of Word\n",
        "For each word in each data frame, if the word has a sentiment score of zero or greater, send it to the positive lexicon. If the Sentiment score is negative the word is sent to the negative lexicon.  "
      ],
      "metadata": {
        "id": "4JltmTng1p2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_lexicon = []\n",
        "neg_lexicon = []\n",
        "df_lengths = []"
      ],
      "metadata": {
        "id": "nh6YaAzn3aU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Positive and Negative Lexicons\n",
        "#i is the dataframe j is rows in the dataframe \n",
        "for i in range(0, len(ls_df_names)):              # For index in range of 0 - length of list storing dataframes \n",
        "  for j in range(0, len(ls_df_names[i].index)):     # For j-index in range of specific dataframe..\n",
        "\n",
        "    if ls_df_names[i]['Sentiment Score'].values[j] >= 0:        # If the specific dataframe row has a sentiment score greater than or equal to 0... \n",
        "      pos_lexicon.append(ls_df_names[i]['Word'].values[j])        # Append that word value to the the lsit of positive lexicon words\n",
        "    \n",
        "    if ls_df_names[i]['Sentiment Score'].values[j] < 0:         # If the specific dataframe row has a sentiment score less than 0...\n",
        "      neg_lexicon.append(ls_df_names[i]['Word'].values[j])        # # Append that word value to the the lsit of negative lexicon words"
      ],
      "metadata": {
        "id": "C7KLdW8f6OQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.1a. Remove duplicate values in each lexicon"
      ],
      "metadata": {
        "id": "ZsArMnxTyfLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_lexicon = [*set(pos_lexicon)]   # Remove duplicate values from + lexicon\n",
        "neg_lexicon = [*set(neg_lexicon)]   # Remove duplicate values from - lexicon"
      ],
      "metadata": {
        "id": "wXxQnHIz6tCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Handling Words Seen In Positive & Negative Lexicons \n",
        " Because we dont want words that are seen in our positive lexicon to also appear in our negative lexicon we must choose a way to decide their classification. My method is take the avgrage sentiment score of the word across every data frame that the word is seen in. \n",
        "\n",
        "  Example: The word 'book' is seen in dataframe(s) 1, 6, and 10. The respective sentiment scores are -1.2, 0.6, and 2.2. This word would be classified into the positive word lexicon because the mean of the score is .533, a positive number. "
      ],
      "metadata": {
        "id": "IThS2W5HupYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "same_wrds = set(pos_lexicon).intersection(neg_lexicon)      #get set of all words that appear in both psoitive and Negative Lexicon\n",
        "                                                            #https://stackoverflow.com/questions/1388818/how-can-i-compare-two-lists-in-python-and-return-matches\n",
        "\n",
        "word_vals_dict = dict.fromkeys(same_wrds, 0)                # Create a dictionary to hold of all words found in positive and negative lexicons     \n",
        "\n",
        "sentiment_vals2sum = []                                     # This list wil be iteritivley created in the loop below, it will stor the scores that will be averaged"
      ],
      "metadata": {
        "id": "I_E7nTOFlM1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(0, len(same_wrds)):    # In the set of words identified in positive and negative lexicon\n",
        "  \n",
        "  i = same_wrds.pop()                 # i will return one word from the set, then the following with each iteration\n",
        "  same_wrds.add(i)                    # https://stackoverflow.com/questions/59825/how-to-retrieve-an-element-from-a-set-without-removing-it\n",
        "\n",
        "  sentiment_vals2sum = []             # Will store the Sentiment Scores collected across data frames \n",
        "\n",
        "  for j in range(0, len(ls_df_names)):                    # in range of dataframes(44)\n",
        "    is_wrd_there = i in ls_df_names[j]['Word'].unique()   # Return true or false. True if desired word 'i' is in the data frame being checked false if not (ls_df_names[0], ls_df_names[1],...)\n",
        "    if is_wrd_there is True:                              # If true... \n",
        "\n",
        "      mask1 = ls_df_names[j]['Word'].values == i                  # Get the sentiment value of the word from its dataframe \n",
        "                                                                  # https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
        "                                                     \n",
        "      sentiment_vals2sum.append(ls_df_names[j][mask1].iat[0,1])   # append sentiment value to list where they are stored eg. sentiment_vals2sum \n",
        "      num_avg = mean(sentiment_vals2sum)                          # Take the mean of the sentiment scores collected in the list above(these are all sentiment scores for one word collected across dataframes where the word was found 'True')\n",
        "      word_vals_dict[i] = num_avg                                 # Give the word we are analyzing a value in the dict where all the words seen in positive and negative lexicon are being stored in rg. word_vals_dict\n",
        "\n",
        "# Allow about 8-10min for runtime (Sorry I am still somewhat new to pthon and don't know how to optimize nested loops)\n"
      ],
      "metadata": {
        "id": "skkNxxWxYBfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vals_dict       # See the average Sentiment Score for each of the words that were found in both the positive and negative lexicon "
      ],
      "metadata": {
        "id": "sIRBR1PiG1D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.2a Remove words that now have a clear positive or negative classification\n",
        "- Words that have a positive avg score are removed from negative lexicon and vice versa"
      ],
      "metadata": {
        "id": "jwd8K5Hky4J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w in word_vals_dict.items():        # .items() returns a tuple of (word, score). See Cell above for all words: scores in dict\n",
        "  if w[1] >= 0:                         # if w[1] (the score) is greater than 0...\n",
        "    neg_lexicon.remove(w[0])            # remove it from the negative lexicon\n",
        "  if w[1] < 0:                          # If w[1] (the score) is less than 0...\n",
        "    pos_lexicon.remove(w[0])            # remove the word from the positive lexicon"
      ],
      "metadata": {
        "id": "k3LInnUMkzpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Tokenize tweets from test data and count occurance of tokens in the positive and negative lexicons"
      ],
      "metadata": {
        "id": "tOOnDXhlOXQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_neg_count_func():\n",
        "  "
      ],
      "metadata": {
        "id": "MnvC7cNg6zWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce"
      ],
      "metadata": {
        "id": "FdHPvIogsqwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare + and - Lexicons with the tokens(in list form) from the tweet\n",
        "\n",
        "for i in range(0, len(df.index)):                      # In the range 0 to length of the tweets dataframe \n",
        "  tokens = df['TWEET'].values[i].lower().split()       # Create tokens for tweet and make them all lowercase (make lowercase to match the lexicons)\n",
        "  \n",
        "  wrd_mtches = reduce(lambda x, y: x + pos_lexicon.count(y), set(tokens), 0)        # wrd_mtches is set to be the number of words found in the tweet that are in the positive lexicon     \n",
        "  neg_wrd_mtch = reduce(lambda x, y: x + neg_lexicon.count(y), set(tokens), 0)      #neg_wrd_mtch is set to be the number of words founf in the tweet that are in the negative lexicon\n",
        "  \n",
        "  df['Count: Words in + Lexicon'].values[i] = wrd_mtches          # Place count values of found postivive words into dataframe of tweets at the correct index \n",
        "  df['Count: Words in - Lexicon'].values[i] = neg_wrd_mtch        # Place count values of found negative words into dataframe of tweets at the correct index\n",
        "\n",
        "  #Allow 8-12min fro runtime "
      ],
      "metadata": {
        "id": "_Ox2TzyKszOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.5* Does the tweet contain the word 'No'? "
      ],
      "metadata": {
        "id": "4EdKeKDC5E8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(df.index)):                       # In the range 0 to length of the tweets dataframe \n",
        "  tokens = df['TWEET'].values[i].lower().split()        # Tokenize and lowercase tweets \n",
        "  if 'no' in tokens:                                    # If no is in tweet dataframe value is 1 if not value is zero\n",
        "    df['Contain The word NO? '].values[i] = 1\n",
        "  else: \n",
        "    df['Contain The word NO? '].values[i] = 0\n"
      ],
      "metadata": {
        "id": "mTmx7oBm5OZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.6* Count 1st and 2nd person pronouns "
      ],
      "metadata": {
        "id": "rzGgePDVRPbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxW6_pQbnpsD",
        "outputId": "341a28e9-7144-48f5-92d1-ea311a8d476c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0, len(df.index)):\n",
        "  tokens = df['TWEET'].values[i].lower().split()\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "  noun_definitions = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "  count = 0\n",
        "\n",
        "  for j in range(0, len(tagged)):\n",
        "    if tagged[j][1] in noun_definitions:\n",
        "      count += 1\n",
        "  \n",
        "  df['Count: Nouns'].values[i] = count"
      ],
      "metadata": {
        "id": "zfdOUtODnB18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Ratio of Unique words to total words in tweet"
      ],
      "metadata": {
        "id": "C3GzWAOjGFhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(df.index)):                       \n",
        "  tokens = df['TWEET'].values[i].lower().split()\n",
        "  x = np.array(tokens)\n",
        "  ratio = len(np.unique(x)) / len(tokens)\n",
        "  df['Ratio: Unique Words-Total Words'].values[i] = ratio"
      ],
      "metadata": {
        "id": "ph09c9BxF-i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Ratio of stop words to total in tweet "
      ],
      "metadata": {
        "id": "-D3r7OnGONYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(df.index)):                       \n",
        "  tokens = df['TWEET'].values[i].lower().split()\n",
        "  x = np.array(tokens)\n",
        "  stop_wrds_count = [w for w in tokens if w in stop_words]\n",
        "  ratio = len(stop_wrds_count) / len(tokens)\n",
        "  df['Ratio: Stop Words-Total Words'].values[i] = ratio"
      ],
      "metadata": {
        "id": "J37TjYj2OTHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "4HM5mqp4TOFd",
        "outputId": "83b2b05e-6857-480b-d587-5a146ee23fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   TWEET  \\\n",
              "0      QT @user In the original draft of the 7th book...   \n",
              "1      Ben Smith / Smith (concussion) remains out of ...   \n",
              "2      Sorry bout the stream last night I crashed out...   \n",
              "3      Chase Headley's RBI double in the 8th inning o...   \n",
              "4      @user Alciato: Bee will invest 150 million in ...   \n",
              "...                                                  ...   \n",
              "36368  @user \\\"\"So amazing to have the beautiful Lady...   \n",
              "36369  9 September has arrived, which means Apple's n...   \n",
              "36370  Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...   \n",
              "36371  @user no I'm in hilton head till the 8th lol g...   \n",
              "36372  WASHINGTON (Reuters) - U.S. Vice President Joe...   \n",
              "\n",
              "       Count: Words in + Lexicon  Count: Words in - Lexicon  \\\n",
              "0                            7.0                        1.0   \n",
              "1                            5.0                        1.0   \n",
              "2                           15.0                        4.0   \n",
              "3                           10.0                       10.0   \n",
              "4                            8.0                        4.0   \n",
              "...                          ...                        ...   \n",
              "36368                        7.0                        2.0   \n",
              "36369                        8.0                        7.0   \n",
              "36370                        4.0                        3.0   \n",
              "36371                        7.0                        6.0   \n",
              "36372                        8.0                        5.0   \n",
              "\n",
              "       Contain The word NO?   Count: Nouns  Contain !  \\\n",
              "0                        0.0           8.0        NaN   \n",
              "1                        0.0           9.0        NaN   \n",
              "2                        0.0           9.0        NaN   \n",
              "3                        0.0           9.0        NaN   \n",
              "4                        0.0           6.0        NaN   \n",
              "...                      ...           ...        ...   \n",
              "36368                    0.0           5.0        NaN   \n",
              "36369                    0.0           4.0        NaN   \n",
              "36370                    0.0           7.0        NaN   \n",
              "36371                    1.0           7.0        NaN   \n",
              "36372                    0.0           9.0        NaN   \n",
              "\n",
              "       Ratio: Unique Words-Total Words  Ratio: Stop Words-Total Words  \\\n",
              "0                             0.833333                       0.333333   \n",
              "1                             0.928571                       0.214286   \n",
              "2                             1.000000                       0.458333   \n",
              "3                             1.000000                       0.260870   \n",
              "4                             0.952381                       0.333333   \n",
              "...                                ...                            ...   \n",
              "36368                         0.947368                       0.368421   \n",
              "36369                         1.000000                       0.368421   \n",
              "36370                         1.000000                       0.235294   \n",
              "36371                         1.000000                       0.285714   \n",
              "36372                         0.954545                       0.272727   \n",
              "\n",
              "       Count: Adjectives in + Lexicon  Count: Adjectives in - Lexicon  \\\n",
              "0                                 NaN                             NaN   \n",
              "1                                 NaN                             NaN   \n",
              "2                                 NaN                             NaN   \n",
              "3                                 NaN                             NaN   \n",
              "4                                 NaN                             NaN   \n",
              "...                               ...                             ...   \n",
              "36368                             NaN                             NaN   \n",
              "36369                             NaN                             NaN   \n",
              "36370                             NaN                             NaN   \n",
              "36371                             NaN                             NaN   \n",
              "36372                             NaN                             NaN   \n",
              "\n",
              "       Log: Tweet word count  Log: Length of Longest Word in Tweet  \\\n",
              "0                        NaN                                   NaN   \n",
              "1                        NaN                                   NaN   \n",
              "2                        NaN                                   NaN   \n",
              "3                        NaN                                   NaN   \n",
              "4                        NaN                                   NaN   \n",
              "...                      ...                                   ...   \n",
              "36368                    NaN                                   NaN   \n",
              "36369                    NaN                                   NaN   \n",
              "36370                    NaN                                   NaN   \n",
              "36371                    NaN                                   NaN   \n",
              "36372                    NaN                                   NaN   \n",
              "\n",
              "       Log: Count of Words with 5+ Characters  \n",
              "0                                         NaN  \n",
              "1                                         NaN  \n",
              "2                                         NaN  \n",
              "3                                         NaN  \n",
              "4                                         NaN  \n",
              "...                                       ...  \n",
              "36368                                     NaN  \n",
              "36369                                     NaN  \n",
              "36370                                     NaN  \n",
              "36371                                     NaN  \n",
              "36372                                     NaN  \n",
              "\n",
              "[36373 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-172b1b48-faa9-4298-a260-8040b06f57f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWEET</th>\n",
              "      <th>Count: Words in + Lexicon</th>\n",
              "      <th>Count: Words in - Lexicon</th>\n",
              "      <th>Contain The word NO?</th>\n",
              "      <th>Count: Nouns</th>\n",
              "      <th>Contain !</th>\n",
              "      <th>Ratio: Unique Words-Total Words</th>\n",
              "      <th>Ratio: Stop Words-Total Words</th>\n",
              "      <th>Count: Adjectives in + Lexicon</th>\n",
              "      <th>Count: Adjectives in - Lexicon</th>\n",
              "      <th>Log: Tweet word count</th>\n",
              "      <th>Log: Length of Longest Word in Tweet</th>\n",
              "      <th>Log: Count of Words with 5+ Characters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QT @user In the original draft of the 7th book...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ben Smith / Smith (concussion) remains out of ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sorry bout the stream last night I crashed out...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36368</th>\n",
              "      <td>@user \\\"\"So amazing to have the beautiful Lady...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36369</th>\n",
              "      <td>9 September has arrived, which means Apple's n...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36370</th>\n",
              "      <td>Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36371</th>\n",
              "      <td>@user no I'm in hilton head till the 8th lol g...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36372</th>\n",
              "      <td>WASHINGTON (Reuters) - U.S. Vice President Joe...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.954545</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36373 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-172b1b48-faa9-4298-a260-8040b06f57f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-172b1b48-faa9-4298-a260-8040b06f57f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-172b1b48-faa9-4298-a260-8040b06f57f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}