{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dldowning/Fall2022-5222/blob/main/All_Features_Downing_Roberts_5222_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "2Fg7eZRAlOgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y0gm-p1OwcOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06d48cb-fb86-4a10-ef7f-45aa6ab97310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 11.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=e87a92a90f659722029d381c7f409926040045c938680e2ee943d49875dd0b5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/e3/f2/1de1c2e3ed742e1df73e0f15d58864e50c7e64f607b548d6cf\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "%pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "import nltk\n",
        "from google.colab import files\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import requests\n",
        "import string\n",
        "import random\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "import emoji\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating dataframe"
      ],
      "metadata": {
        "id": "ki0KdvZFk91C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1ubIaCqJnOzG-m_ns7VmdV87Ecs349BbG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMr-smqMOwym",
        "outputId": "a9caf800-eb00-415b-c8fc-5650bbbd03a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ubIaCqJnOzG-m_ns7VmdV87Ecs349BbG\n",
            "To: /content/tweets_extracted.csv\n",
            "100% 18.3M/18.3M [00:00<00:00, 87.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://drive.google.com/uc?export=download&id=1C8ARH_yok3uOvirD_oKvgEmAR22SuC9R\"\n",
        "response = requests.get(URL)\n",
        "open(\"train_text_labels.csv\", \"wb\").write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpuC3U1RnsOJ",
        "outputId": "28459904-4ff1-43ab-bad6-fde0cff75458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5123208"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://drive.google.com/uc?export=download&id=1z0URnDJ8ck38mQ4CvHi5TUkly8e46glP\"\n",
        "response = requests.get(URL)\n",
        "open(\"test_text.txt\", \"wb\").write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv3I3RB99McR",
        "outputId": "ac6e1bde-56ab-4485-8c52-cf076a42e851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1156877"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://drive.google.com/uc?export=download&id=1xWQ2Lpf866Be4OR8J-cJHuY1S25dWppf\"\n",
        "response = requests.get(URL)\n",
        "open(\"test_labels.txt\", \"wb\").write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCzr8I-pCtIh",
        "outputId": "3ece4ed9-f7ef-4ed6-b062-e66b643da761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36850"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = pd.read_csv(\"/content/train_text_labels.csv\", header=None)\n",
        "df0.columns = ['Twitter', 'Label']\n",
        "len(df0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpaK7zlWlYbh",
        "outputId": "bca042c1-1325-4fdd-b41f-57802da2b523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45615"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = pd.read_csv(\"/content/test_text.txt\", sep=\"\\n\", header=None, quoting=csv.QUOTE_NONE)\n",
        "dft.columns = ['Twitter']\n",
        "len(dft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrWP7cBN67Gj",
        "outputId": "6767f552-ec3b-40a1-bf93-7ba7e8b4101b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12284"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dftl = pd.read_csv(\"/content/test_labels.txt\", sep=\"\\n\", header=None)\n",
        "dftl.columns = ['Label']\n",
        "len(dftl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAyC71rtA_xv",
        "outputId": "a902a4ac-79c7-4a42-adbf-1baa38ab41a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12284"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft['Label']=dftl['Label']\n",
        "dft.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4v2zs-HKFrQL",
        "outputId": "f84a2053-cb3c-4b04-d6da-70a6f4a92cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Twitter  Label\n",
              "0  @user @user what do these '1/2 naked pics' hav...      1\n",
              "1  OH: “I had a blue penis while I was this” [pla...      1\n",
              "2  @user @user That's coming, but I think the vic...      1\n",
              "3  I think I may be finally in with the in crowd ...      2\n",
              "4  @user Wow,first Hugo Chavez and now Fidel Cast...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d0cbf2b-6700-4813-b2d2-3d07407280f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Twitter</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OH: “I had a blue penis while I was this” [pla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user @user That's coming, but I think the vic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I think I may be finally in with the in crowd ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d0cbf2b-6700-4813-b2d2-3d07407280f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d0cbf2b-6700-4813-b2d2-3d07407280f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d0cbf2b-6700-4813-b2d2-3d07407280f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0['Label'].value_counts()\n",
        "#0\tnegative\n",
        "#1\tneutral\n",
        "#2\tpositive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peakGoADUHke",
        "outputId": "d5738b6b-565d-407b-bfc7-2aebfd39357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    20673\n",
              "2    17849\n",
              "0     7093\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the neutrals\n",
        "df = df0[df0['Label'] != 1]\n",
        "df.loc[df.Label == 2, 'Label'] = 1\n",
        "df=df.reset_index(drop=True)\n",
        "len(df)"
      ],
      "metadata": {
        "id": "47HFm_YlozWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c0780d-7cb8-422f-b937-5fa2e1938f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(loc, value, pi)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24942"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = dft[dft['Label'] != 1]\n",
        "dft=dft.reset_index(drop=True)\n",
        "dft.loc[dft.Label == 2, 'Label'] = 1\n",
        "len(df)\n",
        "len(dft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6dIwcfpF97G",
        "outputId": "66a052d2-f271-4b25-d57e-0232ce4f0a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6347"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_original_Tweets = df[['Twitter']].copy()\n",
        "dft_original_Tweets = dft[['Twitter']].copy()"
      ],
      "metadata": {
        "id": "Aan79blBBlii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweetcleanandtoke(dataframe,colname,newcolname):\n",
        "  stop = stopwords.words('english')\n",
        "  pattern_a = r'[^A-Za-z0-9]+' #non-alphanumeric\n",
        "  pattern_b = r'\\b\\w{1,1}\\b' #repeated words\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):                       \n",
        "    dataframe[colname].values[i] = dataframe[colname].values[i].lower()\n",
        "    dataframe[colname].values[i] = dataframe[colname].values[i].replace('@user', '')\n",
        "    dataframe[colname].values[i] = re.sub(emoji_pattern, '', dataframe[colname].values[i])\n",
        "    dataframe[colname].values[i] = re.sub(pattern_a, ' ', dataframe[colname].values[i])\n",
        "    dataframe[colname].values[i] = re.sub(pattern_b, '', dataframe[colname].values[i])\n",
        "    \n",
        "\n",
        "  dataframe[colname] = dataframe[colname].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "  rowname = dataframe[colname].name\n",
        "  dataframe[newcolname]=0\n",
        "  dataframe[newcolname] = dataframe.apply(lambda row: nltk.word_tokenize(row[rowname]), axis=1)\n",
        "\n",
        "  return dataframe\n",
        "  "
      ],
      "metadata": {
        "id": "rRU76z3L7Z_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=tweetcleanandtoke(df,'Twitter','Twitter_tokens')\n",
        "dft=tweetcleanandtoke(dft,'Twitter','Twitter_tokens')"
      ],
      "metadata": {
        "id": "uIgjtTsp9kUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Token_len']=df['Twitter_tokens'].apply(len)\n",
        "dft['Token_len']=dft['Twitter_tokens'].apply(len)\n",
        "\n"
      ],
      "metadata": {
        "id": "aoASGVI0tzhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the tokens for some EDA, also verifying I have no length 0\n",
        "list_of_lengths=[]\n",
        "less_than_9_len=[]\n",
        "for i in range (0, len(df)):\n",
        "  try:\n",
        "    list_of_lengths.append(len(df['Twitter_tokens'][i]));\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    if len(df['Twitter_tokens'][i]) < 9:\n",
        "      less_than_9_len.append(str(i));\n",
        "  except:\n",
        "    pass\n",
        "print(\"These stats are for the training set\")\n",
        "print(\"The min length is: \" + str(min(list_of_lengths)))\n",
        "print(\"The max length is: \" + str(max(list_of_lengths)))\n",
        "print(\"The mean length is: \" + str(mean(list_of_lengths)))\n",
        "print(\"The number of tweets with less than 9 tokens is: \" + str(len(less_than_9_len)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1ZY4K-nhkIQ",
        "outputId": "13addaa4-3214-4f0f-9d2a-e7c592404218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These stats are for the training set\n",
            "The min length is: 1\n",
            "The max length is: 26\n",
            "The mean length is: 11.502004650789832\n",
            "The number of tweets with less than 9 tokens is: 4506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_lengths=[]\n",
        "less_than_9_len=[]\n",
        "for i in range (0, len(dft)):\n",
        "  try:\n",
        "    list_of_lengths.append(len(dft['Twitter_tokens'][i]));\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    if len(dft['Twitter_tokens'][i]) < 9:\n",
        "      less_than_9_len.append(str(i));\n",
        "  except:\n",
        "    pass\n",
        "print(\"These stats are for the testing set\")\n",
        "print(\"The min test length is: \" + str(min(list_of_lengths)))\n",
        "print(\"The max test length is: \" + str(max(list_of_lengths)))\n",
        "print(\"The mean test length is: \" + str(mean(list_of_lengths)))\n",
        "print(\"The number of test tweets with less than 9 tokens is: \" + str(len(less_than_9_len)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHZq8kCiGd8k",
        "outputId": "e0215243-22e6-46fa-da11-65c46a038f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These stats are for the testing set\n",
            "The min test length is: 1\n",
            "The max test length is: 20\n",
            "The mean test length is: 9.237277453915235\n",
            "The number of test tweets with less than 9 tokens is: 2587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading and extracting TSVs"
      ],
      "metadata": {
        "id": "b7aHvxGWv6n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/socialsent/files/socialsent_hist_freq.zip\n",
        "!echo \"N\"| unzip /content/socialsent_hist_freq.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgfBvVXawvI6",
        "outputId": "4b586fc8-5729-498d-9b46-4e0de498c92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-21 07:39:09--  https://nlp.stanford.edu/projects/socialsent/files/socialsent_hist_freq.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 521987 (510K) [application/zip]\n",
            "Saving to: ‘socialsent_hist_freq.zip’\n",
            "\n",
            "socialsent_hist_fre 100%[===================>] 509.75K   259KB/s    in 2.0s    \n",
            "\n",
            "2022-11-21 07:39:11 (259 KB/s) - ‘socialsent_hist_freq.zip’ saved [521987/521987]\n",
            "\n",
            "Archive:  /content/socialsent_hist_freq.zip\n",
            "   creating: frequent_words/\n",
            "  inflating: frequent_words/1850.tsv  \n",
            "  inflating: frequent_words/1860.tsv  \n",
            "  inflating: frequent_words/1870.tsv  \n",
            "  inflating: frequent_words/1880.tsv  \n",
            "  inflating: frequent_words/1890.tsv  \n",
            "  inflating: frequent_words/1900.tsv  \n",
            "  inflating: frequent_words/1910.tsv  \n",
            "  inflating: frequent_words/1920.tsv  \n",
            "  inflating: frequent_words/1930.tsv  \n",
            "  inflating: frequent_words/1940.tsv  \n",
            "  inflating: frequent_words/1950.tsv  \n",
            "  inflating: frequent_words/1960.tsv  \n",
            "  inflating: frequent_words/1970.tsv  \n",
            "  inflating: frequent_words/1980.tsv  \n",
            "  inflating: frequent_words/1990.tsv  \n",
            "  inflating: frequent_words/2000.tsv  \n",
            "  inflating: frequent_words/README.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/frequent_words/\n",
            "  inflating: __MACOSX/frequent_words/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/socialsent/files/socialsent_hist_adj.zip\n",
        "!echo \"N\"| unzip /content/socialsent_hist_adj.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAN6g11dv_zh",
        "outputId": "72c1dc89-adba-48c3-ea6d-eaef73b11fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-21 07:39:12--  https://nlp.stanford.edu/projects/socialsent/files/socialsent_hist_adj.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 201855 (197K) [application/zip]\n",
            "Saving to: ‘socialsent_hist_adj.zip’\n",
            "\n",
            "socialsent_hist_adj 100%[===================>] 197.12K   185KB/s    in 1.1s    \n",
            "\n",
            "2022-11-21 07:39:13 (185 KB/s) - ‘socialsent_hist_adj.zip’ saved [201855/201855]\n",
            "\n",
            "Archive:  /content/socialsent_hist_adj.zip\n",
            "   creating: adjectives/\n",
            "  inflating: adjectives/1850.tsv     \n",
            "  inflating: adjectives/1860.tsv     \n",
            "  inflating: adjectives/1870.tsv     \n",
            "  inflating: adjectives/1880.tsv     \n",
            "  inflating: adjectives/1890.tsv     \n",
            "  inflating: adjectives/1900.tsv     \n",
            "  inflating: adjectives/1910.tsv     \n",
            "  inflating: adjectives/1920.tsv     \n",
            "  inflating: adjectives/1930.tsv     \n",
            "  inflating: adjectives/1940.tsv     \n",
            "  inflating: adjectives/1950.tsv     \n",
            "  inflating: adjectives/1960.tsv     \n",
            "  inflating: adjectives/1970.tsv     \n",
            "  inflating: adjectives/1980.tsv     \n",
            "  inflating: adjectives/1990.tsv     \n",
            "  inflating: adjectives/2000.tsv     \n",
            "  inflating: adjectives/README.txt   \n",
            "   creating: __MACOSX/adjectives/\n",
            "  inflating: __MACOSX/adjectives/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_2000adj = pd.read_csv(\"/content/adjectives/2000.tsv\", sep=\"\\t\", header=None)\n",
        "df_2000adj.columns = ['Word', 'Sentiment', 'Std']\n",
        "df_2000freq = pd.read_csv(\"/content/frequent_words/2000.tsv\", sep=\"\\t\", header=None)\n",
        "df_2000freq.columns = ['Word', 'Sentiment', 'Std']"
      ],
      "metadata": {
        "id": "EDpyKT1qwTT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/socialsent/files/socialsent_subreddits.zip\n",
        "!echo \"N\"| unzip /content/socialsent_subreddits.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqMTZ8gYRgHI",
        "outputId": "b08234dd-b64d-4a06-cc0e-05a3735a9569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-21 07:39:13--  https://nlp.stanford.edu/projects/socialsent/files/socialsent_subreddits.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15659374 (15M) [application/zip]\n",
            "Saving to: ‘socialsent_subreddits.zip’\n",
            "\n",
            "socialsent_subreddi 100%[===================>]  14.93M  4.40MB/s    in 6.4s    \n",
            "\n",
            "2022-11-21 07:39:20 (2.34 MB/s) - ‘socialsent_subreddits.zip’ saved [15659374/15659374]\n",
            "\n",
            "Archive:  /content/socialsent_subreddits.zip\n",
            "   creating: subreddits/\n",
            "  inflating: subreddits/.zip         \n",
            "  inflating: subreddits/2007scape.tsv  \n",
            "  inflating: subreddits/3DS.tsv      \n",
            "  inflating: subreddits/4chan.tsv    \n",
            "  inflating: subreddits/ACTrade.tsv  \n",
            "  inflating: subreddits/AdviceAnimals.tsv  \n",
            "  inflating: subreddits/amiugly.tsv  \n",
            "  inflating: subreddits/Anarcho_Capitalism.tsv  \n",
            "  inflating: subreddits/Android.tsv  \n",
            "  inflating: subreddits/anime.tsv    \n",
            "  inflating: subreddits/apple.tsv    \n",
            "  inflating: subreddits/archeage.tsv  \n",
            "  inflating: subreddits/AskMen.tsv   \n",
            "  inflating: subreddits/askscience.tsv  \n",
            "  inflating: subreddits/AskWomen.tsv  \n",
            "  inflating: subreddits/asoiaf.tsv   \n",
            "  inflating: subreddits/atheism.tsv  \n",
            "  inflating: subreddits/australia.tsv  \n",
            "  inflating: subreddits/aww.tsv      \n",
            "  inflating: subreddits/BabyBumps.tsv  \n",
            "  inflating: subreddits/baseball.tsv  \n",
            "  inflating: subreddits/battlefield_4.tsv  \n",
            "  inflating: subreddits/bestof.tsv   \n",
            "  inflating: subreddits/bicycling.tsv  \n",
            "  inflating: subreddits/BigBrother.tsv  \n",
            "  inflating: subreddits/Bitcoin.tsv  \n",
            "  inflating: subreddits/boardgames.tsv  \n",
            "  inflating: subreddits/bodybuilding.tsv  \n",
            "  inflating: subreddits/books.tsv    \n",
            "  inflating: subreddits/bravefrontier.tsv  \n",
            "  inflating: subreddits/britishproblems.tsv  \n",
            "  inflating: subreddits/buildapc.tsv  \n",
            "  inflating: subreddits/canada.tsv   \n",
            "  inflating: subreddits/cars.tsv     \n",
            "  inflating: subreddits/CasualConversation.tsv  \n",
            "  inflating: subreddits/casualiama.tsv  \n",
            "  inflating: subreddits/CasualPokemonTrades.tsv  \n",
            "  inflating: subreddits/CFB.tsv      \n",
            "  inflating: subreddits/changemyview.tsv  \n",
            "  inflating: subreddits/childfree.tsv  \n",
            "  inflating: subreddits/Christianity.tsv  \n",
            "  inflating: subreddits/cigars.tsv   \n",
            "  inflating: subreddits/circlejerk.tsv  \n",
            "  inflating: subreddits/civ.tsv      \n",
            "  inflating: subreddits/Civcraft.tsv  \n",
            "  inflating: subreddits/ClashOfClans.tsv  \n",
            "  inflating: subreddits/ClubTeen.tsv  \n",
            "  inflating: subreddits/CoDCompetitive.tsv  \n",
            "  inflating: subreddits/CollegeBasketball.tsv  \n",
            "  inflating: subreddits/comicbooks.tsv  \n",
            "  inflating: subreddits/conspiracy.tsv  \n",
            "  inflating: subreddits/Cricket.tsv  \n",
            "  inflating: subreddits/cringe.tsv   \n",
            "  inflating: subreddits/cringepics.tsv  \n",
            "  inflating: subreddits/csgobetting.tsv  \n",
            "  inflating: subreddits/DarkNetMarkets.tsv  \n",
            "  inflating: subreddits/darksouls.tsv  \n",
            "  inflating: subreddits/DarkSouls2.tsv  \n",
            "  inflating: subreddits/dataisbeautiful.tsv  \n",
            "  inflating: subreddits/dayz.tsv     \n",
            "  inflating: subreddits/DebateReligion.tsv  \n",
            "  inflating: subreddits/DestinyTheGame.tsv  \n",
            "  inflating: subreddits/Diablo.tsv   \n",
            "  inflating: subreddits/DIY.tsv      \n",
            "  inflating: subreddits/DnD.tsv      \n",
            "  inflating: subreddits/dogecoin.tsv  \n",
            "  inflating: subreddits/dogemarket.tsv  \n",
            "  inflating: subreddits/DotA2.tsv    \n",
            "  inflating: subreddits/dragonage.tsv  \n",
            "  inflating: subreddits/Drugs.tsv    \n",
            "  inflating: subreddits/elderscrollsonline.tsv  \n",
            "  inflating: subreddits/electronic_cigarette.tsv  \n",
            "  inflating: subreddits/europe.tsv   \n",
            "  inflating: subreddits/Eve.tsv      \n",
            "  inflating: subreddits/exmormon.tsv  \n",
            "  inflating: subreddits/explainlikeimfive.tsv  \n",
            "  inflating: subreddits/Fallout.tsv  \n",
            "  inflating: subreddits/fantasyfootball.tsv  \n",
            "  inflating: subreddits/fatlogic.tsv  \n",
            "  inflating: subreddits/fatpeoplehate.tsv  \n",
            "  inflating: subreddits/fatpeoplestories.tsv  \n",
            "  inflating: subreddits/ffxiv.tsv    \n",
            "  inflating: subreddits/FIFA.tsv     \n",
            "  inflating: subreddits/Fitness.tsv  \n",
            "  inflating: subreddits/food.tsv     \n",
            "  inflating: subreddits/formula1.tsv  \n",
            "  inflating: subreddits/friendsafari.tsv  \n",
            "  inflating: subreddits/Frozen.tsv   \n",
            "  inflating: subreddits/Frugal.tsv   \n",
            "  inflating: subreddits/funny.tsv    \n",
            "  inflating: subreddits/Futurology.tsv  \n",
            "  inflating: subreddits/GameDeals.tsv  \n",
            "  inflating: subreddits/gamegrumps.tsv  \n",
            "  inflating: subreddits/gameofthrones.tsv  \n",
            "  inflating: subreddits/Games.tsv    \n",
            "  inflating: subreddits/gaming.tsv   \n",
            "  inflating: subreddits/gaybros.tsv  \n",
            "  inflating: subreddits/gifs.tsv     \n",
            "  inflating: subreddits/GlobalOffensive.tsv  \n",
            "  inflating: subreddits/GlobalOffensiveTrade.tsv  \n",
            "  inflating: subreddits/gonewild.tsv  \n",
            "  inflating: subreddits/GrandTheftAutoV.tsv  \n",
            "  inflating: subreddits/Guildwars2.tsv  \n",
            "  inflating: subreddits/Guitar.tsv   \n",
            "  inflating: subreddits/Gunners.tsv  \n",
            "  inflating: subreddits/guns.tsv     \n",
            "  inflating: subreddits/halo.tsv     \n",
            "  inflating: subreddits/hearthstone.tsv  \n",
            "  inflating: subreddits/hiphopheads.tsv  \n",
            "  inflating: subreddits/hockey.tsv   \n",
            "  inflating: subreddits/Homebrewing.tsv  \n",
            "  inflating: subreddits/IAmA.tsv     \n",
            "  inflating: subreddits/ImGoingToHellForThis.tsv  \n",
            "  inflating: subreddits/india.tsv    \n",
            "  inflating: subreddits/ireland.tsv  \n",
            "  inflating: subreddits/jailbreak.tsv  \n",
            "  inflating: subreddits/Jokes.tsv    \n",
            "  inflating: subreddits/JusticePorn.tsv  \n",
            "  inflating: subreddits/Justrolledintotheshop.tsv  \n",
            "  inflating: subreddits/KerbalSpaceProgram.tsv  \n",
            "  inflating: subreddits/keto.tsv     \n",
            "  inflating: subreddits/KotakuInAction.tsv  \n",
            "  inflating: subreddits/leagueoflegends.tsv  \n",
            "  inflating: subreddits/legaladvice.tsv  \n",
            "  inflating: subreddits/lewronggeneration.tsv  \n",
            "  inflating: subreddits/Libertarian.tsv  \n",
            "  inflating: subreddits/LifeProTips.tsv  \n",
            "  inflating: subreddits/LiverpoolFC.tsv  \n",
            "  inflating: subreddits/longboarding.tsv  \n",
            "  inflating: subreddits/loseit.tsv   \n",
            "  inflating: subreddits/magicTCG.tsv  \n",
            "  inflating: subreddits/MakeupAddiction.tsv  \n",
            "  inflating: subreddits/malefashionadvice.tsv  \n",
            "  inflating: subreddits/MapPorn.tsv  \n",
            "  inflating: subreddits/MechanicalKeyboards.tsv  \n",
            "  inflating: subreddits/MensRights.tsv  \n",
            "  inflating: subreddits/mildlyinteresting.tsv  \n",
            "  inflating: subreddits/mindcrack.tsv  \n",
            "  inflating: subreddits/Minecraft.tsv  \n",
            "  inflating: subreddits/MLPLounge.tsv  \n",
            "  inflating: subreddits/MLS.tsv      \n",
            "  inflating: subreddits/MMA.tsv      \n",
            "  inflating: subreddits/motorcycles.tsv  \n",
            "  inflating: subreddits/movies.tsv   \n",
            "  inflating: subreddits/Music.tsv    \n",
            "  inflating: subreddits/mylittlepony.tsv  \n",
            "  inflating: subreddits/Naruto.tsv   \n",
            "  inflating: subreddits/NASCAR.tsv   \n",
            "  inflating: subreddits/nba.tsv      \n",
            "  inflating: subreddits/news.tsv     \n",
            "  inflating: subreddits/newzealand.tsv  \n",
            "  inflating: subreddits/nfl.tsv      \n",
            "  inflating: subreddits/NoFap.tsv    \n",
            "  inflating: subreddits/nosleep.tsv  \n",
            "  inflating: subreddits/nottheonion.tsv  \n",
            "  inflating: subreddits/oculus.tsv   \n",
            "  inflating: subreddits/offmychest.tsv  \n",
            "  inflating: subreddits/OkCupid.tsv  \n",
            "  inflating: subreddits/opiates.tsv  \n",
            "  inflating: subreddits/Parenting.tsv  \n",
            "  inflating: subreddits/pathofexile.tsv  \n",
            "  inflating: subreddits/pcgaming.tsv  \n",
            "  inflating: subreddits/pcmasterrace.tsv  \n",
            "  inflating: subreddits/PercyJacksonRP.tsv  \n",
            "  inflating: subreddits/personalfinance.tsv  \n",
            "  inflating: subreddits/photography.tsv  \n",
            "  inflating: subreddits/photoshopbattles.tsv  \n",
            "  inflating: subreddits/pics.tsv     \n",
            "  inflating: subreddits/Planetside.tsv  \n",
            "  inflating: subreddits/pokemon.tsv  \n",
            "  inflating: subreddits/Pokemongiveaway.tsv  \n",
            "  inflating: subreddits/PokemonPlaza.tsv  \n",
            "  inflating: subreddits/pokemontrades.tsv  \n",
            "  inflating: subreddits/polandball.tsv  \n",
            "  inflating: subreddits/politics.tsv  \n",
            "  inflating: subreddits/Portland.tsv  \n",
            "  inflating: subreddits/PotterPlayRP.tsv  \n",
            "  inflating: subreddits/programming.tsv  \n",
            "  inflating: subreddits/PS4.tsv      \n",
            "  inflating: subreddits/raisedbynarcissists.tsv  \n",
            "  inflating: subreddits/Random_Acts_Of_Amazon.tsv  \n",
            "  inflating: subreddits/RandomActsOfGaming.tsv  \n",
            "  inflating: subreddits/randomsuperpowers.tsv  \n",
            "  inflating: subreddits/Rateme.tsv   \n",
            "  inflating: subreddits/reactiongifs.tsv  \n",
            "  inflating: subreddits/README.txt   \n",
            "   creating: __MACOSX/subreddits/\n",
            "  inflating: __MACOSX/subreddits/._README.txt  \n",
            "  inflating: subreddits/reddevils.tsv  \n",
            "  inflating: subreddits/relationship_advice.tsv  \n",
            "  inflating: subreddits/relationships.tsv  \n",
            "  inflating: subreddits/roosterteeth.tsv  \n",
            "  inflating: subreddits/runescape.tsv  \n",
            "  inflating: subreddits/rupaulsdragrace.tsv  \n",
            "  inflating: subreddits/science.tsv  \n",
            "  inflating: subreddits/Seattle.tsv  \n",
            "  inflating: subreddits/sex.tsv      \n",
            "  inflating: subreddits/SFGiants.tsv  \n",
            "  inflating: subreddits/ShinyPokemon.tsv  \n",
            "  inflating: subreddits/Showerthoughts.tsv  \n",
            "  inflating: subreddits/SkincareAddiction.tsv  \n",
            "  inflating: subreddits/skyrim.tsv   \n",
            "  inflating: subreddits/smashbros.tsv  \n",
            "  inflating: subreddits/Smite.tsv    \n",
            "  inflating: subreddits/Sneakers.tsv  \n",
            "  inflating: subreddits/soccer.tsv   \n",
            "  inflating: subreddits/sports.tsv   \n",
            "  inflating: subreddits/SquaredCircle.tsv  \n",
            "  inflating: subreddits/starcitizen.tsv  \n",
            "  inflating: subreddits/starcraft.tsv  \n",
            "  inflating: subreddits/StarWars.tsv  \n",
            "  inflating: subreddits/Steam.tsv    \n",
            "  inflating: subreddits/SteamGameSwap.tsv  \n",
            "  inflating: subreddits/SubredditDrama.tsv  \n",
            "  inflating: subreddits/summonerschool.tsv  \n",
            "  inflating: subreddits/SVExchange.tsv  \n",
            "  inflating: subreddits/sysadmin.tsv  \n",
            "  inflating: subreddits/TalesFromRetail.tsv  \n",
            "  inflating: subreddits/talesfromtechsupport.tsv  \n",
            "  inflating: subreddits/technology.tsv  \n",
            "  inflating: subreddits/techsupport.tsv  \n",
            "  inflating: subreddits/teenagers.tsv  \n",
            "  inflating: subreddits/television.tsv  \n",
            "  inflating: subreddits/tf2.tsv      \n",
            "  inflating: subreddits/thatHappened.tsv  \n",
            "  inflating: subreddits/TheLastAirbender.tsv  \n",
            "  inflating: subreddits/TheRedPill.tsv  \n",
            "  inflating: subreddits/thewalkingdead.tsv  \n",
            "  inflating: subreddits/tifu.tsv     \n",
            "  inflating: subreddits/tipofmytongue.tsv  \n",
            "  inflating: subreddits/titanfall.tsv  \n",
            "  inflating: subreddits/todayilearned.tsv  \n",
            "  inflating: subreddits/toronto.tsv  \n",
            "  inflating: subreddits/trees.tsv    \n",
            "  inflating: subreddits/TrollXChromosomes.tsv  \n",
            "  inflating: subreddits/TumblrInAction.tsv  \n",
            "  inflating: subreddits/twitchplayspokemon.tsv  \n",
            "  inflating: subreddits/TwoXChromosomes.tsv  \n",
            "  inflating: subreddits/ultrahardcore.tsv  \n",
            "  inflating: subreddits/unitedkingdom.tsv  \n",
            "  inflating: subreddits/videos.tsv   \n",
            "  inflating: subreddits/Warthunder.tsv  \n",
            "  inflating: subreddits/whowouldwin.tsv  \n",
            "  inflating: subreddits/wiiu.tsv     \n",
            "  inflating: subreddits/WildStar.tsv  \n",
            "  inflating: subreddits/windowsphone.tsv  \n",
            "  inflating: subreddits/woahdude.tsv  \n",
            "  inflating: subreddits/worldnews.tsv  \n",
            "  inflating: subreddits/WorldofTanks.tsv  \n",
            "  inflating: subreddits/wow.tsv      \n",
            "  inflating: subreddits/WritingPrompts.tsv  \n",
            "  inflating: subreddits/WTF.tsv      \n",
            "  inflating: subreddits/xboxone.tsv  \n",
            "  inflating: subreddits/yugioh.tsv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_files = ['3DS.tsv', \n",
        "'4chan.tsv',\n",
        "'2007scape.tsv',\n",
        "'ACTrade.tsv',\n",
        "'amiugly.tsv',\n",
        "'BabyBumps.tsv',\n",
        "'baseball.tsv',\n",
        "'canada.tsv',\n",
        "'CasualConversation.tsv',\n",
        "'DarkNetMarkets.tsv',\n",
        "'darksouls.tsv',\n",
        "'elderscrollsonline.tsv',\n",
        "'Eve.tsv',\n",
        "'Fallout.tsv',\n",
        "'fantasyfootball.tsv',\n",
        "'GameDeals.tsv',\n",
        "'gamegrumps.tsv',\n",
        "'halo.tsv',\n",
        "'Homebrewing.tsv',\n",
        "'IAmA.tsv',\n",
        "'india.tsv',\n",
        "'jailbreak.tsv',\n",
        "'Jokes.tsv',\n",
        "'KerbalSpaceProgram.tsv',\n",
        "'keto.tsv',\n",
        "'leagueoflegends.tsv',\n",
        "'Libertarian.tsv',\n",
        "'magicTCG.tsv',\n",
        "'MakeupAddiction.tsv',\n",
        "'Naruto.tsv',\n",
        "'nba.tsv',\n",
        "'oculus.tsv',\n",
        "'OkCupid.tsv',\n",
        "'Parenting.tsv',\n",
        "'pathofexile.tsv',\n",
        "'raisedbynarcissists.tsv',\n",
        "'Random_Acts_Of_Amazon.tsv',\n",
        "'science.tsv',\n",
        "'Seattle.tsv',\n",
        "'TalesFromRetail.tsv',\n",
        "'talesfromtechsupport.tsv',\n",
        "'ultrahardcore.tsv',\n",
        "'videos.tsv',\n",
        "'Warthunder.tsv',\n",
        "'whowouldwin.tsv',\n",
        "'xboxone.tsv',\n",
        "'yugioh.tsv',\n",
        "]"
      ],
      "metadata": {
        "id": "MlpvD2rmurcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_dicts=[]\n",
        "for i in range(0,len(list_files)):\n",
        "    dfname='df_'+str(list_files[i])\n",
        "    dfname=dfname.replace('.tsv','')\n",
        "    path=\"/content/subreddits/\" + list_files[i]\n",
        "    dataframe = pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "    dataframe.columns = ['Word', 'Sentiment', 'Std']\n",
        "    dataframe=dataframe.drop(columns=['Std'])\n",
        "    list_dicts.append(dict(dataframe.values))"
      ],
      "metadata": {
        "id": "h9Txu7LuMK_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine dictionaries\n",
        "def dict_merger(dict1, dict2):\n",
        "    new_dict = {**dict1, **dict2}\n",
        "    return new_dict"
      ],
      "metadata": {
        "id": "PfBXyR8eHAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2000adj=df_2000adj.drop(columns='Std')\n",
        "feature1=dict(df_2000adj.values)\n",
        "df_2000freq=df_2000freq.drop(columns='Std')\n",
        "feature2=dict(df_2000freq.values)"
      ],
      "metadata": {
        "id": "VDw8BKZox-gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature3 = {}\n",
        "for i in range(0, 8):\n",
        "  feature3=dict_merger(feature3,list_dicts[i])\n",
        "\n",
        "feature4 = {}\n",
        "for i in range(8, 16):\n",
        "  feature4=dict_merger(feature4,list_dicts[i])\n",
        "\n",
        "feature5 = {}\n",
        "for i in range(16, 23):\n",
        "  feature5=dict_merger(feature5,list_dicts[i])\n",
        "\n",
        "feature6 = {}\n",
        "for i in range(23, 29):\n",
        "  feature6=dict_merger(feature6,list_dicts[i])\n",
        "\n",
        "feature7 = {}\n",
        "for i in range(29, 34):\n",
        "  feature7=dict_merger(feature7,list_dicts[i])\n",
        "\n",
        "feature8 = {}\n",
        "for i in range(34, 41):\n",
        "  feature8=dict_merger(feature8,list_dicts[i])\n",
        "\n",
        "feature9 = {}\n",
        "for i in range(41, 47):\n",
        "  feature9=dict_merger(feature9,list_dicts[i])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cb9HByXsIn9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting features"
      ],
      "metadata": {
        "id": "qh_n5XtflKiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one thing that boosted my scores was instead of taking one word per feature, I summed 9 words per feature\n",
        "#I took words at random and oversampled shorter tweets. boosted the F1 score by letting randomization try to hit a good token\n",
        "def lookups(row):\n",
        "  lookup_index=row.name\n",
        "  #print(\"This is the lookup_index: \" + str(lookup_index))\n",
        "  range_len=(df['Token_len'][lookup_index])\n",
        "  #print(\"This is the range_len: \" + str(range_len))\n",
        "  score=[]\n",
        "  for i in range(0, 9):\n",
        "    #print(\"Starting the for loop at: \" + str(i))\n",
        "    token=random.randint(0, range_len) \n",
        "    token -= 1\n",
        "    #print(\"The token is: \" + str(token))\n",
        "    word_tok=df.iloc[lookup_index, 2][token]\n",
        "    #print(\"The word_tok is: \" + str(word_tok))\n",
        "    try:\n",
        "      score.append(current_dict[word_tok])\n",
        "    except:\n",
        "      pass\n",
        "  try:\n",
        "      sum_score=sum(score)\n",
        "  except:\n",
        "      sum_score=0\n",
        "  return sum_score\n",
        "\n",
        "def lookupst(row):\n",
        "  lookup_index=row.name\n",
        "  #print(\"This is the lookup_index: \" + str(lookup_index))\n",
        "  range_len=(dft['Token_len'][lookup_index])\n",
        "  #print(\"This is the range_len: \" + str(range_len))\n",
        "  score=[]\n",
        "  for i in range(0, 9):\n",
        "    #print(\"Starting the for loop at: \" + str(i))\n",
        "    token=random.randint(0, range_len) \n",
        "    token -= 1\n",
        "    #print(\"The token is: \" + str(token))\n",
        "    word_tok=dft.iloc[lookup_index, 2][token]\n",
        "    #print(\"The word_tok is: \" + str(word_tok))\n",
        "    try:\n",
        "      score.append(current_dict[word_tok])\n",
        "    except:\n",
        "      pass\n",
        "  try:\n",
        "      sum_score=sum(score)\n",
        "  except:\n",
        "      sum_score=0\n",
        "  return sum_score\n",
        "\n",
        "def wordlengther(row):\n",
        "    index=row.name\n",
        "    token_list=df['Twitter_tokens'][index]\n",
        "    longest_word=1\n",
        "    for word in token_list:\n",
        "        if longest_word<len(word):\n",
        "          longest_word=len(word)\n",
        "    log_long=np.log10(longest_word)\n",
        "    return log_long\n",
        "\n",
        "def wordfiver(row):\n",
        "    index=row.name\n",
        "    token_list=df['Twitter_tokens'][index]\n",
        "    five_counts=1\n",
        "    for word in token_list:\n",
        "        if len(word)>=5:\n",
        "          five_counts+=1\n",
        "    log_five=np.log10(five_counts)\n",
        "    return log_five\n",
        "\n",
        "def wordlengthert(row):\n",
        "    index=row.name\n",
        "    token_list=dft['Twitter_tokens'][index]\n",
        "    longest_word=1\n",
        "    for word in token_list:\n",
        "        if longest_word<len(word):\n",
        "          longest_word=len(word)\n",
        "    log_long=np.log10(longest_word)\n",
        "    return log_long\n",
        "\n",
        "def wordfivert(row):\n",
        "    index=row.name\n",
        "    token_list=dft['Twitter_tokens'][index]\n",
        "    five_counts=1\n",
        "    for word in token_list:\n",
        "        if len(word)>=5:\n",
        "          five_counts+=1\n",
        "    log_five=np.log10(five_counts)\n",
        "    return log_five\n",
        "\n",
        "def mostpos(row):  #need the negative version too, mostneg\n",
        "    index=row.name\n",
        "    token_list=df['Twitter_tokens'][index]\n",
        "    max_value=0\n",
        "    for word in token_list:\n",
        "      try: pos_value=feature3[word]   #replace with super dictionary\n",
        "      except: pos_value=0\n",
        "      if max_value<pos_value:\n",
        "        max_value=pos_value\n",
        "    return max_value"
      ],
      "metadata": {
        "id": "4qSZDlp0N_9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))"
      ],
      "metadata": {
        "id": "BXNxohZHMo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['feature24']=0\n",
        "df['feature24']=df.apply(mostpos,axis=1)"
      ],
      "metadata": {
        "id": "Rk7jxparlsMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['feature1']=0\n",
        "df['feature2']=0\n",
        "df['feature3']=0\n",
        "df['feature4']=0\n",
        "df['feature5']=0\n",
        "df['feature6']=0\n",
        "df['feature7']=0\n",
        "df['feature8']=0\n",
        "df['feature9']=0\n",
        "df['feature10']=0\n",
        "df['feature11']=0\n",
        "df['feature12']=0\n"
      ],
      "metadata": {
        "id": "1ZGXMsI8Iu_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_list=['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9']\n",
        "for i in range(0, len(features_list)):\n",
        "  current_dict=list_dicts[i]\n",
        "  df[features_list[i]]=df.apply(lookups, axis=1)\n",
        "  dft[features_list[i]]=dft.apply(lookupst, axis=1)"
      ],
      "metadata": {
        "id": "-zF7OHmETgXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['feature10']=df['Token_len'].apply(np.log10)\n",
        "dft['feature10']=dft['Token_len'].apply(np.log10)\n",
        "df['feature11']=df.apply(wordlengther, axis=1)\n",
        "df['feature12']=df.apply(wordfiver, axis=1)\n",
        "dft['feature11']=dft.apply(wordlengthert, axis=1)\n",
        "dft['feature12']=dft.apply(wordfivert, axis=1)"
      ],
      "metadata": {
        "id": "j0nu5rs9y5gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['feature11']=df.apply(wordlengther, axis=1)\n",
        "df['feature12']=df.apply(wordfiver, axis=1)\n",
        "dft['feature11']=dft.apply(wordlengthert, axis=1)\n",
        "dft['feature12']=dft.apply(wordfivert, axis=1)"
      ],
      "metadata": {
        "id": "foYumH0w0CZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "HzK3S5OACivL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ee3ee31-8734-4f85-e68a-3999c9d7d62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Twitter  Label  \\\n",
              "0      qt original draft 7th book remus lupin survive...      1   \n",
              "1      alciato bee invest 150 million january another...      1   \n",
              "2      lit mum kerry louboutins wonder many willam ow...      1   \n",
              "3      soul train oct 27 halloween special ft dot fin...      1   \n",
              "4      disappointed wwe summerslam want see john cena...      0   \n",
              "...                                                  ...    ...   \n",
              "24937  michael good times 2nd greatest michael michae...      1   \n",
              "24938  think misguided girl vip tonight suckin dick w...      0   \n",
              "24939  amazing beautiful lady gaga show ac tonight lo...      1   \n",
              "24940  september arrived means apple new iphone hours...      1   \n",
              "24941  leeds sheff wed giuseppe bellusci securing luf...      1   \n",
              "\n",
              "                                          Twitter_tokens  Token_len  \\\n",
              "0      [qt, original, draft, 7th, book, remus, lupin,...         11   \n",
              "1      [alciato, bee, invest, 150, million, january, ...         13   \n",
              "2      [lit, mum, kerry, louboutins, wonder, many, wi...         12   \n",
              "3      [soul, train, oct, 27, halloween, special, ft,...         21   \n",
              "4      [disappointed, wwe, summerslam, want, see, joh...         10   \n",
              "...                                                  ...        ...   \n",
              "24937  [michael, good, times, 2nd, greatest, michael,...         10   \n",
              "24938  [think, misguided, girl, vip, tonight, suckin,...         14   \n",
              "24939  [amazing, beautiful, lady, gaga, show, ac, ton...         10   \n",
              "24940  [september, arrived, means, apple, new, iphone...         11   \n",
              "24941  [leeds, sheff, wed, giuseppe, bellusci, securi...         12   \n",
              "\n",
              "       feature24  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
              "0           1.39      1.42     -0.23     -0.43     -0.52      0.56     -0.31   \n",
              "1           3.63      5.64      2.17     -0.54      0.54      0.51      1.24   \n",
              "2           0.73      1.82     -2.73     -1.70     -0.09     -0.71      3.65   \n",
              "3           2.50      0.25      0.14      3.65      1.32     -2.67     -5.03   \n",
              "4           1.75      3.48      0.32     -3.09     -1.57     -1.32      2.60   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "24937       0.35      7.01      6.15     -5.01      3.37     -8.20      0.69   \n",
              "24938       2.70     -0.18      3.41      1.80      3.04      0.19     -0.77   \n",
              "24939       4.50      7.26      1.02      1.47      3.20      6.60     10.28   \n",
              "24940       0.87     -0.91      2.16     -3.69      2.20     -1.24      3.56   \n",
              "24941       0.52     -1.68      1.36      1.28      0.00      0.00     -0.21   \n",
              "\n",
              "       feature7  feature8  feature9  feature10  feature11  feature12  \n",
              "0          2.26      0.81      0.83   1.041393   1.361728   0.954243  \n",
              "1          0.57      5.02      1.29   1.113943   0.845098   1.000000  \n",
              "2          2.08     -0.08      0.11   1.079181   1.000000   0.903090  \n",
              "3          4.65      1.78      1.38   1.322219   0.954243   1.041393  \n",
              "4          0.46      0.74      4.52   1.000000   1.079181   0.602060  \n",
              "...         ...       ...       ...        ...        ...        ...  \n",
              "24937      4.21     -3.92      7.93   1.000000   0.903090   0.903090  \n",
              "24938      0.61      6.33     -0.25   1.146128   1.176091   0.903090  \n",
              "24939      1.18      5.40      1.44   1.000000   0.954243   0.698970  \n",
              "24940      0.14     -3.06      1.31   1.041393   0.954243   1.000000  \n",
              "24941     -0.47      1.02      1.09   1.079181   0.903090   1.041393  \n",
              "\n",
              "[24942 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ff28651-e3f9-4511-8b92-5b6fb592aad6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Twitter</th>\n",
              "      <th>Label</th>\n",
              "      <th>Twitter_tokens</th>\n",
              "      <th>Token_len</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>qt original draft 7th book remus lupin survive...</td>\n",
              "      <td>1</td>\n",
              "      <td>[qt, original, draft, 7th, book, remus, lupin,...</td>\n",
              "      <td>11</td>\n",
              "      <td>1.39</td>\n",
              "      <td>1.42</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>2.26</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>1.361728</td>\n",
              "      <td>0.954243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>alciato bee invest 150 million january another...</td>\n",
              "      <td>1</td>\n",
              "      <td>[alciato, bee, invest, 150, million, january, ...</td>\n",
              "      <td>13</td>\n",
              "      <td>3.63</td>\n",
              "      <td>5.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>-0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.57</td>\n",
              "      <td>5.02</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lit mum kerry louboutins wonder many willam ow...</td>\n",
              "      <td>1</td>\n",
              "      <td>[lit, mum, kerry, louboutins, wonder, many, wi...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1.82</td>\n",
              "      <td>-2.73</td>\n",
              "      <td>-1.70</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>3.65</td>\n",
              "      <td>2.08</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>soul train oct 27 halloween special ft dot fin...</td>\n",
              "      <td>1</td>\n",
              "      <td>[soul, train, oct, 27, halloween, special, ft,...</td>\n",
              "      <td>21</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.14</td>\n",
              "      <td>3.65</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-5.03</td>\n",
              "      <td>4.65</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.38</td>\n",
              "      <td>1.322219</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.041393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>disappointed wwe summerslam want see john cena...</td>\n",
              "      <td>0</td>\n",
              "      <td>[disappointed, wwe, summerslam, want, see, joh...</td>\n",
              "      <td>10</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.48</td>\n",
              "      <td>0.32</td>\n",
              "      <td>-3.09</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.74</td>\n",
              "      <td>4.52</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24937</th>\n",
              "      <td>michael good times 2nd greatest michael michae...</td>\n",
              "      <td>1</td>\n",
              "      <td>[michael, good, times, 2nd, greatest, michael,...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.35</td>\n",
              "      <td>7.01</td>\n",
              "      <td>6.15</td>\n",
              "      <td>-5.01</td>\n",
              "      <td>3.37</td>\n",
              "      <td>-8.20</td>\n",
              "      <td>0.69</td>\n",
              "      <td>4.21</td>\n",
              "      <td>-3.92</td>\n",
              "      <td>7.93</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24938</th>\n",
              "      <td>think misguided girl vip tonight suckin dick w...</td>\n",
              "      <td>0</td>\n",
              "      <td>[think, misguided, girl, vip, tonight, suckin,...</td>\n",
              "      <td>14</td>\n",
              "      <td>2.70</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>3.41</td>\n",
              "      <td>1.80</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>0.61</td>\n",
              "      <td>6.33</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>1.146128</td>\n",
              "      <td>1.176091</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24939</th>\n",
              "      <td>amazing beautiful lady gaga show ac tonight lo...</td>\n",
              "      <td>1</td>\n",
              "      <td>[amazing, beautiful, lady, gaga, show, ac, ton...</td>\n",
              "      <td>10</td>\n",
              "      <td>4.50</td>\n",
              "      <td>7.26</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.47</td>\n",
              "      <td>3.20</td>\n",
              "      <td>6.60</td>\n",
              "      <td>10.28</td>\n",
              "      <td>1.18</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24940</th>\n",
              "      <td>september arrived means apple new iphone hours...</td>\n",
              "      <td>1</td>\n",
              "      <td>[september, arrived, means, apple, new, iphone...</td>\n",
              "      <td>11</td>\n",
              "      <td>0.87</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>2.16</td>\n",
              "      <td>-3.69</td>\n",
              "      <td>2.20</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>3.56</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-3.06</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24941</th>\n",
              "      <td>leeds sheff wed giuseppe bellusci securing luf...</td>\n",
              "      <td>1</td>\n",
              "      <td>[leeds, sheff, wed, giuseppe, bellusci, securi...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.52</td>\n",
              "      <td>-1.68</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.09</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>1.041393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24942 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ff28651-e3f9-4511-8b92-5b6fb592aad6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ff28651-e3f9-4511-8b92-5b6fb592aad6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ff28651-e3f9-4511-8b92-5b6fb592aad6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft"
      ],
      "metadata": {
        "id": "Izz5rSqECkbS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "0fc66f2f-828b-4cf6-9b9a-5c51abd11cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Twitter  Label  \\\n",
              "0     think may finally crowd mannequinchallenge gra...      1   \n",
              "1     wow first hugo chavez fidel castro danny glove...      0   \n",
              "2     twitter thankyouobama shows heartfelt gratitud...      1   \n",
              "3     take away illegals dead people trump wins popu...      0   \n",
              "4            onedirection harrystyles cute little dance      1   \n",
              "...                                                 ...    ...   \n",
              "6342  al crying middle america left twice much votin...      0   \n",
              "6343  even catholic pope francis dude like need hug ...      1   \n",
              "6344     looks like flynn pleased blocked blocked flynn      0   \n",
              "6345  trying conversation dad vegetarianism pointles...      0   \n",
              "6346           stand guy gentleman vice president pence      1   \n",
              "\n",
              "                                         Twitter_tokens  Token_len  feature1  \\\n",
              "0     [think, may, finally, crowd, mannequinchalleng...          6      2.12   \n",
              "1     [wow, first, hugo, chavez, fidel, castro, dann...         16      0.86   \n",
              "2     [twitter, thankyouobama, shows, heartfelt, gra...          6      0.15   \n",
              "3     [take, away, illegals, dead, people, trump, wi...          9      1.83   \n",
              "4      [onedirection, harrystyles, cute, little, dance]          5      2.27   \n",
              "...                                                 ...        ...       ...   \n",
              "6342  [al, crying, middle, america, left, twice, muc...          9     -3.80   \n",
              "6343  [even, catholic, pope, francis, dude, like, ne...         11      0.81   \n",
              "6344  [looks, like, flynn, pleased, blocked, blocked...          7      3.95   \n",
              "6345  [trying, conversation, dad, vegetarianism, poi...          9     -4.52   \n",
              "6346    [stand, guy, gentleman, vice, president, pence]          6     -1.20   \n",
              "\n",
              "      feature2  feature3  feature4  feature5  feature6  feature7  feature8  \\\n",
              "0         0.34     -1.86      4.06      1.51     -0.50      1.11     -3.49   \n",
              "1        -0.25     -0.92      1.64     -0.08     -0.89      4.83     -0.64   \n",
              "2         4.38     -0.12     -0.29      0.00      3.32      0.19      1.49   \n",
              "3        -1.47     -4.64     -0.66     -1.73      6.63     -2.78     -3.25   \n",
              "4         2.74      4.05      2.20      5.18     13.63      9.97     -0.87   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "6342     -4.81     -4.15    -22.27     -0.25     -3.59      1.31     -6.65   \n",
              "6343     -1.90      7.58      3.85     -0.53      0.88      4.31     -2.74   \n",
              "6344     -0.52     -0.71     -1.26      0.38      0.00      2.46      2.67   \n",
              "6345     -0.96     -5.11     -2.74     -1.35      0.14      2.15     -2.32   \n",
              "6346      0.03     -3.22      2.91      2.31      1.54      4.09     -1.04   \n",
              "\n",
              "      feature9  feature10  feature11  feature12  \n",
              "0        -1.26   0.778151   1.255273   0.778151  \n",
              "1         0.96   1.204120   0.845098   1.113943  \n",
              "2         1.88   0.778151   1.113943   0.845098  \n",
              "3         0.84   0.954243   0.903090   0.698970  \n",
              "4         6.54   0.698970   1.079181   0.698970  \n",
              "...        ...        ...        ...        ...  \n",
              "6342     -1.44   0.954243   0.845098   0.845098  \n",
              "6343      3.30   1.041393   1.000000   0.602060  \n",
              "6344      8.81   0.845098   0.845098   0.845098  \n",
              "6345      1.94   0.954243   1.113943   0.903090  \n",
              "6346     -3.60   0.778151   0.954243   0.698970  \n",
              "\n",
              "[6347 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3114e9fd-ce6a-43c9-80a6-fcb6d0b229c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Twitter</th>\n",
              "      <th>Label</th>\n",
              "      <th>Twitter_tokens</th>\n",
              "      <th>Token_len</th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>think may finally crowd mannequinchallenge gra...</td>\n",
              "      <td>1</td>\n",
              "      <td>[think, may, finally, crowd, mannequinchalleng...</td>\n",
              "      <td>6</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.34</td>\n",
              "      <td>-1.86</td>\n",
              "      <td>4.06</td>\n",
              "      <td>1.51</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.11</td>\n",
              "      <td>-3.49</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>1.255273</td>\n",
              "      <td>0.778151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wow first hugo chavez fidel castro danny glove...</td>\n",
              "      <td>0</td>\n",
              "      <td>[wow, first, hugo, chavez, fidel, castro, dann...</td>\n",
              "      <td>16</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.92</td>\n",
              "      <td>1.64</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>4.83</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.204120</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>1.113943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>twitter thankyouobama shows heartfelt gratitud...</td>\n",
              "      <td>1</td>\n",
              "      <td>[twitter, thankyouobama, shows, heartfelt, gra...</td>\n",
              "      <td>6</td>\n",
              "      <td>0.15</td>\n",
              "      <td>4.38</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>take away illegals dead people trump wins popu...</td>\n",
              "      <td>0</td>\n",
              "      <td>[take, away, illegals, dead, people, trump, wi...</td>\n",
              "      <td>9</td>\n",
              "      <td>1.83</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-4.64</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>-1.73</td>\n",
              "      <td>6.63</td>\n",
              "      <td>-2.78</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>onedirection harrystyles cute little dance</td>\n",
              "      <td>1</td>\n",
              "      <td>[onedirection, harrystyles, cute, little, dance]</td>\n",
              "      <td>5</td>\n",
              "      <td>2.27</td>\n",
              "      <td>2.74</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.20</td>\n",
              "      <td>5.18</td>\n",
              "      <td>13.63</td>\n",
              "      <td>9.97</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>6.54</td>\n",
              "      <td>0.698970</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6342</th>\n",
              "      <td>al crying middle america left twice much votin...</td>\n",
              "      <td>0</td>\n",
              "      <td>[al, crying, middle, america, left, twice, muc...</td>\n",
              "      <td>9</td>\n",
              "      <td>-3.80</td>\n",
              "      <td>-4.81</td>\n",
              "      <td>-4.15</td>\n",
              "      <td>-22.27</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-3.59</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-6.65</td>\n",
              "      <td>-1.44</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6343</th>\n",
              "      <td>even catholic pope francis dude like need hug ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[even, catholic, pope, francis, dude, like, ne...</td>\n",
              "      <td>11</td>\n",
              "      <td>0.81</td>\n",
              "      <td>-1.90</td>\n",
              "      <td>7.58</td>\n",
              "      <td>3.85</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4.31</td>\n",
              "      <td>-2.74</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6344</th>\n",
              "      <td>looks like flynn pleased blocked blocked flynn</td>\n",
              "      <td>0</td>\n",
              "      <td>[looks, like, flynn, pleased, blocked, blocked...</td>\n",
              "      <td>7</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.46</td>\n",
              "      <td>2.67</td>\n",
              "      <td>8.81</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345</th>\n",
              "      <td>trying conversation dad vegetarianism pointles...</td>\n",
              "      <td>0</td>\n",
              "      <td>[trying, conversation, dad, vegetarianism, poi...</td>\n",
              "      <td>9</td>\n",
              "      <td>-4.52</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>-5.11</td>\n",
              "      <td>-2.74</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.15</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6346</th>\n",
              "      <td>stand guy gentleman vice president pence</td>\n",
              "      <td>1</td>\n",
              "      <td>[stand, guy, gentleman, vice, president, pence]</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-3.22</td>\n",
              "      <td>2.91</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.54</td>\n",
              "      <td>4.09</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-3.60</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6347 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3114e9fd-ce6a-43c9-80a6-fcb6d0b229c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3114e9fd-ce6a-43c9-80a6-fcb6d0b229c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3114e9fd-ce6a-43c9-80a6-fcb6d0b229c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_original_Tweets.columns = ['Original Tweet']\n",
        "df['Original Tweet'] = df_original_Tweets['Original Tweet']\n",
        "\n",
        "first_column = df.pop('Original Tweet')\n",
        "# insert column using insert(position,column_name,first_column) function\n",
        "df.insert(0, 'Original Tweet', first_column)"
      ],
      "metadata": {
        "id": "6Eure_atCoVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft_original_Tweets.columns = ['Original Tweet']\n",
        "dft['Original Tweet'] = dft_original_Tweets['Original Tweet']\n",
        "\n",
        "first_column = dft.pop('Original Tweet')\n",
        "# insert column using insert(position,column_name,first_column) function\n",
        "dft.insert(0, 'Original Tweet', first_column)"
      ],
      "metadata": {
        "id": "nMS3zf_RDNbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GsEodv-cFUiP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c91be564-045c-43b3-a555-902c67e1928f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Original Tweet  \\\n",
              "0      QT @user In the original draft of the 7th book...   \n",
              "1      @user Alciato: Bee will invest 150 million in ...   \n",
              "2      @user LIT MY MUM 'Kerry the louboutins I wonde...   \n",
              "3      \\\"\" SOUL TRAIN\\\"\" OCT 27 HALLOWEEN SPECIAL ft ...   \n",
              "4      So disappointed in wwe summerslam! I want to s...   \n",
              "...                                                  ...   \n",
              "24937  Michael from Good Times is the 2nd greatest Mi...   \n",
              "24938  Just think some misguided girl is in the VIP t...   \n",
              "24939  @user \\\"\"So amazing to have the beautiful Lady...   \n",
              "24940  9 September has arrived, which means Apple's n...   \n",
              "24941  Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...   \n",
              "\n",
              "                                                 Twitter  Label  \\\n",
              "0      qt original draft 7th book remus lupin survive...      1   \n",
              "1      alciato bee invest 150 million january another...      1   \n",
              "2      lit mum kerry louboutins wonder many willam ow...      1   \n",
              "3      soul train oct 27 halloween special ft dot fin...      1   \n",
              "4      disappointed wwe summerslam want see john cena...      0   \n",
              "...                                                  ...    ...   \n",
              "24937  michael good times 2nd greatest michael michae...      1   \n",
              "24938  think misguided girl vip tonight suckin dick w...      0   \n",
              "24939  amazing beautiful lady gaga show ac tonight lo...      1   \n",
              "24940  september arrived means apple new iphone hours...      1   \n",
              "24941  leeds sheff wed giuseppe bellusci securing luf...      1   \n",
              "\n",
              "                                          Twitter_tokens  Token_len  \\\n",
              "0      [qt, original, draft, 7th, book, remus, lupin,...         11   \n",
              "1      [alciato, bee, invest, 150, million, january, ...         13   \n",
              "2      [lit, mum, kerry, louboutins, wonder, many, wi...         12   \n",
              "3      [soul, train, oct, 27, halloween, special, ft,...         21   \n",
              "4      [disappointed, wwe, summerslam, want, see, joh...         10   \n",
              "...                                                  ...        ...   \n",
              "24937  [michael, good, times, 2nd, greatest, michael,...         10   \n",
              "24938  [think, misguided, girl, vip, tonight, suckin,...         14   \n",
              "24939  [amazing, beautiful, lady, gaga, show, ac, ton...         10   \n",
              "24940  [september, arrived, means, apple, new, iphone...         11   \n",
              "24941  [leeds, sheff, wed, giuseppe, bellusci, securi...         12   \n",
              "\n",
              "       feature24  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
              "0           1.39      1.42     -0.23     -0.43     -0.52      0.56     -0.31   \n",
              "1           3.63      5.64      2.17     -0.54      0.54      0.51      1.24   \n",
              "2           0.73      1.82     -2.73     -1.70     -0.09     -0.71      3.65   \n",
              "3           2.50      0.25      0.14      3.65      1.32     -2.67     -5.03   \n",
              "4           1.75      3.48      0.32     -3.09     -1.57     -1.32      2.60   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "24937       0.35      7.01      6.15     -5.01      3.37     -8.20      0.69   \n",
              "24938       2.70     -0.18      3.41      1.80      3.04      0.19     -0.77   \n",
              "24939       4.50      7.26      1.02      1.47      3.20      6.60     10.28   \n",
              "24940       0.87     -0.91      2.16     -3.69      2.20     -1.24      3.56   \n",
              "24941       0.52     -1.68      1.36      1.28      0.00      0.00     -0.21   \n",
              "\n",
              "       feature7  feature8  feature9  feature10  feature11  feature12  \n",
              "0          2.26      0.81      0.83   1.041393   1.361728   0.954243  \n",
              "1          0.57      5.02      1.29   1.113943   0.845098   1.000000  \n",
              "2          2.08     -0.08      0.11   1.079181   1.000000   0.903090  \n",
              "3          4.65      1.78      1.38   1.322219   0.954243   1.041393  \n",
              "4          0.46      0.74      4.52   1.000000   1.079181   0.602060  \n",
              "...         ...       ...       ...        ...        ...        ...  \n",
              "24937      4.21     -3.92      7.93   1.000000   0.903090   0.903090  \n",
              "24938      0.61      6.33     -0.25   1.146128   1.176091   0.903090  \n",
              "24939      1.18      5.40      1.44   1.000000   0.954243   0.698970  \n",
              "24940      0.14     -3.06      1.31   1.041393   0.954243   1.000000  \n",
              "24941     -0.47      1.02      1.09   1.079181   0.903090   1.041393  \n",
              "\n",
              "[24942 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c6d1e53-4817-420a-80d4-d8100938c22e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Tweet</th>\n",
              "      <th>Twitter</th>\n",
              "      <th>Label</th>\n",
              "      <th>Twitter_tokens</th>\n",
              "      <th>Token_len</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QT @user In the original draft of the 7th book...</td>\n",
              "      <td>qt original draft 7th book remus lupin survive...</td>\n",
              "      <td>1</td>\n",
              "      <td>[qt, original, draft, 7th, book, remus, lupin,...</td>\n",
              "      <td>11</td>\n",
              "      <td>1.39</td>\n",
              "      <td>1.42</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>2.26</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>1.361728</td>\n",
              "      <td>0.954243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
              "      <td>alciato bee invest 150 million january another...</td>\n",
              "      <td>1</td>\n",
              "      <td>[alciato, bee, invest, 150, million, january, ...</td>\n",
              "      <td>13</td>\n",
              "      <td>3.63</td>\n",
              "      <td>5.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>-0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.57</td>\n",
              "      <td>5.02</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user LIT MY MUM 'Kerry the louboutins I wonde...</td>\n",
              "      <td>lit mum kerry louboutins wonder many willam ow...</td>\n",
              "      <td>1</td>\n",
              "      <td>[lit, mum, kerry, louboutins, wonder, many, wi...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1.82</td>\n",
              "      <td>-2.73</td>\n",
              "      <td>-1.70</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>3.65</td>\n",
              "      <td>2.08</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\\"\" SOUL TRAIN\\\"\" OCT 27 HALLOWEEN SPECIAL ft ...</td>\n",
              "      <td>soul train oct 27 halloween special ft dot fin...</td>\n",
              "      <td>1</td>\n",
              "      <td>[soul, train, oct, 27, halloween, special, ft,...</td>\n",
              "      <td>21</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.14</td>\n",
              "      <td>3.65</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-5.03</td>\n",
              "      <td>4.65</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.38</td>\n",
              "      <td>1.322219</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.041393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>So disappointed in wwe summerslam! I want to s...</td>\n",
              "      <td>disappointed wwe summerslam want see john cena...</td>\n",
              "      <td>0</td>\n",
              "      <td>[disappointed, wwe, summerslam, want, see, joh...</td>\n",
              "      <td>10</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.48</td>\n",
              "      <td>0.32</td>\n",
              "      <td>-3.09</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.74</td>\n",
              "      <td>4.52</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24937</th>\n",
              "      <td>Michael from Good Times is the 2nd greatest Mi...</td>\n",
              "      <td>michael good times 2nd greatest michael michae...</td>\n",
              "      <td>1</td>\n",
              "      <td>[michael, good, times, 2nd, greatest, michael,...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.35</td>\n",
              "      <td>7.01</td>\n",
              "      <td>6.15</td>\n",
              "      <td>-5.01</td>\n",
              "      <td>3.37</td>\n",
              "      <td>-8.20</td>\n",
              "      <td>0.69</td>\n",
              "      <td>4.21</td>\n",
              "      <td>-3.92</td>\n",
              "      <td>7.93</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24938</th>\n",
              "      <td>Just think some misguided girl is in the VIP t...</td>\n",
              "      <td>think misguided girl vip tonight suckin dick w...</td>\n",
              "      <td>0</td>\n",
              "      <td>[think, misguided, girl, vip, tonight, suckin,...</td>\n",
              "      <td>14</td>\n",
              "      <td>2.70</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>3.41</td>\n",
              "      <td>1.80</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>0.61</td>\n",
              "      <td>6.33</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>1.146128</td>\n",
              "      <td>1.176091</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24939</th>\n",
              "      <td>@user \\\"\"So amazing to have the beautiful Lady...</td>\n",
              "      <td>amazing beautiful lady gaga show ac tonight lo...</td>\n",
              "      <td>1</td>\n",
              "      <td>[amazing, beautiful, lady, gaga, show, ac, ton...</td>\n",
              "      <td>10</td>\n",
              "      <td>4.50</td>\n",
              "      <td>7.26</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.47</td>\n",
              "      <td>3.20</td>\n",
              "      <td>6.60</td>\n",
              "      <td>10.28</td>\n",
              "      <td>1.18</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24940</th>\n",
              "      <td>9 September has arrived, which means Apple's n...</td>\n",
              "      <td>september arrived means apple new iphone hours...</td>\n",
              "      <td>1</td>\n",
              "      <td>[september, arrived, means, apple, new, iphone...</td>\n",
              "      <td>11</td>\n",
              "      <td>0.87</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>2.16</td>\n",
              "      <td>-3.69</td>\n",
              "      <td>2.20</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>3.56</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-3.06</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24941</th>\n",
              "      <td>Leeds 1-1 Sheff Wed. Giuseppe Bellusci securin...</td>\n",
              "      <td>leeds sheff wed giuseppe bellusci securing luf...</td>\n",
              "      <td>1</td>\n",
              "      <td>[leeds, sheff, wed, giuseppe, bellusci, securi...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.52</td>\n",
              "      <td>-1.68</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.09</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>1.041393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24942 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c6d1e53-4817-420a-80d4-d8100938c22e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c6d1e53-4817-420a-80d4-d8100938c22e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c6d1e53-4817-420a-80d4-d8100938c22e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft"
      ],
      "metadata": {
        "id": "sT1Pcj0dFZVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "1823fa9a-26ae-437b-bce2-b0a011d5049b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Original Tweet  \\\n",
              "0     I think I may be finally in with the in crowd ...   \n",
              "1     @user Wow,first Hugo Chavez and now Fidel Cast...   \n",
              "2     Twitter's #ThankYouObama Shows Heartfelt Grati...   \n",
              "3     @user @user @user @user @user @user take away ...   \n",
              "4       #onedirection #harrystyles cute little dance 😉    \n",
              "...                                                 ...   \n",
              "6342  @user for al the crying you do about how middl...   \n",
              "6343  i'm not even catholic, but pope francis is my ...   \n",
              "6344  @user @user @user Looks like Flynn isn't too p...   \n",
              "6345  Trying to have a conversation with my dad abou...   \n",
              "6346  @user You are a stand up guy and a Gentleman V...   \n",
              "\n",
              "                                                Twitter  Label  \\\n",
              "0     think may finally crowd mannequinchallenge gra...      1   \n",
              "1     wow first hugo chavez fidel castro danny glove...      0   \n",
              "2     twitter thankyouobama shows heartfelt gratitud...      1   \n",
              "3     take away illegals dead people trump wins popu...      0   \n",
              "4            onedirection harrystyles cute little dance      1   \n",
              "...                                                 ...    ...   \n",
              "6342  al crying middle america left twice much votin...      0   \n",
              "6343  even catholic pope francis dude like need hug ...      1   \n",
              "6344     looks like flynn pleased blocked blocked flynn      0   \n",
              "6345  trying conversation dad vegetarianism pointles...      0   \n",
              "6346           stand guy gentleman vice president pence      1   \n",
              "\n",
              "                                         Twitter_tokens  Token_len  feature1  \\\n",
              "0     [think, may, finally, crowd, mannequinchalleng...          6      2.12   \n",
              "1     [wow, first, hugo, chavez, fidel, castro, dann...         16      0.86   \n",
              "2     [twitter, thankyouobama, shows, heartfelt, gra...          6      0.15   \n",
              "3     [take, away, illegals, dead, people, trump, wi...          9      1.83   \n",
              "4      [onedirection, harrystyles, cute, little, dance]          5      2.27   \n",
              "...                                                 ...        ...       ...   \n",
              "6342  [al, crying, middle, america, left, twice, muc...          9     -3.80   \n",
              "6343  [even, catholic, pope, francis, dude, like, ne...         11      0.81   \n",
              "6344  [looks, like, flynn, pleased, blocked, blocked...          7      3.95   \n",
              "6345  [trying, conversation, dad, vegetarianism, poi...          9     -4.52   \n",
              "6346    [stand, guy, gentleman, vice, president, pence]          6     -1.20   \n",
              "\n",
              "      feature2  feature3  feature4  feature5  feature6  feature7  feature8  \\\n",
              "0         0.34     -1.86      4.06      1.51     -0.50      1.11     -3.49   \n",
              "1        -0.25     -0.92      1.64     -0.08     -0.89      4.83     -0.64   \n",
              "2         4.38     -0.12     -0.29      0.00      3.32      0.19      1.49   \n",
              "3        -1.47     -4.64     -0.66     -1.73      6.63     -2.78     -3.25   \n",
              "4         2.74      4.05      2.20      5.18     13.63      9.97     -0.87   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "6342     -4.81     -4.15    -22.27     -0.25     -3.59      1.31     -6.65   \n",
              "6343     -1.90      7.58      3.85     -0.53      0.88      4.31     -2.74   \n",
              "6344     -0.52     -0.71     -1.26      0.38      0.00      2.46      2.67   \n",
              "6345     -0.96     -5.11     -2.74     -1.35      0.14      2.15     -2.32   \n",
              "6346      0.03     -3.22      2.91      2.31      1.54      4.09     -1.04   \n",
              "\n",
              "      feature9  feature10  feature11  feature12  \n",
              "0        -1.26   0.778151   1.255273   0.778151  \n",
              "1         0.96   1.204120   0.845098   1.113943  \n",
              "2         1.88   0.778151   1.113943   0.845098  \n",
              "3         0.84   0.954243   0.903090   0.698970  \n",
              "4         6.54   0.698970   1.079181   0.698970  \n",
              "...        ...        ...        ...        ...  \n",
              "6342     -1.44   0.954243   0.845098   0.845098  \n",
              "6343      3.30   1.041393   1.000000   0.602060  \n",
              "6344      8.81   0.845098   0.845098   0.845098  \n",
              "6345      1.94   0.954243   1.113943   0.903090  \n",
              "6346     -3.60   0.778151   0.954243   0.698970  \n",
              "\n",
              "[6347 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0da042de-c634-4b49-937e-26c63af1abc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Tweet</th>\n",
              "      <th>Twitter</th>\n",
              "      <th>Label</th>\n",
              "      <th>Twitter_tokens</th>\n",
              "      <th>Token_len</th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I think I may be finally in with the in crowd ...</td>\n",
              "      <td>think may finally crowd mannequinchallenge gra...</td>\n",
              "      <td>1</td>\n",
              "      <td>[think, may, finally, crowd, mannequinchalleng...</td>\n",
              "      <td>6</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.34</td>\n",
              "      <td>-1.86</td>\n",
              "      <td>4.06</td>\n",
              "      <td>1.51</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.11</td>\n",
              "      <td>-3.49</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>1.255273</td>\n",
              "      <td>0.778151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
              "      <td>wow first hugo chavez fidel castro danny glove...</td>\n",
              "      <td>0</td>\n",
              "      <td>[wow, first, hugo, chavez, fidel, castro, dann...</td>\n",
              "      <td>16</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.92</td>\n",
              "      <td>1.64</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>4.83</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.204120</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>1.113943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter's #ThankYouObama Shows Heartfelt Grati...</td>\n",
              "      <td>twitter thankyouobama shows heartfelt gratitud...</td>\n",
              "      <td>1</td>\n",
              "      <td>[twitter, thankyouobama, shows, heartfelt, gra...</td>\n",
              "      <td>6</td>\n",
              "      <td>0.15</td>\n",
              "      <td>4.38</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user @user @user @user @user @user take away ...</td>\n",
              "      <td>take away illegals dead people trump wins popu...</td>\n",
              "      <td>0</td>\n",
              "      <td>[take, away, illegals, dead, people, trump, wi...</td>\n",
              "      <td>9</td>\n",
              "      <td>1.83</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-4.64</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>-1.73</td>\n",
              "      <td>6.63</td>\n",
              "      <td>-2.78</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#onedirection #harrystyles cute little dance 😉</td>\n",
              "      <td>onedirection harrystyles cute little dance</td>\n",
              "      <td>1</td>\n",
              "      <td>[onedirection, harrystyles, cute, little, dance]</td>\n",
              "      <td>5</td>\n",
              "      <td>2.27</td>\n",
              "      <td>2.74</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.20</td>\n",
              "      <td>5.18</td>\n",
              "      <td>13.63</td>\n",
              "      <td>9.97</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>6.54</td>\n",
              "      <td>0.698970</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6342</th>\n",
              "      <td>@user for al the crying you do about how middl...</td>\n",
              "      <td>al crying middle america left twice much votin...</td>\n",
              "      <td>0</td>\n",
              "      <td>[al, crying, middle, america, left, twice, muc...</td>\n",
              "      <td>9</td>\n",
              "      <td>-3.80</td>\n",
              "      <td>-4.81</td>\n",
              "      <td>-4.15</td>\n",
              "      <td>-22.27</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-3.59</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-6.65</td>\n",
              "      <td>-1.44</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6343</th>\n",
              "      <td>i'm not even catholic, but pope francis is my ...</td>\n",
              "      <td>even catholic pope francis dude like need hug ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[even, catholic, pope, francis, dude, like, ne...</td>\n",
              "      <td>11</td>\n",
              "      <td>0.81</td>\n",
              "      <td>-1.90</td>\n",
              "      <td>7.58</td>\n",
              "      <td>3.85</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4.31</td>\n",
              "      <td>-2.74</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.602060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6344</th>\n",
              "      <td>@user @user @user Looks like Flynn isn't too p...</td>\n",
              "      <td>looks like flynn pleased blocked blocked flynn</td>\n",
              "      <td>0</td>\n",
              "      <td>[looks, like, flynn, pleased, blocked, blocked...</td>\n",
              "      <td>7</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.46</td>\n",
              "      <td>2.67</td>\n",
              "      <td>8.81</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345</th>\n",
              "      <td>Trying to have a conversation with my dad abou...</td>\n",
              "      <td>trying conversation dad vegetarianism pointles...</td>\n",
              "      <td>0</td>\n",
              "      <td>[trying, conversation, dad, vegetarianism, poi...</td>\n",
              "      <td>9</td>\n",
              "      <td>-4.52</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>-5.11</td>\n",
              "      <td>-2.74</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.15</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>1.113943</td>\n",
              "      <td>0.903090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6346</th>\n",
              "      <td>@user You are a stand up guy and a Gentleman V...</td>\n",
              "      <td>stand guy gentleman vice president pence</td>\n",
              "      <td>1</td>\n",
              "      <td>[stand, guy, gentleman, vice, president, pence]</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-3.22</td>\n",
              "      <td>2.91</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.54</td>\n",
              "      <td>4.09</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-3.60</td>\n",
              "      <td>0.778151</td>\n",
              "      <td>0.954243</td>\n",
              "      <td>0.698970</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6347 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0da042de-c634-4b49-937e-26c63af1abc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0da042de-c634-4b49-937e-26c63af1abc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0da042de-c634-4b49-937e-26c63af1abc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction13_23(dataframe):  #pass in df or dft \n",
        "\n",
        "  dataframe[\"Feature 13\"] = np.nan # Count: Words in + Lexicon\n",
        "  dataframe[\"Feature 14\"] = np.nan # Count: Words in - Lexicon\n",
        "  dataframe[\"Feature 15\"] = np.nan # Count: Nouns in Tweet\n",
        "  dataframe[\"Feature 16\"] = np.nan # Count: Adjectives\n",
        "  dataframe[\"Feature 17\"] = np.nan # Ratio: Unique words - total words\n",
        "  dataframe[\"Feature 18\"] = np.nan # Ratio: Stop Words - Total Words\n",
        "  dataframe[\"Feature 19\"] = np.nan # Ratio Nouns to total words\n",
        "  dataframe[\"Feature 20\"] = np.nan # Ratio of proper nouns to total words\n",
        "  dataframe[\"Feature 21\"] = np.nan # Ratio Capital letters to lowercase letters\n",
        "  dataframe[\"Feature 22\"] = np.nan # Ratio of punctuation characters to Total Characters\n",
        "  dataframe[\"Feature 23\"] = np.nan # Does Tweet conatin the word 'No'\n",
        "  dataframe[\"Feature 24\"] = np.nan # Mean Length of all words in tweet \n",
        "  dataframe[\"Feature 25\"] = np.nan # Log of Mean Length of all words in tweet \n",
        "  dataframe[\"Feature 26\"] = np.nan # Highest word score of all words in tweet \n",
        "  dataframe[\"Feature 27\"] = np.nan # Lowest word score of all words in tweet\n",
        "  dataframe[\"Feature 28\"] = np.nan # Emoji count  \n",
        "\n",
        "\n",
        "\n",
        "  #CREATE POSITIVE AND NEGATIVE LEXICONS\n",
        "  pos_lexicon = []\n",
        "  neg_lexicon = []\n",
        "\n",
        "  for i in range(0, len(list_dicts)):               \n",
        "    for j in range(0, len(list_dicts[i])):    \n",
        "\n",
        "      if list(list_dicts[i].values())[j] >= 0:         \n",
        "        pos_lexicon.append(list(list_dicts[i].keys())[j])        \n",
        "          \n",
        "      if list(list_dicts[i].values())[j] < 0:         \n",
        "        neg_lexicon.append(list(list_dicts[i].keys())[j]) \n",
        "\n",
        "  pos_lexicon = [*set(pos_lexicon)]   # Remove duplicate values from + lexicon\n",
        "  neg_lexicon = [*set(neg_lexicon)]   # Remove duplicate values from - lexicon\n",
        "\n",
        "  wrds_in_PosNeg = intersection(pos_lexicon, neg_lexicon)\n",
        "  wrds_in_PosNeg.pop(0)\n",
        "\n",
        "\n",
        "  #Look for duplicates across dictionaries \n",
        "  duplicate_dict = {}\n",
        "\n",
        "  for i in range(0, len(wrds_in_PosNeg)):\n",
        "    sent_scores_2sum = []\n",
        "\n",
        "    for j in range(0, len(list_dicts)):\n",
        "      subreddit_wrd_list = list(list_dicts[j].keys())\n",
        "\n",
        "      if wrds_in_PosNeg[i] in subreddit_wrd_list:\n",
        "        sent_scores_2sum.append(list_dicts[j].get(wrds_in_PosNeg[i]))\n",
        "        num_avg = mean(sent_scores_2sum)\n",
        "    \n",
        "    duplicate_dict[wrds_in_PosNeg[i]] =  num_avg\n",
        "\n",
        "  # Remove words that now have a clear positive or negative classification\n",
        "  for w in duplicate_dict.items():        # .items() returns a tuple of (word, score). See Cell above for all words: scores in dict\n",
        "    if w[1] >= 0:                         # if w[1] (the score) is greater than 0...\n",
        "      neg_lexicon.remove(w[0])            # remove it from the negative lexicon\n",
        "    if w[1] < 0:                          # If w[1] (the score) is less than 0...\n",
        "      pos_lexicon.remove(w[0])            # remove the word from the positive lexicon\n",
        "\n",
        "\n",
        "  # COUNT POSITIVE AND NEGATIVE WORDS (13 & 14)\n",
        "  neg_lex_set = set(neg_lexicon)\n",
        "  pos_lex_set = set(pos_lexicon)\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    x = set(dataframe['Twitter_tokens'][i])\n",
        "    dataframe['Feature 13'].values[i] = len(x.intersection(pos_lex_set))\n",
        "    dataframe['Feature 14'].values[i] = len(x.intersection(neg_lex_set))\n",
        "\n",
        "\n",
        "  # COUNT NOUNS IN TWEET (15)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    tokens = dataframe['Twitter_tokens'][i]\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    noun_definitions = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    count = 0\n",
        "\n",
        "    for j in range(0, len(tagged)):\n",
        "      if tagged[j][1] in noun_definitions:\n",
        "        count += 1\n",
        "    \n",
        "    dataframe['Feature 15'].values[i] = count  \n",
        "\n",
        "\n",
        "  #COUNT ADJECTIVES IN TWEET (16)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    tokens = dataframe['Twitter_tokens'][i]\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    adj_definitions = ['JJ', 'JJR', 'JJS ']\n",
        "    count = 0\n",
        "\n",
        "    for j in range(0, len(tagged)):\n",
        "      if tagged[j][1] in adj_definitions:\n",
        "        count += 1\n",
        "    \n",
        "    dataframe['Feature 16'].values[i] = count\n",
        "\n",
        "  # RATIO: UNIQUE TO TOTAL WORDS (17)\n",
        "  for i in range(0, len(dataframe.index)):                       \n",
        "    tokens = dataframe['Twitter_tokens'][i]\n",
        "    x = np.array(tokens)\n",
        "    ratio = len(np.unique(x)) / len(tokens)\n",
        "    dataframe['Feature 17'].values[i] = ratio\n",
        "\n",
        "\n",
        "  # STOP WORDS TO TOTAL WORDS (18)\n",
        "  for i in range(0, len(dataframe.index)):                       \n",
        "    tokens = dataframe['Twitter_tokens'][i]\n",
        "    x = np.array(tokens)\n",
        "    stop_wrds_count = [w for w in tokens if w in stop_words]\n",
        "    ratio = len(stop_wrds_count) / len(tokens)\n",
        "    dataframe['Feature 18'].values[i] = ratio\n",
        "\n",
        "\n",
        "  # RATIO OF NOUNS TO TOTAL WORDS (19)\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "      tokens = dataframe['Twitter_tokens'][i]\n",
        "      tokens = [w for w in tokens if not w in stop_words]\n",
        "      tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "      noun_definitions = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "      count = 0\n",
        "\n",
        "      for j in range(0, len(tagged)):\n",
        "        if tagged[j][1] in noun_definitions:\n",
        "          count += 1\n",
        "\n",
        "      ratio = count / len(dataframe['Twitter_tokens'][i])\n",
        "      dataframe['Feature 19'].values[i] = ratio\n",
        "\n",
        "     \n",
        "  # RATIO OF PROPER NOUNS TO TOTAL WORDS (20)\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "      tokens = dataframe['Twitter_tokens'][i]\n",
        "      tokens = [w for w in tokens if not w in stop_words]\n",
        "      tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "      noun_definitions = ['NNP', 'NNPS']\n",
        "      count = 0\n",
        "\n",
        "      for j in range(0, len(tagged)):\n",
        "        if tagged[j][1] in noun_definitions:\n",
        "          count += 1\n",
        "\n",
        "      ratio = count / len(dataframe['Twitter_tokens'][i])\n",
        "      dataframe['Feature 20'].values[i] = ratio\n",
        "\n",
        "  # RATIO OF CAPITAL LETTERS TO LOWERCASE LETERS(21)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    upper_count = len(re.findall(r'[A-Z]', dataframe['Original Tweet'].values[i]))\n",
        "    lower_count = len(re.findall(r'[a-z]', dataframe['Original Tweet'].values[i]))\n",
        "\n",
        "    if lower_count == 0:\n",
        "      dataframe['Feature 21'].values[i] = 0 \n",
        "    if lower_count!= 0: \n",
        "      ratio = upper_count / lower_count\n",
        "      dataframe['Feature 21'].values[i] = ratio\n",
        "\n",
        "\n",
        "  # RATIO PUNCTUATION CHARACTERS TO TOTAL CHARACTERS (22)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    count = 0\n",
        "\n",
        "    for j in dataframe['Original Tweet'][i]:\n",
        "      if j in string.punctuation:\n",
        "        count += 1\n",
        "\n",
        "    ratio = count / len(dataframe['Original Tweet'][i])\n",
        "    dataframe['Feature 22'].values[i] = ratio\n",
        "\n",
        "\n",
        "  # DOES TWEET CONTAIN NO (23)\n",
        "  for i in range(0, len(dataframe.index)):                       # In the range 0 to length of the tweets dataframe         # Tokenize and lowercase tweets \n",
        "    if 'no' in dataframe['Twitter_tokens'][i]:                                    # If no is in tweet dataframe value is 1 if not value is zero\n",
        "      dataframe['Feature 23'].values[i] = 1\n",
        "    else: \n",
        "      dataframe['Feature 23'].values[i] = 0\n",
        "\n",
        "  # MEAN LENGTH OF WORDS IN TWEET (24)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    average = sum(len(token) for token in dataframe['Twitter_tokens'][i]) / len(dataframe['Twitter_tokens'][i])\n",
        "\n",
        "    dataframe['Feature 24'].values[i] = average\n",
        "\n",
        "\n",
        "\n",
        "  # LOG MEAN LENGTH OF WORDS IN TWEET (25)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    average = sum(len(token) for token in dataframe['Twitter_tokens'][i]) / len(dataframe['Twitter_tokens'][i])\n",
        "\n",
        "    dataframe['Feature 25'].values[i] = math.log(average)\n",
        "\n",
        "\n",
        "  # HIGHEST SCORE VALUE OF TOKENS (26)\n",
        "  dataframe['Feature 26'] = dataframe['Feature 26'].astype('object')\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    found_values = []\n",
        "\n",
        "    for j in range(0 , len(dataframe['Twitter_tokens'][i])):\n",
        "      if dataframe['Twitter_tokens'][i][j] in super_dict:\n",
        "        found_values.append(super_dict.get(dataframe['Twitter_tokens'][i][j])) \n",
        "\n",
        "      if not found_values:\n",
        "        found_values.append(0) \n",
        "\n",
        "    dataframe['Feature 26'].values[i] = max(found_values)\n",
        "\n",
        "\n",
        "  # LOWEST SCORE VALUE OF TOKENS (27)\n",
        "  dataframe['Feature 27'] = dataframe['Feature 27'].astype('object')\n",
        "\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    found_values = []\n",
        "\n",
        "    for j in range(0 , len(dataframe['Twitter_tokens'][i])):\n",
        "      if dataframe['Twitter_tokens'][i][j] in super_dict:\n",
        "        found_values.append(super_dict.get(dataframe['Twitter_tokens'][i][j])) \n",
        "\n",
        "      if not found_values:\n",
        "        found_values.append(0) \n",
        "\n",
        "    dataframe['Feature 27'].values[i] = min(found_values)\n",
        "\n",
        "\n",
        "  # COUNT EMOJIS (28)\n",
        "  for i in range(0, len(dataframe.index)):\n",
        "    count = len(re.findall(r'[\\U0001f600-\\U0001f650]', dataframe['Original Tweet'][i]))\n",
        "\n",
        "    dataframe['Feature 28'].values[i] = count\n",
        "\n",
        "  return(dataframe)"
      ],
      "metadata": {
        "id": "t3AWr_KqH2D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combination of all subreddit dict values into one dict \n",
        "super_dict = {}\n",
        "for i in range(0, len(list_dicts)):\n",
        "  super_dict=dict_merger(super_dict,list_dicts[i])"
      ],
      "metadata": {
        "id": "TN1eTNYiNvt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_updated = feature_extraction13_23(df)\n",
        "dft_updated = feature_extraction13_23(dft)"
      ],
      "metadata": {
        "id": "kjVlqmfwMEVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_updated\n",
        "dft = dft_updated"
      ],
      "metadata": {
        "id": "L_dYyihBReQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#dfc=pd.concat([df, dft])\n",
        "dfc.to_csv('tweets_extracted.csv')"
      ],
      "metadata": {
        "id": "vqxrhdNsJRef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forward Selection of Features"
      ],
      "metadata": {
        "id": "IvcZXeOtTU6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown 1ubIaCqJnOzG-m_ns7VmdV87Ecs349BbG\n",
        "#dfc=pd.read_csv('tweets_extracted.csv')\n",
        "#Xc=dfc.drop(columns=['Unnamed: 0', 'Twitter','Label','Twitter_tokens','Token_len','Original Tweet','feature24']) #whats wrong with feature24?\n",
        "#yc=dfc['Label']"
      ],
      "metadata": {
        "id": "KAeXsYHcmc7y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfc=pd.concat([df, dft])\n",
        "Xc=dfc.drop(columns=['Twitter','Label','Twitter_tokens','Token_len','Original Tweet','feature24']) #whats wrong with feature24?\n",
        "yc=dfc['Label']"
      ],
      "metadata": {
        "id": "3QS-ndW6mk2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_feature_maker(x,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, shuffle=True, random_state=42)\n",
        "  sel = SelectFromModel(RandomForestClassifier(n_estimators = 50))\n",
        "  sel.fit(X_train, y_train)\n",
        "  print(\"The best features to use are: \")\n",
        "  feature_index = sel.get_support(indices=True)\n",
        "  x_new=x.iloc[:,feature_index]\n",
        "  print(x_new.head())\n",
        "  weights=sel.estimator_.feature_importances_\n",
        "  weights_percent = 100 * (weights/max(weights))\n",
        "  for i in range(0,len(weights)):\n",
        "    print(\"The feature \" + str(X_train.columns[i]) + \" has a relative importance percentage \" + str(weights_percent[i]))\n",
        "  return x_new\n",
        "\n",
        "  #https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
        "  #call coefficients for a third ranking"
      ],
      "metadata": {
        "id": "_tNVVetTkQtn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xc_selected=best_feature_maker(Xc,yc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hsYY-W1mGnP",
        "outputId": "7b046014-01e4-4f99-d1cf-0e1852ea5d9b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best features to use are: \n",
            "   Unnamed: 0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
            "0           0      1.42     -0.23     -0.43     -0.52      0.56     -0.31   \n",
            "1           1      5.64      2.17     -0.54      0.54      0.51      1.24   \n",
            "2           2      1.82     -2.73     -1.70     -0.09     -0.71      3.65   \n",
            "3           3      0.25      0.14      3.65      1.32     -2.67     -5.03   \n",
            "4           4      3.48      0.32     -3.09     -1.57     -1.32      2.60   \n",
            "\n",
            "   feature7  feature8  feature9  Feature 13  Feature 14  Feature 21  \\\n",
            "0      2.26      0.81      0.83         7.0         1.0    0.130952   \n",
            "1      0.57      5.02      1.29         8.0         3.0    0.066667   \n",
            "2      2.08     -0.08      0.11         5.0         4.0    0.238095   \n",
            "3      4.65      1.78      1.38        14.0         5.0    1.611111   \n",
            "4      0.46      0.74      4.52         3.0         5.0    0.033898   \n",
            "\n",
            "   Feature 22  Feature 26  Feature 27  \n",
            "0    0.033898        1.57       -0.44  \n",
            "1    0.034783        1.52       -0.83  \n",
            "2    0.068627        3.36       -3.12  \n",
            "3    0.121429        2.65       -0.44  \n",
            "4    0.012658        1.55       -1.65  \n",
            "The feature Unnamed: 0 has a relative importance percentage 58.23037807320021\n",
            "The feature feature1 has a relative importance percentage 81.1252053390777\n",
            "The feature feature2 has a relative importance percentage 56.411344646417746\n",
            "The feature feature3 has a relative importance percentage 49.00777994355155\n",
            "The feature feature4 has a relative importance percentage 53.105883028544035\n",
            "The feature feature5 has a relative importance percentage 42.93375690453752\n",
            "The feature feature6 has a relative importance percentage 51.43612694338564\n",
            "The feature feature7 has a relative importance percentage 45.1444057236067\n",
            "The feature feature8 has a relative importance percentage 79.74651202409704\n",
            "The feature feature9 has a relative importance percentage 66.41407443703169\n",
            "The feature feature10 has a relative importance percentage 26.524034706623965\n",
            "The feature feature11 has a relative importance percentage 22.950977966409635\n",
            "The feature feature12 has a relative importance percentage 25.342226699446748\n",
            "The feature Feature 13 has a relative importance percentage 60.33552709410767\n",
            "The feature Feature 14 has a relative importance percentage 100.0\n",
            "The feature Feature 15 has a relative importance percentage 20.27350202490496\n",
            "The feature Feature 16 has a relative importance percentage 17.823334549664317\n",
            "The feature Feature 17 has a relative importance percentage 11.421054639203644\n",
            "The feature Feature 18 has a relative importance percentage 0.317790661346279\n",
            "The feature Feature 19 has a relative importance percentage 31.944915870848543\n",
            "The feature Feature 20 has a relative importance percentage 1.5006099801972652\n",
            "The feature Feature 21 has a relative importance percentage 58.511063302377764\n",
            "The feature Feature 22 has a relative importance percentage 42.00653493923407\n",
            "The feature Feature 23 has a relative importance percentage 0.0\n",
            "The feature Feature 24 has a relative importance percentage 37.32084033440263\n",
            "The feature Feature 25 has a relative importance percentage 35.53953904171859\n",
            "The feature Feature 26 has a relative importance percentage 55.24054714280364\n",
            "The feature Feature 27 has a relative importance percentage 56.86482823063185\n",
            "The feature Feature 28 has a relative importance percentage 0.8685614794258044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LogReg"
      ],
      "metadata": {
        "id": "JMgubEwbkzGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogRegression:\n",
        "    def __init__(self, feature_number, lr, epochs):      \n",
        "        self.intercept = 0\n",
        "        self.weight = np.zeros(feature_number)\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        z = np.dot(X, self.weight) + self.intercept\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def loss(self, s, y):\n",
        "        return (-y * np.log(s) - (1 - y) * np.log(1 - s)).mean()\n",
        "    \n",
        "    def gradient_descent(self, X, s, y):\n",
        "        return np.dot(X.T, (s - y)) / y.shape[0]\n",
        "    \n",
        "    def gradient_descent_intercept(self, s, y):\n",
        "        return np.mean(s - y)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        losses = []\n",
        "        for i in range(self.epochs):\n",
        "            sigma = self.sigmoid(X)\n",
        "\n",
        "            dW = self.gradient_descent(X , sigma, y)\n",
        "            dintercept = self.gradient_descent_intercept(sigma, y)\n",
        "\n",
        "            self.weight -= self.lr * dW\n",
        "            self.intercept -= self.lr * dintercept\n",
        "\n",
        "            loss = self.loss(sigma, y)\n",
        "            if len(losses) % 1000 == 0:\n",
        "                print(\"The iteration is \" + str(i) + \" and the loss is \" +  str(loss))\n",
        "            losses.append(loss)\n",
        "            if i > 1000:\n",
        "              if losses[-1] > losses[-100] - .0001:     #the difference should be an argument\n",
        "                print(\"The weight is \")\n",
        "                print(self.weight)\n",
        "                print(\"The intercept is \")\n",
        "                print(self.intercept)\n",
        "                break\n",
        "        return losses\n",
        "    \n",
        "    def predict(self, train):\n",
        "        x_new = train\n",
        "        result = self.sigmoid(x_new)\n",
        "        y_pred = np.zeros(result.shape[0])\n",
        "        for i in range(len(y_pred)):\n",
        "            if result[i] >= 0.5: \n",
        "                y_pred[i] = 1\n",
        "            else:\n",
        "                y_pred[i] = 0\n",
        "                continue\n",
        "                \n",
        "        return y_pred\n",
        "      \n",
        "    def metrics(self, pred, test):\n",
        "        y_pred=pred\n",
        "        y_test=test\n",
        "        tp=(sum((y_pred == 1) & (y_test==1)))\n",
        "        fp=(sum((y_pred == 1) & (y_test==0)))\n",
        "        fn=(sum((y_pred == 0) & (y_test==1)))\n",
        "        tn=(sum((y_pred == 0) & (y_test==0)))\n",
        "        confusion_matrix=[[tn, fp], [fn, tp]]\n",
        "        print(\"The confusion matrix is: \")\n",
        "        print(confusion_matrix[0])\n",
        "        print(confusion_matrix[1])\n",
        "        print('The accuracy for the Twitter sentiment is {}'.format(sum(y_pred == y_test) / y_test.shape[0]))\n",
        "        print('The precision for the Twitter sentiment is {}'.format((tp/(tp+fp))))\n",
        "        print('The recall for the Twitter sentiment is {}'.format((tp/(tp+fn))))\n",
        "        print('The F1 score for the Twitter sentiment is {}'.format((2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fn))+(tp/(tp+fp))))))\n",
        "        return confusion_matrix"
      ],
      "metadata": {
        "id": "XSwfy98wRmUv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "w_Coim96PYuk",
        "outputId": "3bd5bb52-a8e0-469a-a369-db2dcfed9413"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft.head()"
      ],
      "metadata": {
        "id": "hmf3dyfPIbSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "Xc_selected, yc, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DM6t2DQNaT4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experimented with different splits, best performance was to concatenate the train and test sets\n",
        "#and then randomly resample them to create new train and test sets\n",
        "#this gave better variance and improved the score\n",
        "\"\"\"\n",
        "X_train=df.drop(columns=['Twitter','Label','Twitter_tokens','Token_len'])\n",
        "X_test=dft.drop(columns=['Twitter','Label','Twitter_tokens','Token_len'])\n",
        "y_train=df['Label']\n",
        "y_test=dft['Label']\n",
        "X=X_train\n",
        "y=y_train\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Fj-fW-zITEqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "#very minor boost in performance since our features are already of similar scale, but did give a boost"
      ],
      "metadata": {
        "id": "hagr9i64S0Rc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7c875e1a-adb5-4a05-cec6-29df291a3d0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-61aa0d5f40a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#very minor boost in performance since our features are already of similar scale, but did give a boost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_number, lr, epochs = X_train.shape[1], .5, 1000000\n",
        "\n",
        "LogRegSent = LogRegression(feature_number, lr, epochs)\n",
        "\n",
        "losses = LogRegSent.fit(X_train_norm, y_train)"
      ],
      "metadata": {
        "id": "4nst7aSnSMh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Iterations on Twitter Sentiment\")\n",
        "plt.plot(losses)\n",
        "plt.show\n"
      ],
      "metadata": {
        "id": "AdozPFsDSOTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LogRegSent.predict(X_test_norm)"
      ],
      "metadata": {
        "id": "DMlfHTH1HD4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = LogRegSent.metrics(y_pred, y_test)"
      ],
      "metadata": {
        "id": "LSddLO7DtkZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 1 Additional Feature"
      ],
      "metadata": {
        "id": "75Bn-oE-r35D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def try_add_feature(x,y,feat,target):\n",
        "  x_1=x\n",
        "  x_1.insert(1, feat.name, feat)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_1, y, test_size=0.2, random_state=42)\n",
        "  mms = MinMaxScaler()\n",
        "  X_train_norm, X_test_norm = mms.fit_transform(X_train), mms.transform(X_test)\n",
        "  feature_number, lr, epochs = X_train.shape[1], .5, 1000000\n",
        "  LogRegSent = LogRegression(feature_number, lr, epochs)\n",
        "  losses = LogRegSent.fit(X_train_norm, y_train)\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Loss vs Iterations on Twitter Sentiment\")\n",
        "  plt.plot(losses)\n",
        "  plt.show\n",
        "  y_pred = LogRegSent.predict(X_test_norm)\n",
        "  cm = LogRegSent.metrics(y_pred, y_test)\n",
        "  f1=(2*cm[1][1]/(cm[1][1]+cm[0][1]))*(cm[1][1]/(cm[1][1]+cm[1][0]))/((cm[1][1]/(cm[1][1]+cm[1][0]))+(cm[1][1]/(cm[1][1]+cm[0][1])))\n",
        "  print(\"The F1 with \" + str(feat.name) + \" is: \" + str(f1) + \" but the target was: \" +str(target))\n",
        "  diff=target-f1\n",
        "  print(\"The difference in F1 score with \" + str(feat.name) + \" is: \" + str(diff))\n",
        "  return diff\n",
        "\n"
      ],
      "metadata": {
        "id": "uy359nLBr7j0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  features_selected = Xc_selected.columns\n",
        "  all_features=Xc.columns\n",
        "  removed_features_names=[]\n",
        "  removed_features=[]\n",
        "  for i in range(0,len(all_features)):\n",
        "    good=0\n",
        "    for j in range(0,len(features_selected)):\n",
        "      if all_features[i] == features_selected[j]:\n",
        "        good-=1\n",
        "    if good==0:\n",
        "      removed_features_names.append(Xc.columns[i])\n",
        "\n",
        "  for k in range(0,len(removed_features_names)):\n",
        "    removed_features.append(Xc[removed_features_names[k]])\n",
        "    print(\"Adding \" + str(removed_features_names[k]) + \" to the removed features list\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1MVe_-sdpBw",
        "outputId": "db25b361-a06e-4f32-c4d2-0a930897e68c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding feature11 to the removed features list\n",
            "Adding feature12 to the removed features list\n",
            "Adding Feature 15 to the removed features list\n",
            "Adding Feature 16 to the removed features list\n",
            "Adding Feature 17 to the removed features list\n",
            "Adding Feature 18 to the removed features list\n",
            "Adding Feature 19 to the removed features list\n",
            "Adding Feature 20 to the removed features list\n",
            "Adding Feature 23 to the removed features list\n",
            "Adding Feature 24 to the removed features list\n",
            "Adding Feature 25 to the removed features list\n",
            "Adding Feature 28 to the removed features list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_diff_add=[]\n",
        "f1_selected = 0.8242088242088242 #this should be the F1 score of the Xc_selected features from SelectFromModel\n",
        "for i in range(0,len(removed_features)):\n",
        "  new=removed_features[i]\n",
        "  Xc_OG=Xc_selected\n",
        "  difference=try_add_feature(Xc_OG,yc,new,f1_selected)\n",
        "  f1_diff_add.append(difference)\n",
        "  print(\"---------------\")\n",
        "for i in range(0,len(removed_features)):\n",
        "  print(\"To remove \" + str(removed_features[i]) + \" there was a loss of \" + str(f1_diff_add[i]) + \" in F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CUfsYfJc_-RJ",
        "outputId": "fc37d38d-5dad-4942-bae6-b4bf236e0136"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5160980720422721\n",
            "The iteration is 2000 and the loss is 0.5112549230415377\n",
            "The iteration is 3000 and the loss is 0.5085939383268374\n",
            "The iteration is 4000 and the loss is 0.5066912790463725\n",
            "The iteration is 5000 and the loss is 0.5052836311847053\n",
            "The weight is \n",
            "[ 0.67903279 -0.16906687 -0.86275142  3.71303883  0.61991093 -0.02981357\n",
            "  1.3036199  -0.80963357  0.92236965 -0.25308539  1.99462244  1.85647493\n",
            "  3.26158093 -4.33704624 -0.02571412  0.56283627  1.25825949  1.19626495]\n",
            "The intercept is \n",
            "-5.622863751530904\n",
            "The confusion matrix is: \n",
            "[1158, 1027]\n",
            "[495, 3578]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.7769815418023887\n",
            "The recall for the Twitter sentiment is 0.8784679597348392\n",
            "The F1 score for the Twitter sentiment is 0.8246139663516939\n",
            "The F1 with feature11 is: 0.8246139663516939 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with feature11 is: -0.00040514214286968553\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5153774034914078\n",
            "The iteration is 2000 and the loss is 0.5106808186591327\n",
            "The iteration is 3000 and the loss is 0.5080780766845441\n",
            "The iteration is 4000 and the loss is 0.5062047624375319\n",
            "The iteration is 5000 and the loss is 0.5048105698632485\n",
            "The weight is \n",
            "[ 0.67252502 -1.01828584  0.17270033 -0.16601726  3.71910864  0.59229932\n",
            " -0.0253989   1.25627424 -0.80365739  0.90176607 -0.29379885  2.04115501\n",
            "  1.75140439  3.30485997 -4.29351826 -0.03225316  0.46864986  1.23361589\n",
            "  1.16991219]\n",
            "The intercept is \n",
            "-5.457849460772209\n",
            "The confusion matrix is: \n",
            "[1162, 1023]\n",
            "[494, 3579]\n",
            "The accuracy for the Twitter sentiment is 0.757590284435922\n",
            "The precision for the Twitter sentiment is 0.7777053455019557\n",
            "The recall for the Twitter sentiment is 0.8787134790081021\n",
            "The F1 score for the Twitter sentiment is 0.8251296829971181\n",
            "The F1 with feature12 is: 0.8251296829971181 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with feature12 is: -0.0009208587882938435\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5153639973547932\n",
            "The iteration is 2000 and the loss is 0.5106903219112519\n",
            "The iteration is 3000 and the loss is 0.5080863766200837\n",
            "The iteration is 4000 and the loss is 0.5062092601418905\n",
            "The iteration is 5000 and the loss is 0.5048109670564334\n",
            "The weight is \n",
            "[ 0.67250158 -0.05097698 -1.00691356  0.17494386 -0.15807371  3.72486841\n",
            "  0.59328835 -0.02732598  1.25832132 -0.8044071   0.9021118  -0.29391652\n",
            "  2.04292217  1.75248854  3.31573811 -4.28696756 -0.03264205  0.46897141\n",
            "  1.23475347  1.16863942]\n",
            "The intercept is \n",
            "-5.465726352531968\n",
            "The confusion matrix is: \n",
            "[1163, 1022]\n",
            "[493, 3580]\n",
            "The accuracy for the Twitter sentiment is 0.7579098753595398\n",
            "The precision for the Twitter sentiment is 0.777922642329422\n",
            "The recall for the Twitter sentiment is 0.878958998281365\n",
            "The F1 score for the Twitter sentiment is 0.8253602305475504\n",
            "The F1 with Feature 15 is: 0.8253602305475504 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 15 is: -0.0011514063387261242\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5152673615083309\n",
            "The iteration is 2000 and the loss is 0.5106189234231439\n",
            "The iteration is 3000 and the loss is 0.5080331113744141\n",
            "The iteration is 4000 and the loss is 0.5061701799331322\n",
            "The iteration is 5000 and the loss is 0.5047836291784935\n",
            "The weight is \n",
            "[ 0.67431094  0.27672504  0.03949315 -1.06062821  0.17691685 -0.28132588\n",
            "  3.7198771   0.61207075 -0.04688562  1.27997178 -0.80829481  0.89866181\n",
            " -0.28795456  2.03182832  1.76243103  3.27608547 -4.29512404 -0.03720656\n",
            "  0.43927146  1.22742609  1.17220803]\n",
            "The intercept is \n",
            "-5.414372802947789\n",
            "The confusion matrix is: \n",
            "[1161, 1024]\n",
            "[499, 3574]\n",
            "The accuracy for the Twitter sentiment is 0.7566315116650687\n",
            "The precision for the Twitter sentiment is 0.7772944758590692\n",
            "The recall for the Twitter sentiment is 0.8774858826417874\n",
            "The F1 score for the Twitter sentiment is 0.8243570522431092\n",
            "The F1 with Feature 16 is: 0.8243570522431092 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 16 is: -0.00014822803428493625\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5138172548529856\n",
            "The iteration is 2000 and the loss is 0.5097095216606639\n",
            "The iteration is 3000 and the loss is 0.5074893343967231\n",
            "The iteration is 4000 and the loss is 0.5058975144873765\n",
            "The iteration is 5000 and the loss is 0.5047074359251211\n",
            "The weight is \n",
            "[ 0.67462179 -1.09044424  0.11728831 -0.12647577 -1.02504324  0.30411069\n",
            " -0.54968269  3.64876953  0.69466416  0.00532575  1.28363118 -0.67712158\n",
            "  0.90943118 -0.21458367  2.03897022  1.71771386  3.46170456 -4.14095451\n",
            " -0.03202824  0.36756547  1.24144938  1.16868281]\n",
            "The intercept is \n",
            "-4.3637936281210585\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[494, 3579]\n",
            "The accuracy for the Twitter sentiment is 0.7585490572067753\n",
            "The precision for the Twitter sentiment is 0.7787206266318538\n",
            "The recall for the Twitter sentiment is 0.8787134790081021\n",
            "The F1 score for the Twitter sentiment is 0.825700772868843\n",
            "The F1 with Feature 17 is: 0.825700772868843 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 17 is: -0.0014919486600187826\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5138172149859662\n",
            "The iteration is 2000 and the loss is 0.5097089743171305\n",
            "The iteration is 3000 and the loss is 0.5074883035691103\n",
            "The iteration is 4000 and the loss is 0.5058960400435908\n",
            "The iteration is 5000 and the loss is 0.504705550581762\n",
            "The weight is \n",
            "[ 0.67458896  0.06244601 -1.09052429  0.1175257  -0.12630232 -1.02466023\n",
            "  0.30418164 -0.54997802  3.64908283  0.69501058  0.00528976  1.28382567\n",
            " -0.67715897  0.9094278  -0.21483068  2.03899311  1.71799126  3.46142625\n",
            " -4.14124781 -0.03204388  0.36770695  1.24157266  1.16859483]\n",
            "The intercept is \n",
            "-4.364369562205522\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[494, 3579]\n",
            "The accuracy for the Twitter sentiment is 0.7585490572067753\n",
            "The precision for the Twitter sentiment is 0.7787206266318538\n",
            "The recall for the Twitter sentiment is 0.8787134790081021\n",
            "The F1 score for the Twitter sentiment is 0.825700772868843\n",
            "The F1 with Feature 18 is: 0.825700772868843 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 18 is: -0.0014919486600187826\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5131929811449021\n",
            "The iteration is 2000 and the loss is 0.5089363374124064\n",
            "The iteration is 3000 and the loss is 0.5066805810775098\n",
            "The iteration is 4000 and the loss is 0.5050907450602943\n",
            "The iteration is 5000 and the loss is 0.5039215830637742\n",
            "The weight is \n",
            "[ 0.66292702 -1.04757775  0.04795129 -0.93838377 -0.13108184  0.93007978\n",
            " -1.08032816  0.32820555 -0.9434942   3.66529668  0.77486659  0.10700346\n",
            "  1.33069056 -0.61143269  0.95490846 -0.22300551  2.1437718   1.71451102\n",
            "  3.18520302 -4.3507316  -0.02667232  0.38945208  1.19743438  1.19178398]\n",
            "The intercept is \n",
            "-4.048021283467334\n",
            "The confusion matrix is: \n",
            "[1170, 1015]\n",
            "[502, 3571]\n",
            "The accuracy for the Twitter sentiment is 0.757590284435922\n",
            "The precision for the Twitter sentiment is 0.7786742259049281\n",
            "The recall for the Twitter sentiment is 0.8767493248219985\n",
            "The F1 score for the Twitter sentiment is 0.8248065596489202\n",
            "The F1 with Feature 19 is: 0.8248065596489202 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 19 is: -0.0005977354400960033\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5131179334804126\n",
            "The iteration is 2000 and the loss is 0.5088161954114203\n",
            "The iteration is 3000 and the loss is 0.5065295709407801\n",
            "The iteration is 4000 and the loss is 0.5049184530942253\n",
            "The iteration is 5000 and the loss is 0.5037345628001824\n",
            "The weight is \n",
            "[ 0.66325886  0.69137946 -1.05827362  0.04595769 -0.93629626 -0.12928881\n",
            "  0.92676517 -1.07484298  0.33365162 -0.949178    3.68065348  0.77887263\n",
            "  0.106174    1.33835252 -0.61189929  0.95477624 -0.22833955  2.14685386\n",
            "  1.71358134  3.18035693 -4.34929488 -0.02711375  0.38884341  1.20247981\n",
            "  1.19407823]\n",
            "The intercept is \n",
            "-4.066813556779091\n",
            "The confusion matrix is: \n",
            "[1171, 1014]\n",
            "[502, 3571]\n",
            "The accuracy for the Twitter sentiment is 0.7577500798977309\n",
            "The precision for the Twitter sentiment is 0.7788440567066521\n",
            "The recall for the Twitter sentiment is 0.8767493248219985\n",
            "The F1 score for the Twitter sentiment is 0.824901824901825\n",
            "The F1 with Feature 20 is: 0.824901824901825 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 20 is: -0.0006930006930007115\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5131179334804126\n",
            "The iteration is 2000 and the loss is 0.5088161954114203\n",
            "The iteration is 3000 and the loss is 0.5065295709407801\n",
            "The iteration is 4000 and the loss is 0.5049184530942253\n",
            "The iteration is 5000 and the loss is 0.5037345628001824\n",
            "The weight is \n",
            "[ 0.66325886  0.          0.69137946 -1.05827362  0.04595769 -0.93629626\n",
            " -0.12928881  0.92676517 -1.07484298  0.33365162 -0.949178    3.68065348\n",
            "  0.77887263  0.106174    1.33835252 -0.61189929  0.95477624 -0.22833955\n",
            "  2.14685386  1.71358134  3.18035693 -4.34929488 -0.02711375  0.38884341\n",
            "  1.20247981  1.19407823]\n",
            "The intercept is \n",
            "-4.066813556779089\n",
            "The confusion matrix is: \n",
            "[1171, 1014]\n",
            "[502, 3571]\n",
            "The accuracy for the Twitter sentiment is 0.7577500798977309\n",
            "The precision for the Twitter sentiment is 0.7788440567066521\n",
            "The recall for the Twitter sentiment is 0.8767493248219985\n",
            "The F1 score for the Twitter sentiment is 0.824901824901825\n",
            "The F1 with Feature 23 is: 0.824901824901825 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 23 is: -0.0006930006930007115\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5131481108492901\n",
            "The iteration is 2000 and the loss is 0.5088361355828299\n",
            "The iteration is 3000 and the loss is 0.5065370294158226\n",
            "The iteration is 4000 and the loss is 0.5049203311922414\n",
            "The iteration is 5000 and the loss is 0.5037342475544444\n",
            "The weight is \n",
            "[ 0.66329436 -0.0990307   0.          0.69067349 -1.05354656  0.04544665\n",
            " -0.93497689 -0.12956666  0.91897959 -1.04250367  0.37837518 -0.97072287\n",
            "  3.68131191  0.77667337  0.1068324   1.33647523 -0.61215659  0.95623809\n",
            " -0.2285013   2.1470063   1.71252096  3.17302571 -4.35533704 -0.02660913\n",
            "  0.3873475   1.20240962  1.19383773]\n",
            "The intercept is \n",
            "-4.069097202273504\n",
            "The confusion matrix is: \n",
            "[1170, 1015]\n",
            "[500, 3573]\n",
            "The accuracy for the Twitter sentiment is 0.7579098753595398\n",
            "The precision for the Twitter sentiment is 0.7787707061900611\n",
            "The recall for the Twitter sentiment is 0.8772403633685244\n",
            "The F1 score for the Twitter sentiment is 0.8250779355732594\n",
            "The F1 with Feature 24 is: 0.8250779355732594 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 24 is: -0.0008691113644351267\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5132141243187853\n",
            "The iteration is 2000 and the loss is 0.5088614184374888\n",
            "The iteration is 3000 and the loss is 0.5065427486902321\n",
            "The iteration is 4000 and the loss is 0.5049210133776153\n",
            "The iteration is 5000 and the loss is 0.5037338929981322\n",
            "The weight is \n",
            "[ 0.66270523 -0.29991141  0.05891891  0.          0.68816749 -1.03470886\n",
            "  0.04373178 -0.92663799 -0.13188961  0.89015271 -0.93981604  0.49331035\n",
            " -1.02103772  3.68240838  0.77327504  0.10824269  1.33389737 -0.61043725\n",
            "  0.96098127 -0.22681345  2.147555    1.70889069  3.14769916 -4.37356257\n",
            " -0.02718292  0.38111765  1.2017513   1.19316162]\n",
            "The intercept is \n",
            "-4.053796801546712\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[500, 3573]\n",
            "The accuracy for the Twitter sentiment is 0.757590284435922\n",
            "The precision for the Twitter sentiment is 0.7784313725490196\n",
            "The recall for the Twitter sentiment is 0.8772403633685244\n",
            "The F1 score for the Twitter sentiment is 0.8248874523837006\n",
            "The F1 with Feature 25 is: 0.8248874523837006 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 25 is: -0.0006786281748764056\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5132136675897874\n",
            "The iteration is 2000 and the loss is 0.5088603090106234\n",
            "The iteration is 3000 and the loss is 0.506540994081759\n",
            "The iteration is 4000 and the loss is 0.504918610558818\n",
            "The iteration is 5000 and the loss is 0.5037308375537352\n",
            "The weight is \n",
            "[ 0.66312048  0.08119885 -0.30054072  0.0584131   0.          0.68838925\n",
            " -1.03533148  0.04377447 -0.92651647 -0.13210908  0.89063236 -0.93910732\n",
            "  0.49264424 -1.02052324  3.68321709  0.77338416  0.10817077  1.33432775\n",
            " -0.61082261  0.96085809 -0.22692641  2.14786188  1.70880426  3.1478184\n",
            " -4.37343451 -0.02729454  0.38152236  1.2017622   1.19287092]\n",
            "The intercept is \n",
            "-4.055033838215021\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[500, 3573]\n",
            "The accuracy for the Twitter sentiment is 0.757590284435922\n",
            "The precision for the Twitter sentiment is 0.7784313725490196\n",
            "The recall for the Twitter sentiment is 0.8772403633685244\n",
            "The F1 score for the Twitter sentiment is 0.8248874523837006\n",
            "The F1 with Feature 28 is: 0.8248874523837006 but the target was: 0.8242088242088242\n",
            "The difference in F1 score with Feature 28 is: -0.0006786281748764056\n",
            "---------------\n",
            "To remove 0        1.361728\n",
            "1        0.845098\n",
            "2        1.000000\n",
            "3        0.954243\n",
            "4        1.079181\n",
            "           ...   \n",
            "31284    0.845098\n",
            "31285    1.000000\n",
            "31286    0.845098\n",
            "31287    1.113943\n",
            "31288    0.954243\n",
            "Name: feature11, Length: 31289, dtype: float64 there was a loss of -0.00040514214286968553 in F1\n",
            "To remove 0        0.954243\n",
            "1        1.000000\n",
            "2        0.903090\n",
            "3        1.041393\n",
            "4        0.602060\n",
            "           ...   \n",
            "31284    0.845098\n",
            "31285    0.602060\n",
            "31286    0.845098\n",
            "31287    0.903090\n",
            "31288    0.698970\n",
            "Name: feature12, Length: 31289, dtype: float64 there was a loss of -0.0009208587882938435 in F1\n",
            "To remove 0         7.0\n",
            "1         5.0\n",
            "2         7.0\n",
            "3        11.0\n",
            "4         5.0\n",
            "         ... \n",
            "31284     4.0\n",
            "31285     5.0\n",
            "31286     3.0\n",
            "31287     5.0\n",
            "31288     5.0\n",
            "Name: Feature 15, Length: 31289, dtype: float64 there was a loss of -0.0011514063387261242 in F1\n",
            "To remove 0        2.0\n",
            "1        1.0\n",
            "2        1.0\n",
            "3        4.0\n",
            "4        2.0\n",
            "        ... \n",
            "31284    2.0\n",
            "31285    2.0\n",
            "31286    2.0\n",
            "31287    0.0\n",
            "31288    1.0\n",
            "Name: Feature 16, Length: 31289, dtype: float64 there was a loss of -0.00014822803428493625 in F1\n",
            "To remove 0        1.000000\n",
            "1        1.000000\n",
            "2        0.916667\n",
            "3        1.000000\n",
            "4        1.000000\n",
            "           ...   \n",
            "31284    1.000000\n",
            "31285    1.000000\n",
            "31286    0.714286\n",
            "31287    1.000000\n",
            "31288    1.000000\n",
            "Name: Feature 17, Length: 31289, dtype: float64 there was a loss of -0.0014919486600187826 in F1\n",
            "To remove 0        0.0\n",
            "1        0.0\n",
            "2        0.0\n",
            "3        0.0\n",
            "4        0.0\n",
            "        ... \n",
            "31284    0.0\n",
            "31285    0.0\n",
            "31286    0.0\n",
            "31287    0.0\n",
            "31288    0.0\n",
            "Name: Feature 18, Length: 31289, dtype: float64 there was a loss of -0.0014919486600187826 in F1\n",
            "To remove 0        0.636364\n",
            "1        0.384615\n",
            "2        0.583333\n",
            "3        0.523810\n",
            "4        0.500000\n",
            "           ...   \n",
            "31284    0.444444\n",
            "31285    0.454545\n",
            "31286    0.428571\n",
            "31287    0.555556\n",
            "31288    0.833333\n",
            "Name: Feature 19, Length: 31289, dtype: float64 there was a loss of -0.0005977354400960033 in F1\n",
            "To remove 0        0.0\n",
            "1        0.0\n",
            "2        0.0\n",
            "3        0.0\n",
            "4        0.0\n",
            "        ... \n",
            "31284    0.0\n",
            "31285    0.0\n",
            "31286    0.0\n",
            "31287    0.0\n",
            "31288    0.0\n",
            "Name: Feature 20, Length: 31289, dtype: float64 there was a loss of -0.0006930006930007115 in F1\n",
            "To remove 0        0.0\n",
            "1        0.0\n",
            "2        0.0\n",
            "3        0.0\n",
            "4        0.0\n",
            "        ... \n",
            "31284    0.0\n",
            "31285    0.0\n",
            "31286    0.0\n",
            "31287    0.0\n",
            "31288    0.0\n",
            "Name: Feature 23, Length: 31289, dtype: float64 there was a loss of -0.0006930006930007115 in F1\n",
            "To remove 0        7.000000\n",
            "1        5.230769\n",
            "2        5.416667\n",
            "3        4.428571\n",
            "4        5.300000\n",
            "           ...   \n",
            "31284    5.000000\n",
            "31285    5.090909\n",
            "31286    5.714286\n",
            "31287    7.777778\n",
            "31288    5.833333\n",
            "Name: Feature 24, Length: 31289, dtype: float64 there was a loss of -0.0008691113644351267 in F1\n",
            "To remove 0        1.945910\n",
            "1        1.654558\n",
            "2        1.689481\n",
            "3        1.488077\n",
            "4        1.667707\n",
            "           ...   \n",
            "31284    1.609438\n",
            "31285    1.627456\n",
            "31286    1.742969\n",
            "31287    2.051271\n",
            "31288    1.763589\n",
            "Name: Feature 25, Length: 31289, dtype: float64 there was a loss of -0.0006786281748764056 in F1\n",
            "To remove 0        0.0\n",
            "1        0.0\n",
            "2        0.0\n",
            "3        0.0\n",
            "4        0.0\n",
            "        ... \n",
            "31284    0.0\n",
            "31285    0.0\n",
            "31286    0.0\n",
            "31287    0.0\n",
            "31288    0.0\n",
            "Name: Feature 28, Length: 31289, dtype: float64 there was a loss of -0.0006786281748764056 in F1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(removed_features)):\n",
        "  print(\"To remove \" + str(removed_features[i].name) + \" there was a loss of \" + str(f1_diff_add[i]) + \" in F1\")"
      ],
      "metadata": {
        "id": "rsK5cb3dC-6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf842e77-05c9-4c44-cce6-040007f6ee94"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To remove feature11 there was a loss of -0.00040514214286968553 in F1\n",
            "To remove feature12 there was a loss of -0.0009208587882938435 in F1\n",
            "To remove Feature 15 there was a loss of -0.0011514063387261242 in F1\n",
            "To remove Feature 16 there was a loss of -0.00014822803428493625 in F1\n",
            "To remove Feature 17 there was a loss of -0.0014919486600187826 in F1\n",
            "To remove Feature 18 there was a loss of -0.0014919486600187826 in F1\n",
            "To remove Feature 19 there was a loss of -0.0005977354400960033 in F1\n",
            "To remove Feature 20 there was a loss of -0.0006930006930007115 in F1\n",
            "To remove Feature 23 there was a loss of -0.0006930006930007115 in F1\n",
            "To remove Feature 24 there was a loss of -0.0008691113644351267 in F1\n",
            "To remove Feature 25 there was a loss of -0.0006786281748764056 in F1\n",
            "To remove Feature 28 there was a loss of -0.0006786281748764056 in F1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_diffs_add_percent = []\n",
        "maxnum=(min(f1_diff_add))\n",
        "for i in range (0,len(f1_diff_add)):\n",
        "  num=f1_diff_add[i]\n",
        "  val = 100 * (num/maxnum)\n",
        "  f1_diffs_add_percent.append(val)\n",
        "  print(\"The feature \" + str(removed_features[i].name) + \" has a relative importance percentage \" + str(f1_diffs_add_percent[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKd4VsnNobLY",
        "outputId": "ddcad096-5325-4032-d731-536deaba8ebb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The feature feature11 has a relative importance percentage 27.1552335362924\n",
            "The feature feature12 has a relative importance percentage 61.72188178930035\n",
            "The feature Feature 15 has a relative importance percentage 77.17466221067076\n",
            "The feature Feature 16 has a relative importance percentage 9.935196716693333\n",
            "The feature Feature 17 has a relative importance percentage 100.0\n",
            "The feature Feature 18 has a relative importance percentage 100.0\n",
            "The feature Feature 19 has a relative importance percentage 40.064075669231016\n",
            "The feature Feature 20 has a relative importance percentage 46.449365958208446\n",
            "The feature Feature 23 has a relative importance percentage 46.449365958208446\n",
            "The feature Feature 24 has a relative importance percentage 58.25343644359621\n",
            "The feature Feature 25 has a relative importance percentage 45.486027305246694\n",
            "The feature Feature 28 has a relative importance percentage 45.486027305246694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try 1 Remove Feature"
      ],
      "metadata": {
        "id": "-5A_vwhaWKCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def try_remove_feature(x,y,feat,target):\n",
        "  x_1=x\n",
        "  x_1=x_1.drop(columns=feat)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_1, y, test_size=0.2, random_state=42)\n",
        "  X_train_norm, X_test_norm = mms.fit_transform(X_train), mms.transform(X_test)\n",
        "  feature_number, lr, epochs = X_train.shape[1], .5, 1000000\n",
        "  LogRegSent = LogRegression(feature_number, lr, epochs)\n",
        "  losses = LogRegSent.fit(X_train_norm, y_train)\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Loss vs Iterations on Twitter Sentiment\")\n",
        "  plt.plot(losses)\n",
        "  plt.show\n",
        "  y_pred = LogRegSent.predict(X_test_norm)\n",
        "  cm = LogRegSent.metrics(y_pred, y_test)\n",
        "  f1=(2*cm[1][1]/(cm[1][1]+cm[0][1]))*(cm[1][1]/(cm[1][1]+cm[1][0]))/((cm[1][1]/(cm[1][1]+cm[1][0]))+(cm[1][1]/(cm[1][1]+cm[0][1])))\n",
        "  print(\"The F1 with \" + str(feat) + \" is: \" + str(f1) + \" but the target was: \" +str(target))\n",
        "  diff=target-f1\n",
        "  print(\"The difference in F1 score without \" + str(feat) + \" is: \" + str(diff))\n",
        "  return diff\n",
        "\n"
      ],
      "metadata": {
        "id": "b8_3fAeyWJm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features=Xc.columns\n",
        "f1_diff_rem=[]\n",
        "f1_selected = 0.8242088242088242 #this should be F1 score for logreg of all_features\n",
        "for i in range(0,len(all_features)):\n",
        "  new=all_features[i]\n",
        "  difference=try_remove_feature(Xc,yc,new,f1_selected)\n",
        "  f1_diff_rem.append(difference)\n",
        "  print(\"---------------\")\n",
        "for i in range(0,len(all_features)):\n",
        "  print(\"To remove \" + str(all_features[i].name) + \" there was a loss of \" + str(f1_diff_rem[i]) + \" in F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "24OBixYSW3O2",
        "outputId": "8b6fc1ec-2108-4b90-f25e-84ec68008146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5195032834191019\n",
            "The iteration is 2000 and the loss is 0.5166754381451932\n",
            "The iteration is 3000 and the loss is 0.5153853357807695\n",
            "The weight is \n",
            "[ 1.38788573  0.42138254  1.33821941 -0.40812539  0.98464068  0.13472333\n",
            "  1.97333996  1.72094986 -0.5082399   0.2135605  -0.82109479  3.40225034\n",
            " -4.55484692  0.5213249  -0.12816906 -0.85950083  0.03213392 -0.82745052\n",
            "  0.5021736  -0.04294552  0.31335241  0.         -0.13379037 -0.45237655\n",
            "  1.16061248  1.31735088 -0.02851458]\n",
            "The intercept is \n",
            "-2.404582314802324\n",
            "The confusion matrix is: \n",
            "[1141, 1044]\n",
            "[508, 3565]\n",
            "The accuracy for the Twitter sentiment is 0.7519974432726111\n",
            "The precision for the Twitter sentiment is 0.7734866565415491\n",
            "The recall for the Twitter sentiment is 0.8752762091824208\n",
            "The F1 score for the Twitter sentiment is 0.8212393457728634\n",
            "The F1 with feature1 is: 0.8212393457728634 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature1 is: 0.0029694784359608484\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5156744187564261\n",
            "The iteration is 2000 and the loss is 0.5112701149761044\n",
            "The iteration is 3000 and the loss is 0.5090864029506386\n",
            "The iteration is 4000 and the loss is 0.5076592583088955\n",
            "The weight is \n",
            "[ 3.89300057  0.32217605  1.28394971 -0.73800762  0.92132022 -0.02914517\n",
            "  2.11227998  1.61621068 -0.7237316   0.31015901 -0.86308615  3.2526237\n",
            " -4.49572274  0.78559232 -0.16328931 -0.89818587  0.02472535 -0.95662373\n",
            "  0.64476978 -0.05434752  0.29795909  0.         -0.09391688 -0.47767544\n",
            "  1.19292466  1.18411307 -0.01600621]\n",
            "The intercept is \n",
            "-3.4224148536918606\n",
            "The confusion matrix is: \n",
            "[1161, 1024]\n",
            "[501, 3572]\n",
            "The accuracy for the Twitter sentiment is 0.7563119207414509\n",
            "The precision for the Twitter sentiment is 0.7771975630983464\n",
            "The recall for the Twitter sentiment is 0.8769948440952615\n",
            "The F1 score for the Twitter sentiment is 0.8240858230476411\n",
            "The F1 with feature2 is: 0.8240858230476411 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature2 is: 0.00012300116118313387\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5149663477588768\n",
            "The iteration is 2000 and the loss is 0.5104940502717044\n",
            "The iteration is 3000 and the loss is 0.5082498122615864\n",
            "The iteration is 4000 and the loss is 0.5067631742034245\n",
            "The weight is \n",
            "[ 3.88208245  1.37678164  1.21134927 -0.79338067  0.90193486 -0.09159002\n",
            "  2.03864085  1.54468323 -0.80076912  0.32414347 -0.85933277  3.19510276\n",
            " -4.42152867  0.8337186  -0.1386444  -0.92948761  0.04184508 -1.00544919\n",
            "  0.65288521 -0.05416277  0.31956395  0.         -0.06847171 -0.45845779\n",
            "  1.20677318  1.17846778 -0.01353338]\n",
            "The intercept is \n",
            "-3.6738916833975614\n",
            "The confusion matrix is: \n",
            "[1167, 1018]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7566315116650687\n",
            "The precision for the Twitter sentiment is 0.7780200610553859\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8241136389883358\n",
            "The F1 with feature3 is: 0.8241136389883358 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature3 is: 9.518522048845846e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5153473547806267\n",
            "The iteration is 2000 and the loss is 0.5110995515533718\n",
            "The iteration is 3000 and the loss is 0.5090390626117891\n",
            "The iteration is 4000 and the loss is 0.5077264408368743\n",
            "The weight is \n",
            "[ 3.85363598  1.38593278  0.23971617 -0.6413276   0.89963901 -0.01302227\n",
            "  2.03462277  1.5787128  -0.67192798  0.3061452  -0.84836412  3.21311651\n",
            " -4.40924201  0.70468561 -0.16756077 -0.87992682  0.03920358 -0.94331858\n",
            "  0.61015428 -0.0519744   0.32900135  0.         -0.09240502 -0.45253086\n",
            "  1.22298265  1.22303389 -0.01666168]\n",
            "The intercept is \n",
            "-3.1699781445451793\n",
            "The confusion matrix is: \n",
            "[1163, 1022]\n",
            "[508, 3565]\n",
            "The accuracy for the Twitter sentiment is 0.7555129434324065\n",
            "The precision for the Twitter sentiment is 0.7771964246784391\n",
            "The recall for the Twitter sentiment is 0.8752762091824208\n",
            "The F1 score for the Twitter sentiment is 0.8233256351039261\n",
            "The F1 with feature4 is: 0.8233256351039261 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature4 is: 0.0008831891048981522\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5151965006745272\n",
            "The iteration is 2000 and the loss is 0.5107627937929047\n",
            "The iteration is 3000 and the loss is 0.5084924983703768\n",
            "The iteration is 4000 and the loss is 0.5069680846468868\n",
            "The weight is \n",
            "[ 3.873698    1.33094273  0.14393977  1.16321847  0.82652037 -0.14683937\n",
            "  1.99190713  1.5004878  -0.85037341  0.32514791 -0.85853544  3.20786151\n",
            " -4.40866335  0.86725551 -0.16503862 -0.95655588  0.04549558 -1.04960148\n",
            "  0.66675213 -0.05697846  0.30270207  0.         -0.06873847 -0.47484987\n",
            "  1.14735886  1.16343372 -0.01702528]\n",
            "The intercept is \n",
            "-3.8828625746915786\n",
            "The confusion matrix is: \n",
            "[1167, 1018]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7566315116650687\n",
            "The precision for the Twitter sentiment is 0.7780200610553859\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8241136389883358\n",
            "The F1 with feature5 is: 0.8241136389883358 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature5 is: 9.518522048845846e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5153177668536212\n",
            "The iteration is 2000 and the loss is 0.5108369279898278\n",
            "The iteration is 3000 and the loss is 0.5085905082340404\n",
            "The iteration is 4000 and the loss is 0.5071114849273989\n",
            "The weight is \n",
            "[ 3.90391098  1.37594229  0.26751213  1.23032068 -0.72346639 -0.05979905\n",
            "  2.04309058  1.67445916 -0.7720665   0.34179449 -0.87702462  3.22659569\n",
            " -4.44081143  0.81424992 -0.14079607 -0.91619618  0.04311617 -0.99262866\n",
            "  0.65448553 -0.05375779  0.3394635   0.         -0.06654737 -0.44535717\n",
            "  1.24328876  1.1845878  -0.0106999 ]\n",
            "The intercept is \n",
            "-3.5797756286187354\n",
            "The confusion matrix is: \n",
            "[1153, 1032]\n",
            "[498, 3575]\n",
            "The accuracy for the Twitter sentiment is 0.7555129434324065\n",
            "The precision for the Twitter sentiment is 0.7759930540481875\n",
            "The recall for the Twitter sentiment is 0.8777314019150503\n",
            "The F1 score for the Twitter sentiment is 0.8237327188940092\n",
            "The F1 with feature6 is: 0.8237327188940092 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature6 is: 0.00047610531481501184\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.515007304000119\n",
            "The iteration is 2000 and the loss is 0.510561415649785\n",
            "The iteration is 3000 and the loss is 0.5083060718212948\n",
            "The iteration is 4000 and the loss is 0.5068061543328343\n",
            "The weight is \n",
            "[ 3.88435112  1.34649296  0.18509758  1.21672096 -0.82399976  0.87377983\n",
            "  2.01654164  1.52437351 -0.79988939  0.32851304 -0.85520754  3.19369099\n",
            " -4.40856185  0.8366197  -0.15405954 -0.93033784  0.04453249 -1.0174144\n",
            "  0.65861006 -0.0550968   0.31809731  0.         -0.07018153 -0.46225614\n",
            "  1.17459693  1.17080059 -0.01380708]\n",
            "The intercept is \n",
            "-3.7003345363280395\n",
            "The confusion matrix is: \n",
            "[1167, 1018]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7566315116650687\n",
            "The precision for the Twitter sentiment is 0.7780200610553859\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8241136389883358\n",
            "The F1 with feature7 is: 0.8241136389883358 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature7 is: 9.518522048845846e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5170217275433577\n",
            "The iteration is 2000 and the loss is 0.5126505121698195\n",
            "The iteration is 3000 and the loss is 0.5105057267627562\n",
            "The iteration is 4000 and the loss is 0.50909308857145\n",
            "The weight is \n",
            "[ 3.87874736  1.54668836  0.40701785  1.31062244 -0.63965966  0.96156914\n",
            "  0.02831023  1.59792456 -0.78573545  0.28452699 -0.82503012  3.37318756\n",
            " -4.62599925  0.79637606 -0.084562   -0.90787486  0.05720323 -0.85375895\n",
            "  0.64619329 -0.05723703  0.28780239  0.         -0.07749695 -0.44306085\n",
            "  1.226793    1.17432616 -0.02320783]\n",
            "The intercept is \n",
            "-3.437290324195449\n",
            "The confusion matrix is: \n",
            "[1138, 1047]\n",
            "[501, 3572]\n",
            "The accuracy for the Twitter sentiment is 0.7526366251198466\n",
            "The precision for the Twitter sentiment is 0.7733275600779389\n",
            "The recall for the Twitter sentiment is 0.8769948440952615\n",
            "The F1 score for the Twitter sentiment is 0.8219052001840774\n",
            "The F1 with feature8 is: 0.8219052001840774 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature8 is: 0.00230362402474682\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5158279440024268\n",
            "The iteration is 2000 and the loss is 0.5113641355763351\n",
            "The iteration is 3000 and the loss is 0.5091340046522805\n",
            "The iteration is 4000 and the loss is 0.5076706533600748\n",
            "The weight is \n",
            "[ 4.01245264  1.44928823  0.29083184  1.31594459 -0.72054143  1.11128272\n",
            "  0.00889131  2.05133621 -0.71050217  0.33860121 -0.89518562  3.27461456\n",
            " -4.46202417  0.77135849 -0.17558375 -0.91117154  0.03832018 -1.00011964\n",
            "  0.65403679 -0.05308554  0.34328221  0.         -0.09041085 -0.47836342\n",
            "  1.28465235  1.22101945 -0.01379116]\n",
            "The intercept is \n",
            "-3.4456680442728023\n",
            "The confusion matrix is: \n",
            "[1169, 1016]\n",
            "[503, 3570]\n",
            "The accuracy for the Twitter sentiment is 0.7572706935123042\n",
            "The precision for the Twitter sentiment is 0.7784561709550807\n",
            "The recall for the Twitter sentiment is 0.8765038055487355\n",
            "The F1 score for the Twitter sentiment is 0.824575586095392\n",
            "The F1 with feature9 is: 0.824575586095392 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature9 is: -0.0003667618865678124\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5152275717049293\n",
            "The iteration is 2000 and the loss is 0.5107352610827095\n",
            "The iteration is 3000 and the loss is 0.5084418418266965\n",
            "The iteration is 4000 and the loss is 0.5069087148908721\n",
            "The weight is \n",
            "[ 3.88521663e+00  1.33070127e+00  1.90017374e-01  1.18145539e+00\n",
            " -8.81035441e-01  8.69037906e-01 -1.37683742e-01  2.02949305e+00\n",
            "  1.50311174e+00  2.51878306e-01 -1.06904175e+00  3.03298807e+00\n",
            " -4.56588618e+00  5.85427917e-01 -2.17453235e-01 -8.90008591e-01\n",
            "  3.88983615e-02 -8.88137445e-01  6.62884732e-01 -5.88344450e-02\n",
            "  3.09309310e-01  0.00000000e+00  5.28061472e-03 -3.61556359e-01\n",
            "  1.16629416e+00  1.16898774e+00 -1.81430363e-03]\n",
            "The intercept is \n",
            "-3.926557552739841\n",
            "The confusion matrix is: \n",
            "[1165, 1020]\n",
            "[507, 3566]\n",
            "The accuracy for the Twitter sentiment is 0.7559923298178332\n",
            "The precision for the Twitter sentiment is 0.7775839511556912\n",
            "The recall for the Twitter sentiment is 0.8755217284556838\n",
            "The F1 score for the Twitter sentiment is 0.8236516918812796\n",
            "The F1 with feature10 is: 0.8236516918812796 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature10 is: 0.0005571323275446183\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5149441594292503\n",
            "The iteration is 2000 and the loss is 0.5105797260689109\n",
            "The iteration is 3000 and the loss is 0.5083571745638937\n",
            "The iteration is 4000 and the loss is 0.5068708689654413\n",
            "The weight is \n",
            "[ 3.88895029  1.35082725  0.191942    1.22828627 -0.80810361  0.88519821\n",
            " -0.10756622  2.01912353  1.53260843 -0.78236837 -0.81667359  3.17697749\n",
            " -4.41610834  0.83491474 -0.14792981 -0.90947057  0.0439544  -1.01143791\n",
            "  0.65617868 -0.05406352  0.32752774  0.         -0.02194084 -0.38261435\n",
            "  1.1797327   1.18058068 -0.01086104]\n",
            "The intercept is \n",
            "-3.6658707622543636\n",
            "The confusion matrix is: \n",
            "[1165, 1020]\n",
            "[506, 3567]\n",
            "The accuracy for the Twitter sentiment is 0.756152125279642\n",
            "The precision for the Twitter sentiment is 0.7776324395029431\n",
            "The recall for the Twitter sentiment is 0.8757672477289468\n",
            "The F1 score for the Twitter sentiment is 0.8237875288683603\n",
            "The F1 with feature11 is: 0.8237875288683603 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature11 is: 0.00042129534046397854\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5153775746293254\n",
            "The iteration is 2000 and the loss is 0.5108161487712988\n",
            "The iteration is 3000 and the loss is 0.5084950754932519\n",
            "The iteration is 4000 and the loss is 0.5069536378974138\n",
            "The weight is \n",
            "[ 3.90920422  1.36802113  0.20316104  1.2457734  -0.823826    0.91636569\n",
            " -0.09933777  2.01298702  1.57559867 -1.21681489  0.37739156  3.11019255\n",
            " -4.47089366  0.64659564 -0.23516013 -0.90583245  0.06001472 -0.93989519\n",
            "  0.67399505 -0.04801538  0.38349324  0.         -0.30071813 -0.96325306\n",
            "  1.18604543  1.19875766  0.01249997]\n",
            "The intercept is \n",
            "-3.796086547498959\n",
            "The confusion matrix is: \n",
            "[1160, 1025]\n",
            "[507, 3566]\n",
            "The accuracy for the Twitter sentiment is 0.7551933525087887\n",
            "The precision for the Twitter sentiment is 0.7767370943149641\n",
            "The recall for the Twitter sentiment is 0.8755217284556838\n",
            "The F1 score for the Twitter sentiment is 0.8231763619575253\n",
            "The F1 with feature12 is: 0.8231763619575253 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without feature12 is: 0.0010324622512989379\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5237442001957019\n",
            "The iteration is 2000 and the loss is 0.5156195505399316\n",
            "The iteration is 3000 and the loss is 0.5122554608199105\n",
            "The iteration is 4000 and the loss is 0.5102509721933525\n",
            "The iteration is 5000 and the loss is 0.508929600061987\n",
            "The weight is \n",
            "[ 4.29790655e+00  1.57871444e+00  1.70938922e-01  1.24396356e+00\n",
            " -1.09487040e+00  1.07816977e+00 -2.63313016e-02  2.40110872e+00\n",
            "  1.88519463e+00  7.32881805e-01  1.59570853e-01 -4.50184460e-01\n",
            " -5.90855439e+00  2.37904265e+00 -3.45547470e-03 -2.74273109e-01\n",
            "  8.98486441e-02 -1.92770975e+00  7.21203765e-01 -1.14128693e-01\n",
            "  4.07207682e-02  0.00000000e+00 -5.42553363e-01 -1.40992552e+00\n",
            "  1.59216899e+00  1.03568685e+00 -3.36820447e-02]\n",
            "The intercept is \n",
            "-4.75909761330084\n",
            "The confusion matrix is: \n",
            "[1143, 1042]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7527964205816555\n",
            "The precision for the Twitter sentiment is 0.7739696312364425\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8218357710468731\n",
            "The F1 with Feature 13 is: 0.8218357710468731 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 13 is: 0.002373053161951133\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.535452552409265\n",
            "The iteration is 2000 and the loss is 0.5248117622088769\n",
            "The iteration is 3000 and the loss is 0.5210698889216555\n",
            "The iteration is 4000 and the loss is 0.5191969126421613\n",
            "The weight is \n",
            "[ 4.27694281e+00  2.00921917e+00  7.60403880e-01  1.12961130e+00\n",
            " -8.45905310e-01  1.24154690e+00 -5.93256784e-02  2.81720348e+00\n",
            "  1.91484895e+00 -3.59358723e+00  5.90597071e-01 -1.51484791e+00\n",
            "  5.76370729e+00 -6.08094591e-01 -6.81360843e-02 -1.84186492e+00\n",
            " -5.30491450e-02  2.56279175e-03  6.96794296e-01 -3.34053812e-02\n",
            "  8.15037107e-01  0.00000000e+00  5.50209224e-01  5.07784249e-01\n",
            "  1.03600482e+00  1.78687598e+00 -1.28332567e-02]\n",
            "The intercept is \n",
            "-4.46390762834285\n",
            "The confusion matrix is: \n",
            "[1133, 1052]\n",
            "[541, 3532]\n",
            "The accuracy for the Twitter sentiment is 0.7454458293384468\n",
            "The precision for the Twitter sentiment is 0.7705061082024433\n",
            "The recall for the Twitter sentiment is 0.8671740731647435\n",
            "The F1 score for the Twitter sentiment is 0.8159870624927805\n",
            "The F1 with Feature 14 is: 0.8159870624927805 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 14 is: 0.008221761716043718\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5151811015268057\n",
            "The iteration is 2000 and the loss is 0.5108383341301017\n",
            "The iteration is 3000 and the loss is 0.5086544758050106\n",
            "The iteration is 4000 and the loss is 0.5071981072442232\n",
            "The weight is \n",
            "[ 3.84560993  1.33459737  0.20154458  1.16253334 -0.84231439  0.86698046\n",
            " -0.14837825  2.01619106  1.500043   -0.49932722  0.36027731 -0.71072716\n",
            "  3.39155132 -4.29805123 -0.15754463 -1.04240371  0.04292104 -0.57688108\n",
            "  0.66735564 -0.05036254  0.333016    0.         -0.10716818 -0.53512538\n",
            "  1.18154716  1.16640348 -0.0183777 ]\n",
            "The intercept is \n",
            "-3.7563902308726536\n",
            "The confusion matrix is: \n",
            "[1166, 1019]\n",
            "[508, 3565]\n",
            "The accuracy for the Twitter sentiment is 0.7559923298178332\n",
            "The precision for the Twitter sentiment is 0.7777050610820244\n",
            "The recall for the Twitter sentiment is 0.8752762091824208\n",
            "The F1 score for the Twitter sentiment is 0.8236109506757537\n",
            "The F1 with Feature 15 is: 0.8236109506757537 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 15 is: 0.0005978735330705032\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150354509827182\n",
            "The iteration is 2000 and the loss is 0.5105825557551749\n",
            "The iteration is 3000 and the loss is 0.5083166723824316\n",
            "The iteration is 4000 and the loss is 0.5068077253668886\n",
            "The weight is \n",
            "[ 3.89395189  1.35377791  0.18025905  1.22988306 -0.82243857  0.87648764\n",
            " -0.1104641   2.01649445  1.53306388 -0.82456195  0.32700935 -0.87824039\n",
            "  3.17883794 -4.41243274  0.83603338 -0.90832697  0.04564653 -0.98164825\n",
            "  0.66002277 -0.05594079  0.31102747  0.         -0.06946984 -0.46087386\n",
            "  1.17848338  1.1778204  -0.01423398]\n",
            "The intercept is \n",
            "-3.7011718674794842\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[502, 3571]\n",
            "The accuracy for the Twitter sentiment is 0.7572706935123042\n",
            "The precision for the Twitter sentiment is 0.7783347863993025\n",
            "The recall for the Twitter sentiment is 0.8767493248219985\n",
            "The F1 score for the Twitter sentiment is 0.8246160951391295\n",
            "The F1 with Feature 16 is: 0.8246160951391295 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 16 is: -0.00040727093030523687\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5161272221873754\n",
            "The iteration is 2000 and the loss is 0.5110700454965597\n",
            "The iteration is 3000 and the loss is 0.5084514268593799\n",
            "The iteration is 4000 and the loss is 0.5067421389177502\n",
            "The weight is \n",
            "[ 3.91839519  1.33001448  0.17150642  1.18235787 -0.97027991  0.86991436\n",
            " -0.17036143  2.03001587  1.53920009 -0.72467851  0.29607303 -0.83443821\n",
            "  2.97956431 -4.59073856  1.1829923  -0.06014548  0.03274856 -1.20362846\n",
            "  0.67079824 -0.05979511  0.37637343  0.         -0.10428882 -0.60098256\n",
            "  1.16360795  1.1848247  -0.01196549]\n",
            "The intercept is \n",
            "-4.35949232825105\n",
            "The confusion matrix is: \n",
            "[1167, 1018]\n",
            "[504, 3569]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.7780684543274471\n",
            "The recall for the Twitter sentiment is 0.8762582862754726\n",
            "The F1 score for the Twitter sentiment is 0.8242494226327944\n",
            "The F1 with Feature 17 is: 0.8242494226327944 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 17 is: -4.0598423970195086e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150547687564873\n",
            "The iteration is 2000 and the loss is 0.510567815340851\n",
            "The iteration is 3000 and the loss is 0.5082945495750792\n",
            "The iteration is 4000 and the loss is 0.5067920562742945\n",
            "The weight is \n",
            "[ 3.88686493  1.35077962  0.19004055  1.22315183 -0.81673121  0.87907191\n",
            " -0.11021108  2.02060033  1.52984364 -0.79529595  0.32909252 -0.85447099\n",
            "  3.19597005 -4.40990268  0.83210382 -0.15344177 -0.92265264 -1.0168355\n",
            "  0.65733129 -0.05489878  0.31955981  0.         -0.07060287 -0.46185208\n",
            "  1.18246181  1.17630911 -0.01386411]\n",
            "The intercept is \n",
            "-3.6863634143603163\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.7781897491821156\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8242088242088242\n",
            "The F1 with Feature 18 is: 0.8242088242088242 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 18 is: 0.0\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5156972511917469\n",
            "The iteration is 2000 and the loss is 0.5113388468218016\n",
            "The iteration is 3000 and the loss is 0.5090734216168284\n",
            "The iteration is 4000 and the loss is 0.5075459063827239\n",
            "The weight is \n",
            "[ 3.90185902  1.29567261  0.09066389  1.18290375 -0.90687248  0.84002338\n",
            " -0.1010104   1.92302704  1.52667544 -0.45601355  0.37562971 -0.74229139\n",
            "  3.42979896 -4.22435445 -0.17651646  0.09138233 -1.05390477  0.05689525\n",
            "  0.63719692 -0.0607673   0.29247979  0.         -0.14093305 -0.61533828\n",
            "  1.22218164  1.15304391 -0.03399975]\n",
            "The intercept is \n",
            "-3.9831538591243567\n",
            "The confusion matrix is: \n",
            "[1158, 1027]\n",
            "[519, 3554]\n",
            "The accuracy for the Twitter sentiment is 0.7529562160434644\n",
            "The precision for the Twitter sentiment is 0.7758131412355381\n",
            "The recall for the Twitter sentiment is 0.8725754971765284\n",
            "The F1 score for the Twitter sentiment is 0.8213542870348972\n",
            "The F1 with Feature 19 is: 0.8213542870348972 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 19 is: 0.0028545371739270298\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5151293522800031\n",
            "The iteration is 2000 and the loss is 0.5106887720962705\n",
            "The iteration is 3000 and the loss is 0.5084481568682315\n",
            "The iteration is 4000 and the loss is 0.5069686698732397\n",
            "The weight is \n",
            "[ 3.869791    1.34807474  0.19114453  1.21317655 -0.81347858  0.87837946\n",
            " -0.10755449  2.0179319   1.52758128 -0.79062778  0.32603843 -0.85766332\n",
            "  3.20053292 -4.41273121  0.83379805 -0.15473939 -0.92495146  0.04549766\n",
            " -1.00556226 -0.05422661  0.3196353   0.         -0.07429843 -0.46610875\n",
            "  1.17797939  1.174371   -0.01456245]\n",
            "The intercept is \n",
            "-3.6647790154879067\n",
            "The confusion matrix is: \n",
            "[1166, 1019]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7564717162032598\n",
            "The precision for the Twitter sentiment is 0.7778504469151951\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8240184757505774\n",
            "The F1 with Feature 20 is: 0.8240184757505774 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 20 is: 0.00019034845824683622\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.515055208752334\n",
            "The iteration is 2000 and the loss is 0.510568056382808\n",
            "The iteration is 3000 and the loss is 0.5082945981882143\n",
            "The iteration is 4000 and the loss is 0.5067919289154443\n",
            "The weight is \n",
            "[ 3.88683754  1.3510313   0.19009592  1.2231757  -0.81678937  0.8790572\n",
            " -0.11031334  2.02052366  1.52986512 -0.79557849  0.32886205 -0.85411883\n",
            "  3.19605746 -4.41003717  0.83194776 -0.15342975 -0.92282529  0.04404901\n",
            " -1.01676184  0.657232    0.31919897  0.         -0.07064852 -0.46171349\n",
            "  1.18262695  1.17625319 -0.01393839]\n",
            "The intercept is \n",
            "-3.6865131502760313\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.7781897491821156\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8242088242088242\n",
            "The F1 with Feature 21 is: 0.8242088242088242 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 21 is: 0.0\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150957826851749\n",
            "The iteration is 2000 and the loss is 0.5106261867588319\n",
            "The iteration is 3000 and the loss is 0.5083682285004711\n",
            "The iteration is 4000 and the loss is 0.5068786958918386\n",
            "The weight is \n",
            "[ 3.88082899  1.34399769  0.19716851  1.22675593 -0.79936919  0.88914586\n",
            " -0.10415132  2.0146679   1.53628684 -0.78588497  0.34126899 -0.87592048\n",
            "  3.17677379 -4.4298182   0.83636586 -0.13518571 -0.93356151  0.04222982\n",
            " -1.00657857  0.6557852  -0.04845923  0.         -0.0729883  -0.46893315\n",
            "  1.18017469  1.17970741 -0.01690331]\n",
            "The intercept is \n",
            "-3.6437427640023032\n",
            "The confusion matrix is: \n",
            "[1170, 1015]\n",
            "[504, 3569]\n",
            "The accuracy for the Twitter sentiment is 0.7572706935123042\n",
            "The precision for the Twitter sentiment is 0.7785776614310645\n",
            "The recall for the Twitter sentiment is 0.8762582862754726\n",
            "The F1 score for the Twitter sentiment is 0.824535058334296\n",
            "The F1 with Feature 22 is: 0.824535058334296 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 22 is: -0.0003262341254717249\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150547527386746\n",
            "The iteration is 2000 and the loss is 0.5105674722707798\n",
            "The iteration is 3000 and the loss is 0.5082939009504399\n",
            "The iteration is 4000 and the loss is 0.5067911279795699\n",
            "The weight is \n",
            "[ 3.88718406  1.35110688  0.19007335  1.22333126 -0.81680121  0.87911775\n",
            " -0.11038861  2.02056978  1.52998215 -0.79551275  0.32910098 -0.85426255\n",
            "  3.19578628 -4.41007129  0.83225729 -0.15324494 -0.9227016   0.04407588\n",
            " -1.01677211  0.65734686 -0.05491463  0.31966306 -0.07050237 -0.46171003\n",
            "  1.18255135  1.17623994 -0.01384957]\n",
            "The intercept is \n",
            "-3.6869069739039104\n",
            "The confusion matrix is: \n",
            "[1168, 1017]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.7781897491821156\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8242088242088242\n",
            "The F1 with Feature 23 is: 0.8242088242088242 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 23 is: 0.0\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150142338198576\n",
            "The iteration is 2000 and the loss is 0.5105539884157846\n",
            "The iteration is 3000 and the loss is 0.5082906524502955\n",
            "The iteration is 4000 and the loss is 0.506791052574468\n",
            "The weight is \n",
            "[ 3.88695946  1.35181023  0.18970414  1.22450136 -0.81556203  0.87864748\n",
            " -0.11007864  2.02065865  1.53011812 -0.78569117  0.3141036  -0.86207892\n",
            "  3.19485328 -4.40957774  0.83356587 -0.15335383 -0.92139386  0.04418809\n",
            " -1.01681521  0.65743954 -0.0554071   0.31985497  0.         -0.50293613\n",
            "  1.1814963   1.1765914  -0.01534731]\n",
            "The intercept is \n",
            "-3.682287180587306\n",
            "The confusion matrix is: \n",
            "[1169, 1016]\n",
            "[506, 3567]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.778311149901811\n",
            "The recall for the Twitter sentiment is 0.8757672477289468\n",
            "The F1 score for the Twitter sentiment is 0.8241682070240296\n",
            "The F1 with Feature 24 is: 0.8241682070240296 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 24 is: 4.06171847946446e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150178449418232\n",
            "The iteration is 2000 and the loss is 0.510551605383475\n",
            "The iteration is 3000 and the loss is 0.5082968584172213\n",
            "The iteration is 4000 and the loss is 0.5067991157768873\n",
            "The weight is \n",
            "[ 3.88646761  1.35725088  0.18618334  1.22476429 -0.82172072  0.87260192\n",
            " -0.1126344   2.01919814  1.53530634 -0.72739916  0.15785985 -1.00695294\n",
            "  3.2411513  -4.37934203  0.87389706 -0.1505644  -0.93878756  0.04662416\n",
            " -1.04557611  0.66088801 -0.05449488  0.32850514  0.         -0.28586115\n",
            "  1.18595448  1.17655289 -0.02164327]\n",
            "The intercept is \n",
            "-3.7092354483404413\n",
            "The confusion matrix is: \n",
            "[1170, 1015]\n",
            "[507, 3566]\n",
            "The accuracy for the Twitter sentiment is 0.7567913071268776\n",
            "The precision for the Twitter sentiment is 0.778432656625191\n",
            "The recall for the Twitter sentiment is 0.8755217284556838\n",
            "The F1 score for the Twitter sentiment is 0.8241275710654032\n",
            "The F1 with Feature 25 is: 0.8241275710654032 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 25 is: 8.125314342100065e-05\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.516089274176429\n",
            "The iteration is 2000 and the loss is 0.5116403038336834\n",
            "The iteration is 3000 and the loss is 0.5094292355637013\n",
            "The iteration is 4000 and the loss is 0.5079622703700885\n",
            "The weight is \n",
            "[ 3.84428309  1.37258381  0.43509984  1.31702787 -0.65648873  1.0516663\n",
            "  0.08687628  2.06011743  1.70621071 -0.69873455  0.31963274 -0.84997776\n",
            "  3.38265085 -4.38180402  0.82289358 -0.12822826 -0.88203965  0.03550625\n",
            " -1.05651676  0.63480375 -0.06317884  0.30761668  0.         -0.11516276\n",
            " -0.51447832  1.17506417 -0.01007103]\n",
            "The intercept is \n",
            "-3.5972522864615852\n",
            "The confusion matrix is: \n",
            "[1156, 1029]\n",
            "[504, 3569]\n",
            "The accuracy for the Twitter sentiment is 0.7550335570469798\n",
            "The precision for the Twitter sentiment is 0.7762070465419748\n",
            "The recall for the Twitter sentiment is 0.8762582862754726\n",
            "The F1 score for the Twitter sentiment is 0.8232037827240226\n",
            "The F1 with Feature 26 is: 0.8232037827240226 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 26 is: 0.0010050414848016276\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5165553135610594\n",
            "The iteration is 2000 and the loss is 0.512033922504869\n",
            "The iteration is 3000 and the loss is 0.5097920068767684\n",
            "The iteration is 4000 and the loss is 0.5083203949715727\n",
            "The weight is \n",
            "[ 3.99402033  1.37256383  0.27910915  1.34285265 -0.73473421  0.91980272\n",
            "  0.02761551  2.02031737  1.63614813 -0.75957689  0.345572   -0.92105245\n",
            "  3.11831609 -4.61857629  0.79879867 -0.16638057 -0.91544527  0.05472991\n",
            " -0.96933368  0.6408766  -0.05716972  0.34091627  0.         -0.04808628\n",
            " -0.42891953  1.18174667  0.0089774 ]\n",
            "The intercept is \n",
            "-3.454931021984293\n",
            "The confusion matrix is: \n",
            "[1151, 1034]\n",
            "[513, 3560]\n",
            "The accuracy for the Twitter sentiment is 0.7527964205816555\n",
            "The precision for the Twitter sentiment is 0.7749238136700044\n",
            "The recall for the Twitter sentiment is 0.874048612816106\n",
            "The F1 score for the Twitter sentiment is 0.8215068651205722\n",
            "The F1 with Feature 27 is: 0.8215068651205722 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 27 is: 0.002701959088251993\n",
            "---------------\n",
            "The iteration is 0 and the loss is 0.6931471805599453\n",
            "The iteration is 1000 and the loss is 0.5150548208896456\n",
            "The iteration is 2000 and the loss is 0.5105674049878149\n",
            "The iteration is 3000 and the loss is 0.5082937521616797\n",
            "The iteration is 4000 and the loss is 0.5067909328767857\n",
            "The weight is \n",
            "[ 3.8872583   1.35112116  0.1900625   1.22336118 -0.81687996  0.87908101\n",
            " -0.11040998  2.02061164  1.52997977 -0.79533963  0.32887155 -0.85404478\n",
            "  3.1957939  -4.41010675  0.83229353 -0.15333565 -0.92268175  0.04408435\n",
            " -1.01687722  0.65736475 -0.05495543  0.31975762  0.         -0.0706822\n",
            " -0.461898    1.18252665  1.17617513]\n",
            "The intercept is \n",
            "-3.686973043465019\n",
            "The confusion matrix is: \n",
            "[1169, 1016]\n",
            "[505, 3568]\n",
            "The accuracy for the Twitter sentiment is 0.7569511025886865\n",
            "The precision for the Twitter sentiment is 0.7783595113438045\n",
            "The recall for the Twitter sentiment is 0.8760127670022096\n",
            "The F1 score for the Twitter sentiment is 0.8243040314196604\n",
            "The F1 with Feature 28 is: 0.8243040314196604 but the target was: 0.8242088242088242\n",
            "The difference in F1 score without Feature 28 is: -9.520721083611239e-05\n",
            "---------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-d063feda207e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To remove \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" there was a loss of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_diff_rem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" in F1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(all_features)):\n",
        "  print(\"To remove \" + str(all_features[i]) + \" there was a loss of \" + str(f1_diff_rem[i]) + \" in F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvV0U87jSz1h",
        "outputId": "e13d8d91-c1e2-4486-b471-0b9b2835ff5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To remove feature1 there was a loss of 0.0029694784359608484 in F1\n",
            "To remove feature2 there was a loss of 0.00012300116118313387 in F1\n",
            "To remove feature3 there was a loss of 9.518522048845846e-05 in F1\n",
            "To remove feature4 there was a loss of 0.0008831891048981522 in F1\n",
            "To remove feature5 there was a loss of 9.518522048845846e-05 in F1\n",
            "To remove feature6 there was a loss of 0.00047610531481501184 in F1\n",
            "To remove feature7 there was a loss of 9.518522048845846e-05 in F1\n",
            "To remove feature8 there was a loss of 0.00230362402474682 in F1\n",
            "To remove feature9 there was a loss of -0.0003667618865678124 in F1\n",
            "To remove feature10 there was a loss of 0.0005571323275446183 in F1\n",
            "To remove feature11 there was a loss of 0.00042129534046397854 in F1\n",
            "To remove feature12 there was a loss of 0.0010324622512989379 in F1\n",
            "To remove Feature 13 there was a loss of 0.002373053161951133 in F1\n",
            "To remove Feature 14 there was a loss of 0.008221761716043718 in F1\n",
            "To remove Feature 15 there was a loss of 0.0005978735330705032 in F1\n",
            "To remove Feature 16 there was a loss of -0.00040727093030523687 in F1\n",
            "To remove Feature 17 there was a loss of -4.0598423970195086e-05 in F1\n",
            "To remove Feature 18 there was a loss of 0.0 in F1\n",
            "To remove Feature 19 there was a loss of 0.0028545371739270298 in F1\n",
            "To remove Feature 20 there was a loss of 0.00019034845824683622 in F1\n",
            "To remove Feature 21 there was a loss of 0.0 in F1\n",
            "To remove Feature 22 there was a loss of -0.0003262341254717249 in F1\n",
            "To remove Feature 23 there was a loss of 0.0 in F1\n",
            "To remove Feature 24 there was a loss of 4.06171847946446e-05 in F1\n",
            "To remove Feature 25 there was a loss of 8.125314342100065e-05 in F1\n",
            "To remove Feature 26 there was a loss of 0.0010050414848016276 in F1\n",
            "To remove Feature 27 there was a loss of 0.002701959088251993 in F1\n",
            "To remove Feature 28 there was a loss of -9.520721083611239e-05 in F1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_diffs_rem_percent = []\n",
        "maxnum=(max(f1_diff_rem))\n",
        "for i in range (0,len(f1_diff_rem)):\n",
        "  num=f1_diff_rem[i]\n",
        "  val = 100 * (num/maxnum)\n",
        "  f1_diffs_add_percent.append(val)\n",
        "  print(\"The feature \" + str(all_features[i]) + \" has a relative importance percentage \" + str(f1_diffs_rem_percent[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "exMp_DZMum0K",
        "outputId": "c2d703c5-1cdc-4e8c-da91-c5bcbe3bf5fe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c691863ad1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf1_diffs_rem_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaxnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_diff_rem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_diff_rem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_diff_rem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f1_diff_rem' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizations"
      ],
      "metadata": {
        "id": "4abZieyLZrX6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZPPS0I6mR5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "some visualization of how much the F1 difference is for each feature\n",
        "order the features from best to worst with a label of each feature\n",
        "that way we can try to describe why the best measured better\n",
        "and why the worst measured worse\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qZ_zO2MWZuYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "try remove one\n",
        "To remove feature1 there was a loss of 0.0029694784359608484 in F1\n",
        "To remove feature2 there was a loss of 0.00012300116118313387 in F1\n",
        "To remove feature3 there was a loss of 9.518522048845846e-05 in F1\n",
        "To remove feature4 there was a loss of 0.0008831891048981522 in F1\n",
        "To remove feature5 there was a loss of 9.518522048845846e-05 in F1\n",
        "To remove feature6 there was a loss of 0.00047610531481501184 in F1\n",
        "To remove feature7 there was a loss of 9.518522048845846e-05 in F1\n",
        "To remove feature8 there was a loss of 0.00230362402474682 in F1\n",
        "To remove feature9 there was a loss of -0.0003667618865678124 in F1\n",
        "To remove feature10 there was a loss of 0.0005571323275446183 in F1\n",
        "To remove feature11 there was a loss of 0.00042129534046397854 in F1\n",
        "To remove feature12 there was a loss of 0.0010324622512989379 in F1\n",
        "To remove Feature 13 there was a loss of 0.002373053161951133 in F1\n",
        "To remove Feature 14 there was a loss of 0.008221761716043718 in F1\n",
        "To remove Feature 15 there was a loss of 0.0005978735330705032 in F1\n",
        "To remove Feature 16 there was a loss of -0.00040727093030523687 in F1\n",
        "To remove Feature 17 there was a loss of -4.0598423970195086e-05 in F1\n",
        "To remove Feature 18 there was a loss of 0.0 in F1\n",
        "To remove Feature 19 there was a loss of 0.0028545371739270298 in F1\n",
        "To remove Feature 20 there was a loss of 0.00019034845824683622 in F1\n",
        "To remove Feature 21 there was a loss of 0.0 in F1\n",
        "To remove Feature 22 there was a loss of -0.0003262341254717249 in F1\n",
        "To remove Feature 23 there was a loss of 0.0 in F1\n",
        "To remove Feature 24 there was a loss of 4.06171847946446e-05 in F1\n",
        "To remove Feature 25 there was a loss of 8.125314342100065e-05 in F1\n",
        "To remove Feature 26 there was a loss of 0.0010050414848016276 in F1\n",
        "To remove Feature 27 there was a loss of 0.002701959088251993 in F1\n",
        "To remove Feature 28 there was a loss of -9.520721083611239e-05 in F1\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ub0mg5yBediE",
        "outputId": "90b0cd08-c7c5-482e-9802-cb2428730fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntry remove one\\nTo remove feature1 there was a loss of 0.0029694784359608484 in F1\\nTo remove feature2 there was a loss of 0.00012300116118313387 in F1\\nTo remove feature3 there was a loss of 9.518522048845846e-05 in F1\\nTo remove feature4 there was a loss of 0.0008831891048981522 in F1\\nTo remove feature5 there was a loss of 9.518522048845846e-05 in F1\\nTo remove feature6 there was a loss of 0.00047610531481501184 in F1\\nTo remove feature7 there was a loss of 9.518522048845846e-05 in F1\\nTo remove feature8 there was a loss of 0.00230362402474682 in F1\\nTo remove feature9 there was a loss of -0.0003667618865678124 in F1\\nTo remove feature10 there was a loss of 0.0005571323275446183 in F1\\nTo remove feature11 there was a loss of 0.00042129534046397854 in F1\\nTo remove feature12 there was a loss of 0.0010324622512989379 in F1\\nTo remove Feature 13 there was a loss of 0.002373053161951133 in F1\\nTo remove Feature 14 there was a loss of 0.008221761716043718 in F1\\nTo remove Feature 15 there was a loss of 0.0005978735330705032 in F1\\nTo remove Feature 16 there was a loss of -0.00040727093030523687 in F1\\nTo remove Feature 17 there was a loss of -4.0598423970195086e-05 in F1\\nTo remove Feature 18 there was a loss of 0.0 in F1\\nTo remove Feature 19 there was a loss of 0.0028545371739270298 in F1\\nTo remove Feature 20 there was a loss of 0.00019034845824683622 in F1\\nTo remove Feature 21 there was a loss of 0.0 in F1\\nTo remove Feature 22 there was a loss of -0.0003262341254717249 in F1\\nTo remove Feature 23 there was a loss of 0.0 in F1\\nTo remove Feature 24 there was a loss of 4.06171847946446e-05 in F1\\nTo remove Feature 25 there was a loss of 8.125314342100065e-05 in F1\\nTo remove Feature 26 there was a loss of 0.0010050414848016276 in F1\\nTo remove Feature 27 there was a loss of 0.002701959088251993 in F1\\nTo remove Feature 28 there was a loss of -9.520721083611239e-05 in F1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "try add one\n",
        "I think these are wrong\n",
        "To remove feature10 there was a loss of 0.001561222732809453 in F1\n",
        "To remove feature11 there was a loss of 0.0017659703644463676 in F1\n",
        "To remove feature12 there was a loss of 0.00199686392251619 in F1\n",
        "To remove Feature 15 there was a loss of 0.002091774687873027 in F1\n",
        "To remove Feature 16 there was a loss of 0.002310002310002335 in F1\n",
        "To remove Feature 17 there was a loss of 0.002961711044851878 in F1\n",
        "To remove Feature 18 there was a loss of 0.002961711044851878 in F1\n",
        "To remove Feature 19 there was a loss of 0.00046200046200051137 in F1\n",
        "To remove Feature 20 there was a loss of 0.0007337779981033687 in F1\n",
        "To remove Feature 23 there was a loss of 0.0007337779981033687 in F1\n",
        "To remove Feature 24 there was a loss of 8.125314342100065e-05 in F1\n",
        "To remove Feature 25 there was a loss of -9.520721083611239e-05 in F1\n",
        "To remove Feature 28 there was a loss of 0.0 in F1\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GgDVS-uYei_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "need to put in sklearn's coefficients\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2tLNpOWrg0SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Verify our LogReg against sklearn"
      ],
      "metadata": {
        "id": "KjL4vj0yvd5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calling sklearn to compare. checks out my math above is correct!\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "confusion_matrix_twitter = confusion_matrix(y_test, y_pred)\n",
        "print(\"The confusion matrix for Twitter sentinment:\")\n",
        "print(confusion_matrix_twitter)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_twitter)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JNWDZ8A8Sbyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics \n",
        "print(\"The confusion matrix for Twitter sentiment:\")\n",
        "print(confusion_matrix_twitter)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", metrics.f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "dy9yad9Bs3Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "E_I7rz3jSeDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "POPn5VzdWvnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I implemented it from scratch above, just using this as a benchmark to test my code. \n",
        "#My code's metrics are really really close so I consider that a win\n",
        "from sklearn import metrics  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#X=np.concatenate((X_train_norm, X_test_norm))\n",
        "#y=np.concatenate((y_train, y_test))\n",
        "#y.loc[y.Label == 2, 'Label'] = 1\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
        "Xc, yc, test_size=0.33, random_state=42)\n",
        "#X_train2, X_test2, y_train2, y_test2 = X_train_norm, X_test_norm, y_train, y_test\n",
        "goodmodel = LogisticRegression()\n",
        "goodmodel.fit(X_train2, y_train2)\n",
        "y_pred2 = pd.Series(goodmodel.predict(X_test2))\n",
        "confusion_matrix_twitter2 = confusion_matrix(y_test2, y_pred2)\n",
        "print(\"The confusion matrix for Twitter sentiment:\")\n",
        "print(confusion_matrix_twitter2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_twitter2)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test2, y_pred2))\n",
        "print(\"Precision:\", metrics.precision_score(y_test2, y_pred2))\n",
        "print(\"Recall:\", metrics.recall_score(y_test2, y_pred2))\n",
        "print(\"F1:\"), metrics.f1_score(y_test2, y_pred2)"
      ],
      "metadata": {
        "id": "glUnTXt8om5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test2"
      ],
      "metadata": {
        "id": "YMFkOfozo6in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2"
      ],
      "metadata": {
        "id": "QOZLX2oio7Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Complete\")"
      ],
      "metadata": {
        "id": "xR1--HPQpTwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow about 16min for runtime\n",
        "#Extract all features and put them in on data frame\n",
        "\n",
        "def feature_extraction(data_location, label_location):\n",
        "  df = pd.read_csv(data_location, header=None)\n",
        "  df.columns = ['TWEET']\n",
        "\n",
        "  df[\"Tweet Tokens\"] = np.nan\n",
        "  df[\"Count: Words in + Lexicon\"] = np.nan\n",
        "  df[\"Count: Words in - Lexicon\"] = np.nan\n",
        "  df[\"Contain The word NO? \"] = np.nan\n",
        "  df[\"Count: Nouns\"] = np.nan\n",
        "  df[\"Ratio: Unique Words-Total Words\"] = np.nan\n",
        "  df[\"Ratio: Stop Words-Total Words\"] = np.nan\n",
        "  df[\"Count: Adjectives in Tweet\"] = np.nan\n",
        "  df[\"Log: Tweet word count\"] = np.nan\n",
        "  df[\"Log: Length of Longest Word in Tweet\"] = np.nan\n",
        "  df[\"Log: Count of Words with 5+ Characters\"] = np.nan\n",
        "\n",
        "  # ADD LABELS COLUMN TO DF \n",
        "  labels = pd.read_csv(label_location, sep=\"\\n\", header=None)\n",
        "  df = pd.concat([df,labels], axis = 1)\n",
        "  df.rename(columns = {0:'Labels'}, inplace = True)\n",
        "\n",
        "  # DROP NEUTRAL LABELS FROM DF\n",
        "  df.drop(df.loc[df['Labels']==1].index, inplace=True)\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  # CHANGE ALL 2 LABEL VALUES TO 1 \n",
        "  for i in range(0, len(df.index)):\n",
        "    if df['Labels'].values[i] == 2:\n",
        "      df.at[i,'Labels'] = 1\n",
        "\n",
        "  # CLEAN TWEETS\n",
        "  pattern_a = r'[^A-Za-z0-9]+'\n",
        "  pattern_b = r'\\b\\w{1,1}\\b'\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "  for i in range(0, len(df.index)):                       \n",
        "    df['TWEET'].values[i] = df['TWEET'].values[i].lower()\n",
        "    df['TWEET'].values[i] = df['TWEET'].values[i].replace('@user', '')\n",
        "    df['TWEET'].values[i] = re.sub(pattern_a, ' ', df['TWEET'].values[i])\n",
        "    df['TWEET'].values[i] = re.sub(pattern_b, '', df['TWEET'].values[i])\n",
        "    df['TWEET'].values[i] = re.sub(emoji_pattern, '', df['TWEET'].values[i])\n",
        "\n",
        "  #TOKENIZE TWEETS\n",
        "  df['Tweet Tokens'] = df['Tweet Tokens'].astype('object')\n",
        "  for i in range(0, len(df.index)):\n",
        "    \n",
        "    tokens = df['TWEET'].values[i].split()\n",
        "    df.at[i, 'Tweet Tokens'] = tokens\n",
        "\n",
        "  #Create Positive and Negative Lexicons\n",
        "  pos_lexicon = []\n",
        "  neg_lexicon = []\n",
        "\n",
        "  for i in range(0, len(subreddit_dataframes)):               \n",
        "    for j in range(0, len(subreddit_dataframes[i].index)):    \n",
        "\n",
        "      if subreddit_dataframes[i]['Sentiment Score'].values[j] >= 0:         \n",
        "        pos_lexicon.append(subreddit_dataframes[i]['Word'].values[j])        \n",
        "      \n",
        "      if subreddit_dataframes[i]['Sentiment Score'].values[j] < 0:         \n",
        "        neg_lexicon.append(subreddit_dataframes[i]['Word'].values[j])        \n",
        "\n",
        "\n",
        "  pos_lexicon = [*set(pos_lexicon)]   # Remove duplicate values from + lexicon\n",
        "  neg_lexicon = [*set(neg_lexicon)]   # Remove duplicate values from - lexicon\n",
        "\n",
        "\n",
        "  #Handle duplicate words in postitive and negative lexicon\n",
        "  same_wrds = set(pos_lexicon).intersection(neg_lexicon)      #get set of all words that appear in both psoitive and Negative Lexicon\n",
        "                                                              #https://stackoverflow.com/questions/1388818/how-can-i-compare-two-lists-in-python-and-return-matches\n",
        "\n",
        "  word_vals_dict = dict.fromkeys(same_wrds, 0)                # Create a dictionary to hold of all words found in positive and negative lexicons     \n",
        "  sentiment_vals2sum = []\n",
        "\n",
        "  for k in range(0, len(same_wrds)):    # In the set of words identified in positive and negative lexicon\n",
        "  \n",
        "    i = same_wrds.pop()                 # i will return one word from the set, then the following with each iteration\n",
        "    same_wrds.add(i)                    # https://stackoverflow.com/questions/59825/how-to-retrieve-an-element-from-a-set-without-removing-it\n",
        "\n",
        "    sentiment_vals2sum = []             # Will store the Sentiment Scores collected across data frames \n",
        "\n",
        "    for j in range(0, len(subreddit_dataframes)):                    # in range of dataframes(44)\n",
        "      is_wrd_there = i in subreddit_dataframes[j]['Word'].unique()   # Return true or false. True if desired word 'i' is in the data frame being checked false if not (ls_df_names[0], ls_df_names[1],...)\n",
        "      if is_wrd_there is True:                              # If true... \n",
        "\n",
        "        mask1 = subreddit_dataframes[j]['Word'].values == i                  # Get the sentiment value of the word from its dataframe \n",
        "                                                                    # https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
        "                                                      \n",
        "        sentiment_vals2sum.append(subreddit_dataframes[j][mask1].iat[0,1])   # append sentiment value to list where they are stored eg. sentiment_vals2sum \n",
        "        num_avg = mean(sentiment_vals2sum)                          # Take the mean of the sentiment scores collected in the list above(these are all sentiment scores for one word collected across dataframes where the word was found 'True')\n",
        "        word_vals_dict[i] = num_avg  \n",
        "\n",
        "  # Remove words that now have a clear positive or negative classification\n",
        "  for w in word_vals_dict.items():        # .items() returns a tuple of (word, score). See Cell above for all words: scores in dict\n",
        "    if w[1] >= 0:                         # if w[1] (the score) is greater than 0...\n",
        "      neg_lexicon.remove(w[0])            # remove it from the negative lexicon\n",
        "    if w[1] < 0:                          # If w[1] (the score) is less than 0...\n",
        "      pos_lexicon.remove(w[0])            # remove the word from the positive lexicon\n",
        "\n",
        "  # COUNT POSITIVE AND NEGATIVE WORDS\n",
        "  neg_lex_set = set(neg_lexicon)\n",
        "  pos_lex_set = set(pos_lexicon)\n",
        "\n",
        "  for i in range(0, len(df.index)):\n",
        "    x = set(df['Tweet Tokens'][i])\n",
        "    df['Count: Words in + Lexicon'].values[i] = len(x.intersection(pos_lex_set))\n",
        "    df['Count: Words in - Lexicon'].values[i] = len(x.intersection(neg_lex_set))\n",
        "\n",
        "  # DOES TWEET CONTAIN NO\n",
        "  for i in range(0, len(df.index)):                       # In the range 0 to length of the tweets dataframe         # Tokenize and lowercase tweets \n",
        "    if 'no' in df['Tweet Tokens'][i]:                                    # If no is in tweet dataframe value is 1 if not value is zero\n",
        "      df['Contain The word NO? '].values[i] = 1\n",
        "    else: \n",
        "      df['Contain The word NO? '].values[i] = 0\n",
        "\n",
        "  # COUNT NOUNS IN TWEET\n",
        "  for i in range(0, len(df.index)):\n",
        "    tokens = df['Tweet Tokens'][i]\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    noun_definitions = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    count = 0\n",
        "\n",
        "    for j in range(0, len(tagged)):\n",
        "      if tagged[j][1] in noun_definitions:\n",
        "        count += 1\n",
        "    \n",
        "    df['Count: Nouns'].values[i] = count    \n",
        "\n",
        "  # RATIO: UNIQUE TO TOTAL WORDS\n",
        "  for i in range(0, len(df.index)):                       \n",
        "    tokens = df['Tweet Tokens'][i]\n",
        "    x = np.array(tokens)\n",
        "    ratio = len(np.unique(x)) / len(tokens)\n",
        "    df['Ratio: Unique Words-Total Words'].values[i] = ratio    \n",
        "\n",
        "  # STOP WORDS TO TOTAL WORDS\n",
        "  for i in range(0, len(df.index)):                       \n",
        "    tokens = df['Tweet Tokens'][i]\n",
        "    x = np.array(tokens)\n",
        "    stop_wrds_count = [w for w in tokens if w in stop_words]\n",
        "    ratio = len(stop_wrds_count) / len(tokens)\n",
        "    df['Ratio: Stop Words-Total Words'].values[i] = ratio\n",
        "\n",
        "  #ADJECTIVES IN TWEET\n",
        "  for i in range(0, len(df.index)):\n",
        "    tokens = df['Tweet Tokens'][i]\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    adj_definitions = ['JJ', 'JJR', 'JJS ']\n",
        "    count = 0\n",
        "\n",
        "    for j in range(0, len(tagged)):\n",
        "      if tagged[j][1] in adj_definitions:\n",
        "        count += 1\n",
        "    \n",
        "    df['Count: Adjectives in Tweet'].values[i] = count\n",
        "\n",
        "  # LOG OF TWEET WORD COUNT \n",
        "  for i in range(0, len(df.index)):           #In range: length of dataframe of tweets\n",
        "    tokens = df['Tweet Tokens'][i]       #tokenize tweets \n",
        "    log_val = math.log(len(tokens))\n",
        "    df['Log: Tweet word count'].values[i] = log_val\n",
        "\n",
        "\n",
        "  # LOG LENGTH OF LONGEST WORD IN TWEET\n",
        "  for i in range(0, len(df.index)):           #In range: length of dataframe of tweets\n",
        "    tokens = df['Tweet Tokens'][i]   #tokenize tweets \n",
        "    longest_wrd = max(tokens, key=len)\n",
        "    log_val =  math.log(len(longest_wrd))\n",
        "    df['Log: Length of Longest Word in Tweet'].values[i] = log_val\n",
        "\n",
        "  #LOG OF COUNT OF WORD WITH 5+ CHARACTERS\n",
        "  for i in range(0, len(df.index)):           #In range: length of dataframe of tweets\n",
        "    tokens = df['Tweet Tokens'][i]    #tokenize tweets\n",
        "    count = 0\n",
        "      \n",
        "    if any(len(i) >= 5 for i in tokens) is True:\n",
        "\n",
        "      for j in range(0, len(tokens)):\n",
        "        if len(tokens[j]) >= 5:\n",
        "          count = count +1\n",
        "\n",
        "      log_val = math.log(count)\n",
        "      df['Log: Count of Words with 5+ Characters'].values[i] = log_val\n",
        "      \n",
        "    else:\n",
        "      df['Log: Count of Words with 5+ Characters'].values[i] = 0\n",
        "\n",
        "\n",
        "  return(df)"
      ],
      "metadata": {
        "id": "92GOZSc0Nmps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to run: \n",
        "# dataframe = feature_extraction(data_location, label_location)"
      ],
      "metadata": {
        "id": "p45NGKHsMeWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}